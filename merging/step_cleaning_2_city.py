# -*- coding: utf-8 -*-
"""Step CLEANING - 2 - CIty.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QZfs4OlgfiH-PzxiaMCl0XCdqWVVT6HH
"""

# TODO: we need to merge based on the similarity, that's what the WRI people want us to do.

"""So, there are new power plants taht have been found in BU_CGP that were not already in Power Plant (that is, WEPP and GPPD). So, we need to post-process City.

This notebook can be run one time at the end when all entries of countries have been added together.
* the assumption here that all cities from all the datasets are here together. So, if each datasets produced a City table, then all these cities have been concatanated together.

I am re-using what I already did for step 1:
* The updating of the country names from "Step 1 - Power Plant, ..." notebook
* the removing the duplicates and the creation of the bridging table follows "Step 1.b - Creating..." (the main difference from there is that here we don't change the PP_key of Power Plant because they are already fixed)


Note that this code is meant to be extended:
* as of now, we are only dropping the duplicates. So, it is not actually needed to have a bridging table becuase in the end all the cities names present in Power Plant are still here (if we were to keep the city names here).
* the idea in the future is to make sure that cities that are likely duplicates (e.g., because of spelling differences) are united together then the bridging table is indeed needed.

"""

import pandas as pd

# import custom function
from functions.cleaning import create_placeholder, make_city_key, preprocess_text, get_all_files, my_reset_index, update_country_names, my_read_excel

# needed folder paths
input_folder = "Cleaned Data/" # or we can put this info in the "files name" sheet
intermediate_folder = "Intermediate Results/" # to save some intermediate results
# output_folder = "Final tables/" # where we store the final tables (e.g. Power Plant)
output_folder = "Current Final Tables/"

# load the excel containing all the mapping information
map_table_name = "Variables.xlsx"
map_table = pd.read_excel(map_table_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(map_table))


# the sheet where there is the information that the client can change
files_names_sheet = "file names"
groupby_info_sheet = "formatting"
vertical_slicing_info_sheet = "variables to use"
aggregation_info_sheet = "aggregation"
db_structure_sheet = "db structure"
un_country_info_sheet = "UN Country Info"
country_names_dict_sheet = "country names dicionary"
bri_countries_sheet = "BRI countries"

# the name of the city table
city_name = "City"

# the name of the bridging table between Power Plant and City
# bridging_keys_table_name = "CITYKEY_BRIDGE_PP_C.xlsx"
bridging_keys_table_name = "CITYKEY_BRIDGE_PP_C.csv"

# list of the names of the various Country tables
# TODO: uniform all the names of these files
# city_files_list = ["City_not_cleaned.xlsx", "City_need_post_processing_BU_CGP_.xlsx", "BU_CGEF_fuzzy_matching - fuzz.partial_ratio/City_BUCGEF.xlsx", "City_SAISCLA_IADGEGI.xlsx", "City_FDI_RMA.xlsx"]
# city_files_list = ["City_not_cleaned.xlsx", "City_need_post_processing_BU_CGP_.xlsx", "BU_CGEF_fuzzy_matching - fuzz.partial_ratio/City_BUCGEF.xlsx", "City_SAISCLA_IADGEGI.xlsx", "City_FDI_RMA.xlsx", "City_IJ_Global.xlsx"]

"""List of sources to clean and merge together after processing each data source:
* after WEPP & GPPD: ['City_WEPPGPPD.xlsx']
* after BU_CGP: ['City_BUCGP.xlsx', 'City_WEPPGPPD.xlsx']
* after BU_CGEF: ['City_BUCGEF.xlsx', 'City_BUCGP.xlsx', 'City_WEPPGPPD.xlsx']
* after SAIS_CLA+IAD_GEGI: ['City_SAISCLA_IADGEGI.xlsx',  'City_BUCGEF.xlsx', 'City_BUCGP.xlsx', 'City_WEPPGPPD.xlsx']
* after IJ_Global: ['City_IJ_Global.xlsx', 'City_SAISCLA_IADGEGI.xlsx',
 'City_BUCGEF.xlsx',
 'City_BUCGP.xlsx',
 'City_WEPPGPPD.xlsx']
* afetr REF_MA: ['City_REFINITIVMA.xlsx',
 'City_IJ_Global.xlsx',
 'City_SAISCLA_IADGEGI.xlsx',
 'City_BUCGEF.xlsx',
 'City_BUCGP.xlsx',
 'City_WEPPGPPD.xlsx']
* after FDI_Markets: ['City_FDIMARKETS.xlsx',
 'City_REFINITIVMA.xlsx',
 'City_IJ_Global.xlsx',
 'City_SAISCLA_IADGEGI.xlsx',
 'City_BUCGEF.xlsx',
 'City_BUCGP.xlsx',
 'City_WEPPGPPD.xlsx']
* after REF_LOAN: ['City_REFINITIVLOAN.xlsx',
 'City_FDIMARKETS.xlsx',
 'City_REFINITIVMA.xlsx',
 'City_IJ_Global.xlsx',
 'City_SAISCLA_IADGEGI.xlsx',
 'City_BUCGEF.xlsx',
 'City_BUCGP.xlsx',
 'City_WEPPGPPD.xlsx']
"""

# get the files
city_files_list = get_all_files(intermediate_folder, "City")
city_files_list

"""#### 0. Read data

Concatanate the data from the list of files.
"""

# read all files
datasets = []
for file in city_files_list:
    datasets.append(my_read_excel(intermediate_folder , file))

datasets[0].shape

# concatanate
city = pd.concat(datasets)
city = my_reset_index(city)
if "Unnamed: 0" in city:
    city = city.drop(columns=['Unnamed: 0'])
city.shape

# check that there were no problems with concat: all entries have been added
city.shape[0] == sum([x.shape[0] for x in datasets])

"""Note that re-setting the index makes sure that each entry in the table has a unique number, the index. This can be used to create unique placeholders.

Note: as did in Step 2, after the creation of the tables "city_key" will be a copy of the "PP_key", so they keys will start with the prefix "PLANTKEY". In this notebook we will change them in City so we don't have to change them in the Power Plant table.
"""

city.head()

city.tail()

"""#### 1. Create placeholders

We need to put some placeholders for those cities that do not have a name (but do have information on province and/or country). This is needed to preserve all the information from the sources. For example:
* there is a power plant that has no province but country name
* with the placeholder now there is an entry in the City table
* with this entry we can connect Power Plant and the Country table and thus getting the original information.

Moreover, the placeholder is needed to enable the dropping of redundant information.

Same for the "province" field.
"""

city.shape[0]

city.isna().sum()

# 54.8s
for i, row in city.iterrows():
    city.at[i, "city"] = create_placeholder(row, "city", "unnamed_city_", i)
    city.at[i, "province"] = create_placeholder(row, "province", "unnamed_province_", i)
    #@TODO to check
    city.at[i, "country"] = create_placeholder(row, "country", "unnamed_country_", i)

city.head()

# check NaNs: there should be zero in all the cols
city.isna().sum()

# fix the types
city['city'] = city['city'].astype("string")
city['province'] = city['province'].astype("string")
city['country'] = city['country'].astype("string")

"""#### 2. Preprocess the strings"""

# put everyhting in lower case because the un names dictionary is in lower case
city['city'] = city['city'].apply(lambda x: preprocess_text(x))
city['province'] = city['province'].apply(lambda x: preprocess_text(x))

# country we just lower because we created a whole dictionary based on that
# the issue is that in the UN data they also have punctiation
# TODO: solution: we remove the punctiation both in the UN and in the dicitonary or it's fine like this
city['country'] = city['country'].apply(lambda x: x.lower())
city.head()

"""#### 2. Fix the country name

The official UN name of the countries will be used as keys to bridge the Country and City table.
"""

# TODO: there0s now a fucntion for this, see Step 5

# Check
city.loc[city['country'].isin(['c么te divoire', 'china hong kong special administrative region'])]

"""There is a dictionary that changes the name of the countries to the offifcial UN ones. Note: this dictionary is used to also fix Country, so it should be updated with all countries that are in both the Country and City tables."""

# load dictionary containing the naming conversions
country_names_dict_df = map_table[country_names_dict_sheet]

# convert the names
city = update_country_names(country_names_dict_df, city)

city.head()

# Check
print(f"Rows with old values: {city.loc[city['country'].isin(['c么te divoire', 'china hong kong special administrative region'])].shape[0]}")
print(f"Rows with new values: {city.loc[city['country'].isin([country_names_dict_df.loc[country_names_dict_df['original name'] == x]['UN name'].values[0] for x in ['c么te divoire', 'china hong kong special administrative region']])].shape[0]}")

# check
city.loc[city['country'].isin([country_names_dict_df.loc[country_names_dict_df['original name'] == x]['UN name'].values[0] for x in ['c么te divoire', 'china hong kong special administrative region']])]

# show examples
city.loc[city['city_key'].isin(["PLANTKEY_IJGLOBAL_EQUITY_100", "PLANTKEY_IJGLOBAL_EQUITY_166"])]

"""#### 3. Groupby to get the duplicates of the keys

The issue with dropping is that we have the "city_key" column that is used to join the Power Plant and City entities. So, when dropping duplicates we need to take track of which rows represent the same city (in the same province and country), take note of their city_keys and then keeping one example in the City entity.
* this can done using groupby: we group together the cities that are identical and then we can easily get all their keys together.
* the keys of the same cities will be kept in a DataFrame that can be used to either:
    * it can be used when we want to join the Power Plant and City: we join Power Plant with this table and then we join with City
    * we can update the keys in Power Plant according to this table
"""

city_agg = city.groupby(by=['country', 'province', 'city']).agg({"city_key": '; '.join})
city_agg = city_agg.reset_index()
# rename the current city_key so to faciliate operations
city_agg = city_agg.rename(columns={"city_key": "city_key_agg"})
city_agg.head()

print("current numbers of cities: " + str(city_agg.shape[0]))
perc = city_agg.shape[0] / city.shape[0] * 100
print("current rows % over initial number of rows: " + str(perc))
print("dropped rows % over initial number of rows: " + str(100 - perc))
# the percentages below should be around 55 and 44 (in that order)

"""So, now we need to keep only one key per entry: this will become the key that represent each city.

Just for convinience we re-use one of the already existing keys to represent a city. New keys (e.g. using the index of the table) could be equally used.
"""

# create new column from city_key_agg: keep the first key in city_key_agg
city_agg['city_key'] = city_agg['city_key_agg'].apply(lambda x: x.split("; ")[0])
city_agg.head()

# even if we want to drop based on the city, province, and country info, there is nothing to drop
city_agg.drop_duplicates(subset=['city', "province", "country"]).shape[0] == city_agg.shape[0]

"""So, now, city has a unique key per entry, the only thing that is left to do is changing the key to something more representative of the city."""

city_agg['city_key'] = city_agg['city_key'].apply(make_city_key)
city_agg.head()

print(f"Current number of cities: {city_agg.shape[0]}\n")
# count for each datasource how many cities are there that come from there
for original_datasource in ['WEPPGPPD', "BUCGP", "BUCGEF", "SAISCLA", "IJGLOBAL", "REFINITIVMA", "FDIMARKETS", "REFINITIVLOAN"]:
    print(f"{original_datasource}: {city_agg.loc[city_agg['city_key'].str.contains(original_datasource)].shape[0]}")

output_folder

# save the tables
city_columns_to_save = city_agg.columns.to_list()
city_columns_to_save.remove("city_key_agg")
# city_agg[city_columns_to_save].to_excel(output_folder + city_name + ".xlsx", index=False)
city_agg[city_columns_to_save].to_csv(output_folder + city_name + ".csv", index=False)

"""#### 4. Create the bridging dataframe

Since this dataframe will be used as a bridge between Power Plant and City, we need to have an entry for all the keys present in Power Plant. That's why we kept all the city_keys when aggregating (we concatatenated).
"""

# create new dataframe
keys_df = city_agg[['city_key', "city_key_agg"]]
keys_df.head()

"""The final bridging table will have two columns:
* PP_key: the multiple keys coming from "city_key_agg". These values will not be present in Power Plant but they are in City. These are the keys that identfiy the different power plants.
* C_key: the keys that are present in City. These represent the cities.
"""

# create the DataFrame with the matching keys

new_rows = [] # will contain all the new rows

# iterate through all the matches that we have in keys_df
for i, row in keys_df.iterrows():

    # split the value contained in city_key_agg and iterate through those keys
    for key in row['city_key_agg'].split("; "):
        if key != "":
            # to each of the keys in city_key_agg associate the matching city_key
            new_rows.append([key, row['city_key']])

# note columns name and order: "PP_key" is matched with the values that come from city_key_agg, "C_key" with the city_key
keys_df_final = pd.DataFrame(new_rows, columns=["PP_key", "city_key"])

keys_df_final.head()

keys_df_final.shape[0]

output_folder

# keys_df_final.to_excel(output_folder + bridging_keys_table_name, index=False)
keys_df_final.to_csv(output_folder + bridging_keys_table_name, index=False)

