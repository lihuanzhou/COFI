# -*- coding: utf-8 -*-
"""Step 8 - REFINITIV_LOAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1azcFk838Oikjp-V3IiEFOBJ0ev1dNqhE

Things to keep in mind:
*  Each row is about an investor's investing in a power plant.
* Each row is a company's loan to a power plant project
* Each Deal PermID is a investment - which can have multiple investors
* The "all managers..." Column should be the investor band
"""

import pandas as pd
import numpy as np

# import custom-made functions
from functions.cleaning import get_variables, create_placeholder, create_unique_id, get_variables_advanced, preprocess_text, my_reset_index
from functions.cleaning import  check_if_new_country_names, update_country_names, convert_primary_fuel
from functions.joining import join_pp_until_country, join_equity_transaction, join_debt_transaction
from functions.final_tables_creation import get_tables_to_create_empty
from functions.matching import find_matches_power_plant
from functions.merging import merge_debt_equity_tables_ij_global
# TODO: the preprocess_text should go in the cleaning functions file

# other useful libraries
import itertools
from statistics import mean, stdev
import statistics

# for the matching of power plant names
from thefuzz import process
from thefuzz import fuzz

from functions.joining import join_pp_full_debt_full
from functions.cleaning import my_read_excel, my_reset_index
from functions.cleaning import get_thresholds_methods_dicts
from functions.matching import find_matches_equity_and_debt_complete
from functions.matching import find_matches_power_plant_or_fill_PP_key
from functions.cleaning import print_final_statistics
from functions.cleaning import fix_investor_names, count_sources_in_id, clean_primary_fuel_column
from functions.matching import prepare_pp_full_for_matching_commissioning_year

# needed folder paths
input_folder = "Cleaned Data/" # or we can put this info in the "files name" sheet
intermediate_folder = "Intermediate Results/" # to save some intermediate results
current_final_tables_folder = "Current Final Tables/"

# load the excel containing all the mapping information
map_table_name = "Variables.xlsx"
map_table = pd.read_excel(map_table_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(map_table))

# load the excel containing all the mapping information
dictionary_fuel_name = "Dictionaries.xlsx"
dictionary_fuel = pd.read_excel(dictionary_fuel_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(dictionary_fuel))

# the sheet where there is the information that the client can change
files_names_sheet = "file names"
groupby_info_sheet = "formatting"
vertical_slicing_info_sheet = "variables to use"
aggregation_info_sheet = "aggregation"
db_structure_sheet = "db structure"
un_country_info_sheet = "UN Country Info"
country_names_dict_sheet = "country names dicionary"
bri_countries_sheet = "BRI countries"
filtering_sheet = "filtering"
merging_sheet = "merging"
matching_threshold_sheet = "matching thersholds"
matching_method_sheet = "matching methods"
matching_paremeter_sheet = "matching parameters"
# we will need this later
thresholds_df = map_table[matching_threshold_sheet]
methods_df = map_table[matching_method_sheet]
parameters_df = map_table[matching_paremeter_sheet]

# the tables we are creating here
tables_to_create = ["Power Plant", "City", "Country", "Debt", "Transaction", "Investor"]

# already exising final data<<
# debt_file = "Debt.xlsx"
# power_plant_file = "Power Plant.xlsx"
# city_file = "City.xlsx"
# country_file = "Country.xlsx"
# bridge_file = "CITYKEY_BRIDGE_PP_C.xlsx"
# equity_file = "Equity.xlsx"
# transaction_file = "Transaction.xlsx"
debt_file = "Debt.csv"
power_plant_file = "Power Plant.csv"
city_file = "City.csv"
country_file = "Country.csv"
bridge_file = "CITYKEY_BRIDGE_PP_C.csv"
equity_file = "Equity.csv"
transaction_file = "Transaction.csv"

# names of the databases used here
ref_loan_name = "REFINITIV_LOAN"

# names of the final tables to be used here
debt_name = "Debt"

# saving information
saving_suffix = "_REFINITIV_LOAN.xlsx"

"""## 1. Read data

#### 1.1 Load data
"""

# this table contains the name of the data sources
file_names = map_table[files_names_sheet]
file_names

ref_loan = my_read_excel(input_folder, file_names.loc[file_names['Database'] == ref_loan_name]["File name"].values[0])

ref_loan_full = ref_loan.copy(deep=True)

ref_loan.head()

"""#### 1.2 Load data for existing Power Plants and Debt data for matching purposes"""

columns_to_keep_pp = ["PP_key", "power_plant_name", "installed_capacity", "primary_fuel", "country"]
columns_to_keep_debt = ["debt_id", "PP_key", "debt_investment_year", "investor_name", "amount"]

pp_full, db_pp = join_pp_full_debt_full(columns_to_keep_pp=columns_to_keep_pp, columns_to_keep_debt=columns_to_keep_debt, new_output_folder=current_final_tables_folder, get_commissioning_year=True)

count_sources_in_id(pp_full, "PP_key", "PP full")

"""#### 1.3 Load Debt table"""

debt = my_read_excel(current_final_tables_folder, debt_file)

debt.tail()

"""## 2. Clean the data

#### 2.1 Vertical slicing
"""

vertical_slicing = map_table[vertical_slicing_info_sheet]
vertical_slicing.head()

ref_loan_variables = get_variables(vertical_slicing, ref_loan_name, tables_to_create)
ref_loan_variables

ref_loan = ref_loan[ref_loan_variables]

ref_loan.columns

"""#### 2.2 Clean if needed"""

# create a quick df to check the number of rows missing and the percentage
# over the total
rows = []
for col in ref_loan.columns:
    missing = ref_loan[col].isna().sum()
    percentage = missing / ref_loan.shape[0] * 100
    rows.append([col, missing, percentage])
pd.DataFrame(rows, columns=['Column', "Missing #", "Missing %"])

"""##### Renamings of columns"""

# do this renaming
ref_loan = ref_loan.rename(columns={"All Managers, inc. Int'l Co-Managers (Name)": "debt_investor_name"})

"""##### Country"""

# OLD: since there are multiple rows for the the same ID that have no country
# we want to check here if the other rows with the same ID present more information
def check_if_values_in_columns(x):
    # return True if there is other information than NaN for all columns but the r_id and the investors names
    rows_id = ref_loan.loc[ref_loan["r_id"] == x]
    if rows_id.shape[0] <= 1:
        return False
    for col in rows_id.columns:
        if col != "r_id" and col != "debt_investor_name":
            # note: nunique doesn't count "nan" when doing the nunique (see example below)
            if rows_id[col].nunique() > 0:
                return True
    return False

# Nunique and np.nans
example = pd.DataFrame([[2, 1],[np.nan, 2]], columns=['A', "B"])
print(example)
print(f"Nunique on \"A\": {example['A'].nunique()}")

# so there is information present for some of them
ids_issue = [x for x in ref_loan.loc[ref_loan['country'].isna()]['r_id'].unique() if check_if_values_in_columns(x)]
len(ids_issue)

# needed to run the functions to do the converting
country_names_dict_df = map_table[country_names_dict_sheet]
un_data = map_table[un_country_info_sheet]

ref_loan['country'] = ref_loan['country'].apply(lambda x: x.lower())

# check if there are new names that are needs to be added to the conversion list
# TODO: in the final product this can also be a feature when they load the db
check_if_new_country_names(list(ref_loan['country'].unique()), un_data, country_names_dict_df)

# convert the names
ref_loan = update_country_names(country_names_dict_df, ref_loan)

"""##### Primary fuel"""

ref_loan['primary_fuel'].unique()

# make conversion
ref_loan = clean_primary_fuel_column(ref_loan)

ref_loan['primary_fuel'].unique()

"""##### installed_capacity"""

# TODO: this should go in the cleaning step....

# there are some values that contain a "," in the capacity
ref_loan['installed_capacity'] = ref_loan['installed_capacity'].fillna("") # this helps in finding the values
ref_loan.loc[ref_loan['installed_capacity'].str.contains(",")]

# first fix: there are rows where there are two capacity values
ref_loan.loc[ref_loan['installed_capacity'].str.count("W") == 2]['installed_capacity'].values

for i, row  in ref_loan.loc[ref_loan['installed_capacity'].str.count("W") == 2].iterrows():
    capacity_value = row['installed_capacity']
    sum = 0
    for el in [x.strip() for x in capacity_value.split(",")]:
        if "MW" in el:
            sum += float(el.replace("MW", ""))
        elif "GW" in el:
            sum += float(el.replace("GW", "")) * 1000
    print(f"{i}: {capacity_value} -> {sum}")
    ref_loan.at[i, "installed_capacity"] = sum

ref_loan['installed_capacity'] = ref_loan['installed_capacity'].astype("string")

# second fix: convert the GW to MW
ref_loan.loc[ref_loan['installed_capacity'].str.contains("GW")]

# make the change
for i, row in ref_loan.loc[ref_loan['installed_capacity'].str.contains("GW")].iterrows():
    ref_loan.at[i, "installed_capacity"] = str(float(row['installed_capacity'].replace("GW", "").strip()) * 1000)

# third fix: remove the "MW"
ref_loan.loc[ref_loan['installed_capacity'].str.contains("MW")]

for i, row in ref_loan.loc[ref_loan['installed_capacity'].str.contains("MW")].iterrows():
    ref_loan.at[i, "installed_capacity"] = row['installed_capacity'].replace("MW", "").replace(",", "").strip()

# there may be some trailing and leading white spaces left
ref_loan['installed_capacity'] = ref_loan['installed_capacity'].apply(lambda x: x.strip())

# then let's convert bakc the empty strings to np.nans so we can convert later the installed capacity to floats
ref_loan['installed_capacity'] = ref_loan['installed_capacity'].apply(lambda x: np.nan if x == "" else x)
# make conversion
ref_loan['installed_capacity'] = ref_loan['installed_capacity'].astype("float64")

"""##### debt_investment_year"""

# we need to extract the year from this datetime format
ref_loan['debt_investment_year'].head(100).tail(10)

ref_loan['debt_investment_year'].dtype

ref_loan['debt_investment_year'] = pd.to_datetime(ref_loan['debt_investment_year'])

# get the year
ref_loan['debt_investment_year'] = ref_loan['debt_investment_year'].dt.year

# check
ref_loan['debt_investment_year']

"""##### investor_name"""

ref_loan['debt_investor_name'] = ref_loan['debt_investor_name'].fillna("")

ref_loan = fix_investor_names(ref_loan, "debt_investor_name", False, None, drop_duplicates=False)

"""#### 2.3 Pre-process REFINITIV_LOAN and existing Power Plant and Debt data for matching."""

pp_full.columns

for col in pp_full.columns:
    if col in ["power_plant_name"]:
        pp_full[col] = pp_full[col].fillna("")
        pp_full[col] = pp_full[col].apply(preprocess_text)
        pp_full[col] = pp_full[col].astype("string")
    elif col in ["country", "primary_fuel"]:
        pp_full[col] = pp_full[col].fillna("")
        pp_full[col] = pp_full[col].astype("string")
    elif col in ['installed_capacity']:
        pp_full[col] = pp_full[col].astype("float64")
    elif col in ["city", "province"]:
        pp_full[col] = pp_full[col].fillna("")
        pp_full[col] = pp_full[col].apply(preprocess_text)
        pp_full[col] = pp_full[col].apply(lambda x: "" if "unnamed" in x else x)
        pp_full[col] = pp_full[col].astype("string")
    elif col ==  "commissioning_year":
        # pp_full[col] = pp_full[col].apply(lambda x: np.nan if (isinstance(x, str)) else x)
        pp_full[col] = pp_full[col].astype("float64")

db_pp.columns

for col in db_pp.columns:
    if col in ["investor_name", "power_plant_name"]:
        db_pp[col] = db_pp[col].fillna("")
        db_pp[col] = db_pp[col].apply(lambda x: preprocess_text(x, True))
        db_pp[col] = db_pp[col].astype("string")
    elif col in ["country", "primary_fuel"]:
        db_pp[col] = db_pp[col].fillna("")
        db_pp[col] = db_pp[col].astype("string")
    elif col in ['debt_investment_year', "amount", "installed_capacity"]:
        db_pp[col] = db_pp[col].astype("float64")
    elif col in ["city", "province"]:
        db_pp[col] = db_pp[col].fillna("")
        db_pp[col] = db_pp[col].apply(lambda x: preprocess_text(x, True))
        db_pp[col] = db_pp[col].apply(lambda x: "" if "unnamed" in x else x)
        db_pp[col] = db_pp[col].astype("string")

ref_loan.columns

for col in ref_loan.columns:
    if col in ["debt_investor_name", "power_plant_name", "province", "parent_company_of_investor"]:
        ref_loan[col] = ref_loan[col].fillna("")
        ref_loan[col] = ref_loan[col].apply(lambda x: preprocess_text(x, True))
        ref_loan[col] = ref_loan[col].astype("string")
    elif col in ["country", "primary_fuel"]:
        ref_loan[col] = ref_loan[col].fillna("")
        ref_loan[col] = ref_loan[col].astype("string")
    elif col in ['debt_investment_year', "debt_investment_amount", "installed_capacity"]:
        ref_loan[col] = ref_loan[col].astype("float64")

"""## 3. Match with existing data

#### 3.1 Load thresholds and methodos to use
"""

thrs_dict_dt, mtds_dict_dt, prmts_dict_dt = get_thresholds_methods_dicts(thresholds_df, methods_df, parameters_df, ref_loan_name, "Debt")

thrs_dict_dt

mtds_dict_dt

prmts_dict_dt

"""#### 3.2 Find the matches in Debt"""

for col in ["country", "primary_fuel", "power_plant_name", "debt_investor_name", "installed_capacity"]:
    missing_no = ref_loan.loc[(ref_loan[col] == "") | (ref_loan[col].isna())].shape[0]
    print(f"{col} missing #: {missing_no}")
    print(f"{col} missing %: {missing_no / ref_loan.shape[0] * 100}\n")

ref_loan = my_reset_index(ref_loan)
db_pp = my_reset_index(db_pp)

ref_loan.columns

# no. of matches in defualt settings: 84
matches_debt = find_matches_equity_and_debt_complete(ref_loan, db_pp, equity_or_debt="Debt",
                                                     thresholds_dict=thrs_dict_dt,
                                                    methods_dict=mtds_dict_dt, parameters_dict=prmts_dict_dt)

matches_debt

matches_debt.keys()

# check
index = 507
ref_loan.head(index + 1).tail(1)[["debt_investment_year", "debt_investor_name", "debt_investment_amount", "power_plant_name", "installed_capacity", "primary_fuel", "country", "r_id"]]

db_pp.loc[db_pp['debt_id'] == matches_debt[index]]

"""#### 3.4 Merge what needs to be merged

What we need to aggregate:
* r_id needs to go into both Debt and Transaction
* number_of_lenders needs to go in Debt
"""

# UPDATE DEBT: put the j_id there
# TODO: this can be a function since the mechanism used is the same when updating a new dataset
# e.g. we use the same logic (and updated code) in Step 6 with REF_MA
debt['r_id'] = debt['r_id'].astype("string") # since there could be multiple j_ids then we want to keep them all
debt['r_id'] = debt['r_id'].fillna("")
for index_refl in matches_debt:
    # get the values to put in Debt: r_id and number_of_lenders
    r_id = ref_loan.iloc[index_refl]['r_id']
    num_lenders = ref_loan.iloc[index_refl]['number_of_lenders']
    # get the index of the matched debt row (we use the index to then assign to the row the j_id)
    index_debt = debt.loc[debt['debt_id'] == matches_debt[index_refl]].index.to_list()[0]
    # get the debt row just for convinience
    debt_row = debt.iloc[index_debt]
    # assign the r_id to the debt row
    # note: multiple IJ_G rows could match the same debt row so we need to take track of all their j_id
    # but different IJ_G could have the same j_id so we also don't want to repeat
    if debt_row['r_id'] == "": # add the first one
        debt.at[index_debt, "r_id"] = str(r_id)
    elif not str(r_id) in debt_row['r_id']: # if the j_id is not already there then add this id
        debt.at[index_debt, "r_id"] = str(r_id) + "; " + debt_row['r_id']
    # assign the number of lenders
    debt.at[index_debt, "number_of_lenders"] = num_lenders

# CREATE THE TRANSACTION AND INVESTOR ROWS: from the db_ij rows
# TODO: this can be a function since the mechanism used is the same when updating a new dataset
# e.g. we use the same logic (and updated code) in Step 6 with REF_MA

transaction_rows = []
investor_rows = []
for index_refl in matches_debt:
    refl_row = ref_loan.iloc[index_refl]
    tr_row = [matches_debt[index_refl], refl_row['debt_investor_name'], refl_row['debt_investment_amount'], "N", "N", refl_row['r_id']]
    in_row = [refl_row['debt_investor_name'], ""]
    transaction_rows.append(tr_row)
    investor_rows.append(in_row)
print(f"There are as many new transaction rows as matches: {len(transaction_rows) == len(matches_debt)}")
print(f"There are as many new investor rows as matches: {len(investor_rows) == len(matches_debt)}")

# REMOVE FROM DB_IJ: the rows that were matched
# TODO: this can be a function since the mechanism used is the same when updating a new dataset
# e.g. we use the same logic (and updated code) in Step 6 with REF_MA
new_debt = ref_loan.drop(list(matches_debt.keys()))
print(f"Dropping went fine: {new_debt.shape[0] == ref_loan.shape[0] - len(matches_debt)}")

"""## 4. Find matches with Power Plant"""

thrs_dict_pp, mtds_dict_pp, prmts_dict_pp = get_thresholds_methods_dicts(thresholds_df, methods_df, parameters_df, ref_loan_name, "Power Plant")

thrs_dict_pp

mtds_dict_pp

prmts_dict_pp

new_debt = my_reset_index(new_debt)

# prepare the pp_full dataset for the matching (it fixes up the year_range and then creates the upper and lower limit columns which are needed)
pp_full = prepare_pp_full_for_matching_commissioning_year(pp_full)

# 29.6s
# no. of matches in default settings: 44
matched_db_debt = find_matches_power_plant_or_fill_PP_key(new_debt, pp_full, "PLANTKEY_REFINITIVLOAN_",
                                                        thresholds_dict=thrs_dict_pp,
                                                    methods_dict=mtds_dict_pp, equity_or_debt="Debt", parameters_dict=prmts_dict_pp)

matched_db_debt.loc[~matched_db_debt['PP_key'].str.contains("REFINITIVLOAN")].sort_values(by="PP_key").tail()

pp_full.loc[pp_full['PP_key'].isin(matched_db_debt.loc[~matched_db_debt['PP_key'].str.contains("REFINITIVLOAN")]['PP_key'].values)].sort_values(by="PP_key").tail()

for name, db in [["Debt", matched_db_debt]]:
    print(name)
    print(f"Matches #: {db.loc[~(db['PP_key'].str.contains('REFINITIVLOAN'))].shape[0]}")
    print(f"Matches %: {db.loc[~(db['PP_key'].str.contains('REFINITIVLOAN'))].shape[0] / db.shape[0] * 100}\n")

# show example of the commissioning year
example = "PLANTKEY_WEPPGPPD_172340"
matched_db_debt.loc[matched_db_debt['PP_key'] == example]

pp_full.loc[pp_full['PP_key'] == example]

"""##### Check"""

# check that the matching went fine
# do some checks
matched = matched_db_debt
newly_created_name = "REFINITIVLOAN"

# check that all the entries now have a proper PP_key

# no. of matched power_plants
key_wepp = matched.loc[matched['PP_key'].str.contains("WEPPGPPD")].shape[0]
key_bucgp = matched.loc[matched['PP_key'].str.contains("BUCGP")].shape[0]
key_bucgef = matched.loc[matched['PP_key'].str.contains("BUCGEF")].shape[0]
key_sais = matched.loc[matched['PP_key'].str.contains("SAIS")].shape[0]
key_ijg = matched.loc[matched['PP_key'].str.contains("IJG")].shape[0]
key_rma = matched.loc[matched['PP_key'].str.contains("REFINITIVMA")].shape[0]
key_fdi = matched.loc[matched['PP_key'].str.contains("FDIMARKETS")].shape[0]


# no. of newly created keys, these are new entries
key_new = matched.loc[matched['PP_key'].str.contains(newly_created_name)].shape[0]

# check that matched keys got the PP_key from Power Plant
print(f"All matched keys got the PP_key from Power Plant: {key_wepp + key_bucgp + key_bucgef + key_sais + key_ijg + key_rma + key_fdi== matched.loc[~(matched['PP_key'].str.contains(newly_created_name))].shape[0]}")

# check that each row has a PP_key
print(f"Each row has a PP_key: {key_wepp + key_bucgp + key_bucgef + key_sais + key_ijg+ key_rma +  key_fdi + key_new == matched.shape[0]}")

pd.DataFrame(data=[["WEPP + GPPD", key_wepp], ["BUCGP", key_bucgp], ["BUCGEF", key_bucgef], ["SAIS_CLA + IAD_GEGI", key_sais],["IJGLOBAL", key_ijg], ["REFINITIVMA", key_rma],["FDIMARKETS", key_fdi], ["REFINITIVLOAN", key_new]], columns=['Data source', "Count"])

matched_keys = matched.loc[~matched['PP_key'].str.contains("REFINITIVLOAN")]['PP_key'].unique()
matched_keys[0:5]

# show example
key_sample = matched_keys[3]
matched_db_debt.loc[matched_db_debt['PP_key'] == key_sample]

pp_full.loc[pp_full["PP_key"] == key_sample]

# check the string distance
for func in [fuzz.partial_ratio, fuzz.token_set_ratio]:
    diffs = []
    for i, row in matched_db_debt.loc[~(matched_db_debt['PP_key'].str.contains("REFINITIVLOAN"))].iterrows():
        key_sample = row['PP_key']
        matched_row = pp_full.loc[pp_full["PP_key"] == key_sample]
        if row['power_plant_name'] != "" and matched_row['power_plant_name'].values[0] != "":
            diff = func(row['power_plant_name'], matched_row['power_plant_name'].values[0])
        diffs.append(diff)
    cleaned_diffs = [x for x in diffs if not np.isnan(x)]
    print(str(func).split(" ")[1])
    print(f"Mean: {mean(cleaned_diffs)}")
    print(f"Max: {max(cleaned_diffs)}")
    print(f"Min: {min(cleaned_diffs)}\n")

"""## 5. Create tables

#### 5.0 Create the emtpy tables
"""

db_structure = map_table[db_structure_sheet]
tables_df = get_tables_to_create_empty(db_structure)
# used as support for the creation
tables_df_tmp = get_tables_to_create_empty(db_structure)

"""#### 5.0 Deal with duplicates Power Plant and City info"""

from functions.cleaning import fix_duplicated_power_plants_keys

examples_duplicated = ['focusing on power generation',
 'an incineration plant',
 'kahalgaon superthermal power plants at bilaspur in chhatisgarh state bhagalpur in bihar state respectively']
matched_db_debt.loc[matched_db_debt['power_plant_name'].isin(examples_duplicated)].sort_values(by="power_plant_name")

matched_db_debt, diff = fix_duplicated_power_plants_keys(matched_db_debt,list(tables_df['Power Plant'].columns) + list(tables_df['City'].columns), "PLANTKEY_REFINITIVLOAN_")

matched_db_debt.loc[matched_db_debt['power_plant_name'].isin(examples_duplicated)].sort_values(by="power_plant_name")

"""#### 5.1 Add debt_id"""

# create unique ids with custom function
matched_db_debt = create_unique_id(matched_db_debt, "debt_id", "DEBTID_REFINITIVLOAN_")
matched_db_debt.head()

"""#### 5.2 Adding to the Debt table"""

# get the new entries
entity = debt_name
for column in tables_df_tmp[entity].columns:
    if column in matched_db_debt.columns:
        tables_df_tmp[entity][column] = matched_db_debt[column]

# concat
tables_df['Debt'] = pd.concat([debt, tables_df_tmp['Debt']])
# check
tables_df['Debt'].shape[0]  == debt.shape[0]  +  tables_df_tmp['Debt'].shape[0]

tables_df['Debt']

# all the rows with an r_id do have a valid number of lenders
tables_df['Debt'].loc[(tables_df['Debt']['r_id'] != "") & (~tables_df['Debt']['number_of_lenders'].isna())].shape[0] == tables_df['Debt'].loc[(tables_df['Debt']['r_id'] != "") ].shape[0]

# all the rows without an r_id do NOT have a valid number of lenders
tables_df['Debt'].loc[(tables_df['Debt']['r_id'].isna()) & (tables_df['Debt']['number_of_lenders'].isna())].shape[0] == tables_df['Debt'].loc[(tables_df['Debt']['r_id'].isna()) ].shape[0]

"""#### 5.3 Creating Transaction and Investor tables"""

# we need to rename some columns to match the columns in Investor and Transactio
# TODO: we have the same issue above with one of the columns name containing a comma, so we should make sure that if there are multiple
# columns names in the same row then we separeate them by something else (e.g., ";" or "|")
# as a quick fix I'll just manually add the renamings
# renamings = get_variables_advanced(vertical_slicing, ref_loan_name, ['Investor', "Transaction"])
renamings = {}
# the following two are not in the vertical_slicing (which comes from Variables.xlsx) because we created these two columns not from data coming from the sources (they are based
# on the index of the dataframes....)
renamings["debt_id"] = "investment_id"
renamings["debt_investor_name"] = 'investor_name'
renamings['debt_investment_amount'] =  'amount'
renamings

# do renaming
# Note: we need to filter because some renaimings rules only apply to eq_agg_full and some only to dt_agg_full
matched_db_debt_v2= matched_db_debt.rename(columns=dict(filter(lambda x: x[0] in matched_db_debt.columns, renamings.items())))
matched_db_debt_v2.head(2)

# get the new entries, use two supporting variables to fill them
transaction_new_debt = pd.DataFrame(columns=tables_df['Transaction'].columns)
entity = "Transaction"
for column in transaction_new_debt.columns:
    if column in matched_db_debt_v2.columns:
        transaction_new_debt[column] = matched_db_debt_v2[column]
transaction_new_debt["investment_averaged"] = "N"
transaction_new_debt["investment_weighted"] = "N"

# concat
transaction_dataframes = [pd.DataFrame(transaction_rows, columns=tables_df['Transaction'].columns), transaction_new_debt]
tables_df['Transaction'] = pd.concat(transaction_dataframes)

# get the new entries, use two supporting variables to fill them
entity = "Investor"
investor_new_debt = pd.DataFrame(columns=tables_df[entity].columns)

for column in investor_new_debt.columns:
    if column in matched_db_debt_v2.columns:
        investor_new_debt[column] = matched_db_debt_v2[column]

# concat
investor_dataframes = [pd.DataFrame(investor_rows, columns=tables_df['Investor'].columns), investor_new_debt]
tables_df['Investor'] = pd.concat(investor_dataframes)

"""#### 5.4 Make Power Plant and City and Country for the new power plants found here"""

# check that all entries in power plants have a PP_key
missing_power_plants = matched_db_debt_v2.loc[matched_db_debt_v2['PP_key'].str.contains("PLANTKEY_REFINITIVLOAN")]
print("Missing power plant: " + str(missing_power_plants.shape[0]))
print(f"check: {matched_db_debt_v2.shape[0] - matched_db_debt_v2.loc[~(matched_db_debt_v2['PP_key'].str.contains('PLANTKEY_REFINITIVLOAN'))].shape[0] == missing_power_plants.shape[0]}")

# get the data from equity
for entity in ['Power Plant', "Country"]:
    for column in tables_df[entity].columns:
        if column in missing_power_plants.columns:
            tables_df[entity][column] = missing_power_plants[column]

entity = "City"
for column in tables_df[entity].columns:
    if column in missing_power_plants.columns:
        tables_df[entity][column] = missing_power_plants[column]
    elif column == "city_key":
        tables_df[entity][column] = missing_power_plants["PP_key"]

# drop the duplicates that we fixed before
before_dropping = tables_df['Power Plant'].shape[0]
tables_df['Power Plant'] = tables_df['Power Plant'].drop_duplicates()
rows_dropped =  before_dropping - tables_df['Power Plant'].shape[0]
print(f"Rows dropped: {rows_dropped}")
print(f"Check: the rows we are dropping now are the same as the rows that we found as duplicated before: {rows_dropped == diff}")

before_dropping = tables_df['City'].shape[0]
tables_df['City'] = tables_df['City'].drop_duplicates()
rows_dropped =  before_dropping - tables_df['City'].shape[0]
print(f"Rows dropped: {rows_dropped}")
print(f"Check: the rows we are dropping now are the same as the rows that we found as duplicated before: {rows_dropped == diff}")

"""#### 5.5 Visual check"""

tables_df['Power Plant']

for col in ["power_plant_name", 'installed_capacity', "primary_fuel"]:
    print(f"Valid {col}: {tables_df['Power Plant'].loc[~(tables_df['Power Plant'][col].isna()) & (tables_df['Power Plant'][col] != '')].shape[0]}")

tables_df['City']

for col in ['city', "province"]:
    print(f"There are no {col}: {tables_df['City'][col].isna().sum() == tables_df['City'].shape[0]}")
col = "country"
print(f"All valid country values: {tables_df['City'].loc[~(tables_df['City'][col].isna()) & (tables_df['City'][col] != '')].shape[0] == tables_df['City'].shape[0]}")

tables_df['Country']

col = "country"
print(f"All valid country values: {tables_df['Country'].loc[~(tables_df['Country'][col].isna()) & (tables_df['Country'][col] != '')].shape[0] == tables_df['Country'].shape[0]}")

tables_df["Debt"]

# all the r_ids in Debt come from either the merging (matches_debt) or the new_debt rows (new_debt)
# needed to allow for the check
tables_df['Debt']['r_id'] = tables_df['Debt']['r_id'].fillna("")
tables_df['Debt']['r_id'] = tables_df['Debt']['r_id'].astype("string")

# check the debt rows from REF_LOAN
# there are as many rows in Debt that have an r_id and are from REF_LOAN as in the matched_db_debt from which we took the data from
print(f"All the Debt rows from REF_LOAN do have an ID: {len(matched_db_debt) == len(tables_df['Debt'].loc[(tables_df['Debt']['debt_id'].str.contains('REFINITIVLOAN')) & (~tables_df['Debt']['r_id'].isna())])}")

# check the debt rows that were updated with REF_LOAN's ids
# get the IDs that are in the updated rows
r_id_list = [[y.strip() for y in x.split(";") ]for x in tables_df['Debt'].loc[(~tables_df['Debt']['debt_id'].str.contains("REFINITIVLOAN")) & (tables_df['Debt']['r_id'] != "")]['r_id']] # we need to split because there could be more than one r_id per row
r_id_list = list(itertools.chain(*r_id_list)) # merge the list of lists into a single list
while "" in r_id_list: # there is an empty string if there was no r_id in the row
    r_id_list.remove("")
# we now compare that there are as many unique IDs in Debt as in the original ref_loan rows that we matched
print(f"The IDs of the matched investment ref_loan rows are in Debt: {len(set(r_id_list)) == ref_loan.iloc[list(matches_debt.keys())]['r_id'].nunique()}")

# all the original r_ids are in Debt
r_id_list = [[y.strip() for y in x.split(";") ]for x in tables_df['Debt']["r_id"]] # we need to split because there could be more than one r_id per row
r_id_list = list(itertools.chain(*r_id_list)) # merge the list of lists into a single list
while "" in r_id_list: # there is an empty string if there was no r_id in the row
    r_id_list.remove("")

original_ids = [str(x) for x in ref_loan['r_id'].unique()]
print(f"All the original IDs are in Debt: {len(set(r_id_list) & set(original_ids)) == len(set(original_ids))}")

# check that all the new entries in Debt (the ones that were not matched) do have the r_id
new = tables_df['Debt'].loc[tables_df['Debt']['debt_id'].str.contains("REFINITIVLOAN")]
new.loc[~(new['r_id'].isna()) & (new['r_id'] != "")].shape[0] == new.shape[0]

# check that Only the rows with an r_id hava a number_of_lenders
print(f"All the number_of_lenders come from REFINITIV_LOAN: {tables_df['Debt']['number_of_lenders'].isna().sum() == tables_df['Debt'].loc[tables_df['Debt']['r_id'] ==''].shape[0]}")

tables_df['Transaction']

print(f"All valid investement_id: {tables_df['Transaction']['investment_id'].isna().sum() == 0}")

# all the transactions do have the r_id
tables_df['Transaction'].loc[~tables_df['Transaction']['r_id'].isna()].shape[0] == tables_df['Transaction'].shape[0]

# there are as many transaction rows as the rows stemming from the debt matching and the new_debt
len(tables_df['Transaction']) == len(matches_debt) + len(new_debt)

tables_df['Investor']

# there are no parent companies
tables_df['Investor'].loc[~(tables_df['Investor']['parent_company_of_investor'].isna()) & (tables_df['Investor']['parent_company_of_investor'] != "")].shape[0] == 0

"""#### 5.6 Save"""

print_final_statistics(tables_df, tables_to_create)

# all entities but Debt get saved in the intermediate folder
for entity in ['Power Plant', 'City', 'Country', 'Transaction', 'Investor']:
    tables_df[entity].to_csv(intermediate_folder + entity + "_REFINITIVLOAN.csv", index=False)

# since Debt is ready and doesn't need any further post-processing, we can save it here
entity = "Debt"
current_final_tables_folder + entity + ".csv"

tables_df['Debt'].to_csv(current_final_tables_folder + entity + ".csv", index=False)

