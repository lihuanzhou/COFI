# -*- coding: utf-8 -*-
"""Step FINAL - ZZ_2 - Create flat file.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HUpWCRjskc_DvvY7tbrdnoZDBdDMcCgr
"""

import pandas as pd
import numpy as np

from functions.joining import join_pp_until_country
from functions.cleaning import my_read_excel
from functions.cleaning import my_reset_index
from functions.cleaning import fix_investor_names
from functions.cleaning import count_sources_in_id

# variables
output_folder = "Current Final Tables/"

# already exising final data
# power_plant_file = "Power Plant.xlsx"
# city_file = "City.xlsx"
# country_file = "Country.xlsx"
# bridge_file = "CITYKEY_BRIDGE_PP_C.xlsx"
# equity_file = "Equity.xlsx"
# transaction_file = "Transaction.xlsx"
# debt_file = "Debt.xlsx"
# investor_file = "Investor.xlsx"
power_plant_file = "Power Plant.csv"
city_file = "City.csv"
country_file = "Country.csv"
bridge_file = "CITYKEY_BRIDGE_PP_C.csv"
equity_file = "Equity.csv"
transaction_file = "Transaction.csv"
debt_file = "Debt.csv"
investor_file = "Investor.csv"

# load the excel containing all the mapping information
map_table_name = "Variables.xlsx"
map_table = pd.read_excel(map_table_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(map_table))
db_structure_sheet = "db structure"

"""Steps:
1. Join the power plant info re-using what's already there
1. Join the equity and debt table until investor and keep them separate
1. Join power plant with equity and debt and keep them seperate
    * here we only keep those entries in power plant that do have a match in equity and debt
1. concat the two previous tables

Note: some of the tables may contain columns that were there in old formats, but are not there anymore, so we will take this into account. The final output will be with the current db structure.
"""

# load the structure of the database
db_structure = map_table[db_structure_sheet]
tables_variables = {}
for table in db_structure['Name of new table'].unique():
    if table not in tables_variables:
        tables_variables[table] = [x.strip() for x in db_structure.loc[db_structure['Name of new table'] == table]['Variables'].to_list()]
tables_variables

"""#### 1. Join power plant info"""

pp_full = join_pp_until_country(new_output_folder=output_folder)
keys = [x.split("_")[1] for x in pp_full['PP_key'].values]
print(f"FIRST JOIN , {'AIDDATA' in keys}")

# check which columns in pp_full are not present in the current db structure
columns_pp_to_keep = tables_variables['Power Plant'] + tables_variables['City'] + tables_variables['Country']

[x for x in pp_full.columns if x not in columns_pp_to_keep]
# bu_id: it's fine, it used to be in Power Plant but it was moved

# remove the unneeded columns
if "bu_id" in pp_full.columns:
    pp_full = pp_full.drop(columns=['bu_id'])

# for WEPP+GPPD: save the flat file of these tables since there is no Debt, Equity, etc. table
# pp_full[tables_variables['Power Plant'] + tables_variables['City'] + tables_variables['Country'][1:]].to_excel(output_folder + "Flat file.xlsx", index=False)

count_sources_in_id(pp_full, "PP_key")

"""#### 2. Join equity and debt until investor"""

output_folder

equity = my_read_excel(output_folder, equity_file)
debt = my_read_excel(output_folder, debt_file)
transaction = my_read_excel(output_folder, transaction_file)
investor = my_read_excel(output_folder, investor_file)


print("EQUIIITTYYYY")
count_sources_in_id(equity, "equity_id")
print("---------------")
print("DEEEBBBTTTTTT")
count_sources_in_id(debt, "debt_id")
print("---------------")
print("TRANSACTIONNNNN")
count_sources_in_id(transaction, "investment_id")
print("---------------")

# check if there are old columns
for table_name, dataset in [['Equity', equity], ["Debt", debt], ["Transaction", transaction], ["Investor", investor]]:
    print(f"{table_name}: {[x for x in dataset.columns if x not in tables_variables[table_name]]}")

# original_source is interesting and to be kept but change name
transaction = transaction.rename(columns={"original_source": "source_transaction_info"})

if "source" in debt.columns:
    debt = debt.drop(columns=["source"])

# check that r_id has some valid values
print(transaction['r_id'].isna().sum() < transaction.shape[0])
print(debt['r_id'].isna().sum() < debt.shape[0])

# # only keep the correct columns (add the missing columns if needed)
# equity = equity.drop(columns=['equity_investment_weighted'])
# debt = debt.drop(columns=['new_data_id'])

# join from equity till investor

# equity & transaction
equity_tr = equity.merge(transaction, left_on="equity_id", right_on="investment_id")
print(f"Joining Equity and Transaction, all Equity entries had matching transaction: {equity_tr.shape[0] >= equity.shape[0]}")
# Note: when joining there could be more entries in Transaction for one Equity entry, so that's why we use the ">="

# add investor
# pre-process the investor by removing excess white spaces
equity_tr['investor_name'] = equity_tr['investor_name'].apply(lambda x: x.strip())
investor['investor_name'] = investor['investor_name'].apply(lambda x: x.strip())
# join
equity_full = equity_tr.merge(investor, left_on="investor_name", right_on = "investor_name")
print(f"Joining Equity, Transaction, and Investor, all Equity entries had matching transaction: {equity_full.shape[0] == equity_tr.shape[0]}")
# Note: here each equity only has one match in Investor, so when checking there we can use "==" (and not ">=")

# change name to the r_id
debt = debt.rename(columns={"r_id": "r_id_debt"})
transaction = transaction.rename(columns={"r_id": "r_id_transaction"})

# join from debt till investor

# debt & transaction
debt_tr = debt.merge(transaction, left_on="debt_id", right_on="investment_id")
print(f"Joining Debt and Transaction, all Equity entries had matching transaction: {debt_tr.shape[0] >= debt.shape[0]}")
# Note: when joining there could be more entries in Transaction for one Equity entry, so that's why we use the ">="



# add investor
# pre-process the investor by removing excess white spaces
debt_tr['investor_name'] = debt_tr['investor_name'].apply(lambda x: x.strip())
investor['investor_name'] = investor['investor_name'].apply(lambda x: x.strip())
# join
debt_full = debt_tr.merge(investor, left_on="investor_name", right_on = "investor_name")
print(f"Joining Debt, Transaction, and Investor, all Debt entries had matching transaction: {debt_full.shape[0] == debt_tr.shape[0]}")
# Note: here each equity only has one match in Investor, so when checking there we can use "==" (and not ">=")

"""#### 3. Join power plant info and equity and debt info"""

equity_pp = equity_full.merge(pp_full, left_on="PP_key", right_on="PP_key", how="left")
print(f"Joining Power Plant and Equity info, all Equity entries had matching power plants: {equity_full.shape[0] == equity_pp.shape[0]}")

debt_pp = debt_full.merge(pp_full, left_on="PP_key", right_on="PP_key", how="left")
print(f"Joining Power Plant and Equity info, all Equity entries had matching power plants: {debt_full.shape[0] == debt_pp.shape[0]}")

"""#### 4. Concat everything and order data"""

debt_pp.reset_index(inplace=True, drop=True)
equity_pp.reset_index(inplace=True, drop=True)

final_db = pd.concat([debt_pp, equity_pp])
print(f"Concat all data, all entries kept: {final_db.shape[0] == debt_pp.shape[0] + equity_pp.shape[0]}")

# create a new column that unites the equity and debt investment year so it is easier to order the data
final_db['investment_year'] = final_db.apply(lambda row: row['debt_investment_year'] if not np.isnan(row['debt_investment_year']) else row['equity_investment_year'], axis=1)

"""We now order in a way that faciliates the checking of the results:
* since the commissioning year has been problematic, we want to keep close the power plants that are related to one another (that's why we order for location_id, primary_fuel, and commissioning_year) so to faciliate spotting the issues for them.
* we want the investment on the same plant to be close by (that's why we use "PP_key") and in a chronological order regardless of the kind of investment ("investement_year").

In this way, the rows that have the same location_id+primary_fuel pair are close by regardless to the fact that they have been matched to different datasets. Examples of location_id: 1015379, 1023816.
"""

# order
# final_db = final_db.sort_values(by=['PP_key', "investment_year"]) # old way
final_db = final_db.sort_values(by=['location_id', "primary_fuel", "commissioning_year", "PP_key", "investment_year"])
final_db = final_db.reset_index()
final_db = final_db.drop(columns=['index'])

final_db.head(20)

final_db['source_transaction_info'].isna().sum()

final_db['source_transaction_info'].value_counts()

# get the list of columns from the db structure and get them in the order that is there without duplicates
final_columns_order = []
for var in db_structure['Variables'].to_list():
    if var not in final_columns_order:
        final_columns_order.append(var.strip())
# add the source transaction column
final_columns_order = final_columns_order + ['source_transaction_info', "r_id_debt", "r_id_transaction"]
final_columns_order

final_db = final_db[final_columns_order]
final_db.head()

# check that all columns are in final_db
[x for x in final_columns_order if x not in final_db.columns]

# final_db['equity_id'] = final_db['equity_id'].fillna("")
# final_db['debt_id'] = final_db['debt_id'].fillna("")
# for id in final_db['PP_key'].unique():
#     tmp = final_db.loc[final_db['PP_key'] == id]
#     if tmp.shape[0] > 2:
#         eq_ids = [x for x in tmp['equity_id'].to_list() if x != ""]
#         debt_ids = [x for x in tmp['debt_id'].to_list() if x != ""]
#         if len(eq_ids) > 0 and len(debt_ids) > 0:
#             print(id)

# last fix
final_db = final_db.drop(columns=['investment_id'])

"""#### 5. determine for final_db : for each row why it is relevant"""

final_db['debt_id'] = final_db['debt_id'].fillna("")
final_db['equity_id'] = final_db['equity_id'].fillna("")

final_db[['PP_key', "debt_id", "equity_id", "source_transaction_info"]].head(2)

def extract_between_underscores(id_text):
    if "SAIS" not in id_text:
        return id_text.split("_")[1].strip()
    return "SAISCLA_IADGEGI"

def get_investment_source_info(row):
    if row['debt_id'] != "":
        source_invst = extract_between_underscores(row['debt_id'])
    elif row['equity_id'] != "":
        source_invst = extract_between_underscores(row['equity_id'])
    else:
        print("ERORR! No investment information present!")
        source_invst = None

    return source_invst

final_db['reason_of_interest'] = ""

final_db['source_transaction_info'].unique()

# BU_CGEF: power plant matching
for i, row in final_db.loc[(~final_db['PP_key'].str.contains("BUCGEF") & (final_db['debt_id'].str.contains("BUCGEF")))].iterrows():
    final_db.at[i, "reason_of_interest"] = "BU_CGEF - power plant matching"

# SAIS_CLA+IAD_GEGI: power plant matching
for i, row in final_db.loc[(~final_db['PP_key'].str.contains("SAIS") & (final_db['debt_id'].str.contains("SAIS")))].iterrows():
    final_db.at[i, "reason_of_interest"] = "SAIS_CLA+IAD_GEGI - power plant matching"

# IJGLOBAL: matching debt
for i, row in final_db.loc[(~final_db['debt_id'].str.contains("IJGLOBAL") & (final_db['debt_id'] != "") & (final_db['source_transaction_info'] == "IJ_Global"))].iterrows():
    final_db.at[i, "reason_of_interest"] = "IJGLOBAL - investment matching"

# IJGLOBAL: power plant matching (debt rows)
for i, row in final_db.loc[(~final_db['PP_key'].str.contains("IJGLOBAL") & (final_db['debt_id'].str.contains("IJGLOBAL")))].iterrows():
    final_db.at[i, "reason_of_interest"] = "IJGLOBAL - power plant matching"

# IJGLOBAL: power plant matching (equity rows)
for i, row in final_db.loc[(~final_db['PP_key'].str.contains("IJGLOBAL") & (final_db['equity_id'].str.contains("IJGLOBAL")))].iterrows():
    final_db.at[i, "reason_of_interest"] = "IJGLOBAL - power plant matching"

# REFINITIVMA: matching equity
for i, row in final_db.loc[(~final_db['equity_id'].str.contains("REFINITIVMA") & (final_db['equity_id'] != "") & (final_db['source_transaction_info'] == "REFINITIVMA"))].iterrows():
    final_db.at[i, "reason_of_interest"] = "REFINITIVMA - investment matching"

# REFINITIVMA: power plant matching
for i, row in final_db.loc[(~final_db['PP_key'].str.contains("REFINITIVMA") & (final_db['equity_id'].str.contains("REFINITIVMA")))].iterrows():
    final_db.at[i, "reason_of_interest"] = "REFINITVMA - power plant matching"

# FDIMARKETS: power plant matching (custom logic)
for i, row in final_db.loc[(~final_db['PP_key'].str.contains("FDIMARKETS") & (final_db['equity_id'].str.contains("FDIMARKETS")))].iterrows():
    final_db.at[i, "reason_of_interest"] = "FDIMARKETS - power plant matching (custom logic)"

# REFINITIVLOAN: matching debt
for i, row in final_db.loc[(~final_db['debt_id'].str.contains("REFINITIVLOAN") & (final_db['debt_id'] != "") & (final_db['source_transaction_info'] == "REFINITIVLOAN"))].iterrows():
    final_db.at[i, "reason_of_interest"] = "REFINITIVLOAN - investment matching"

# REFINITIVLOAN: power plant matching
for i, row in final_db.loc[(~final_db['PP_key'].str.contains("REFINITIVLOAN") & (final_db['debt_id'].str.contains("REFINITIVLOAN")))].iterrows():
    final_db.at[i, "reason_of_interest"] = "REFINITVLOAN - power plant matching"

final_db['reason_of_interest'].value_counts()

"""#### 6. Drop non-chinese debt investors

We noticed that there were some banks that were not properly cleaned because the dictionary was not updated: we updated it and now we clean the names again.
"""

# the banks names that were not cleaned
still_wrong = ['icbc asia', 'china co financing fund for latin america and the caribbean', 'china co financing fund', 'bank of china hong kong ltd', 'bank of china manila', 'people s bank of china', 'china constr bk asia ltd', 'agricultural bank of china hk', 'bank of china ltd paris branch', 'industrial coml bk of china', 'bank of china hong kong branch']

final_db.loc[final_db['investor_name'].isin(still_wrong)][['debt_id', "investor_name"]]

# clean
final_db = fix_investor_names(final_db, "investor_name", False, None, drop_duplicates=False)

# note: "china co financing fund for latin america and the caribbean" is the correct name, we just didn't put it in the dictionary in the first place
final_db.loc[final_db['investor_name'].isin(still_wrong)][['debt_id', "investor_name"]]["investor_name"].unique()

# TODO: chekc that when re-running there are no cofinancing anymore because these have been dealt with!!

"""We have a file that contains a dictioanry to use to rename chinese banks correctly. We can use it to filter out all the non-chinese investors."""

# # the dictionary to use
# custom_bank_dictionary = pd.read_excel("custom_investor_dictionary_final.xlsx")
# # the dictionary cotnains also some equtiy investros for conveninence, we can exclude them here
# custom_bank_dictionary = custom_bank_dictionary.loc[custom_bank_dictionary['Equity investor'] == False]
# custom_bank_dictionary.head()

"""TEMPORARLY WE DO NOT DO THIS: We keep all the rows that have an investor whose name is in the dictionary. We take into account both the non-standard name (the "Old" colum) and the standard name ("New") in case by any chance some names were not converted throughout the whole process."""

# # get all the possible bank names
# names_to_include = set(list(custom_bank_dictionary['Old'].unique()) + list(custom_bank_dictionary['New'].unique()))

# # needed to count a statistics
# original_debt_rows_no = final_db.loc[(final_db['debt_id'] != "")].shape[0]

# # Drop the rows that don't have an investor in the dictionary
# # since we are dropping, it is good to reset the index
# final_db = my_reset_index(final_db)
# # get the names that we are dropping
# to_drop_names = final_db.loc[(final_db['debt_id'] != "") & (~final_db['investor_name'].isin(names_to_include))]['investor_name'].unique()
# # get the index of the rows we have to drop
# to_drop_index = final_db.loc[(final_db['debt_id'] != "") & (~final_db['investor_name'].isin(names_to_include))].index.to_list()
# # drop
# final_db = final_db.drop(to_drop_index)

# # print statistics
# print(f"There aren't any non-chinese lenders left: {final_db.loc[(final_db['debt_id'] != "") & (~final_db['investor_name'].isin(names_to_include))].shape[0] == 0}")
# print(f"Rows dropped #: {len(to_drop_index)}")
# print(f"Rows dropped % over all debt rows: {len(to_drop_index) / original_debt_rows_no * 100}\n")
# print(f"Debt investors dropped: {to_drop_names}")

"""WHAT WE DO: WE HAVE A LIST OF WELL-KNOWN NON-CHINESE LENDERS. we chose this approach because there were some chinese investors as lenders that were not banks. We will ask WRI to see if they are interested in keeping these."""

# load the table
non_chinese_lenders = pd.read_excel("non chinese investors.xlsx")
non_chinese_lenders

non_chinese_lenders_list = non_chinese_lenders['investor_name'].to_list()

original_shape = final_db.shape[0]
final_db = final_db.loc[~final_db['investor_name'].isin(non_chinese_lenders_list)]
print(f"Rows dropped #: {(original_shape - final_db.shape[0])}")
print(f"Rows dropped %: {(original_shape - final_db.shape[0]) / final_db.shape[0] * 100}")

"""#### 7. Save"""

# check: the columns below it makes sense that they don't have any value at all
# this is because either I didn't compute them yet or new
for col in final_db.columns:
    if final_db[col].isna().sum() == final_db.shape[0]:
        print(col)

if "r_id" in final_db.columns:
    final_db = final_db.drop(columns=["r_id"])

# save in csv
final_db.to_csv(output_folder + "Flat file.csv", index=False)

# save in excel
final_db.to_excel(output_folder + "Flat file - Excel.xlsx", index=False)

import pandas as pd

# # save power plant in an excel format for checking purposes
# power_plant = pd.read_csv(output_folder + "Power Plant.csv")
# # 1m2.1s
# power_plant.to_excel(output_folder + "Power Plant.xlsx")

"""#### 8. Get the power plants that are not in final_db"""

missing_pp_ids = []
for id in pp_full['PP_key'].unique():
    if id not in final_db['PP_key'].unique():
        missing_pp_ids.append(id)

pp_full.loc[pp_full['PP_key'].isin(missing_pp_ids)].shape[0] == len(missing_pp_ids)

final_db['PP_key'].nunique()

pp_full.loc[pp_full['PP_key'].isin(missing_pp_ids)].to_csv(output_folder + "Power Plants without investments.csv", index=False)

