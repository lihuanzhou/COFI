# -*- coding: utf-8 -*-
"""Step 1 - WEPP and GPPD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t5mhr4Sfai0-FK4g9p3cUUjOJGk3fE0K
"""

import pandas as pd
import numpy as np
import itertools

from functions.cleaning import get_variables, create_placeholder, convert_primary_fuel, create_unique_id, my_reset_index
from functions.groupby import read_agg_info, do_groupby
from functions.final_tables_creation import get_tables_to_create_empty
from functions.cleaning import print_final_statistics

from functions.cleaning import my_read_excel, my_reset_index
from functions.cleaning import check_if_new_country_names, update_country_names
from functions.cleaning import convert_primary_fuel
from functions.cleaning import create_placeholder

from functions.cleaning import clean_primary_fuel_column, get_joining_info

# needed folder paths
input_folder = "Cleaned Data/" # or we can put this info in the "files name" sheet
back_up_folder = "Backup Copies/" # to save some byproducts of this analysis that can be nice to have (e.g., the aggregated WEPP results)
intermediate_folder = "Intermediate Results/" # to save some intermediate results
output_folder = "Final tables/" # where we store the final tables (e.g. Power Plant)

# load the excel containing all the mapping information
map_table_name = "Variables.xlsx"
map_table = pd.read_excel(map_table_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(map_table))

# load the excel containing all the mapping information
dictionary_fuel_name = "Dictionaries.xlsx"
dictionary_fuel = pd.read_excel(dictionary_fuel_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(dictionary_fuel))


# the sheet where there is the information that the client can change
files_names_sheet = "file names"
groupby_info_sheet = "formatting"
vertical_slicing_info_sheet = "variables to use"
aggregation_info_sheet = "aggregation"
db_structure_sheet = "db structure"
un_country_info_sheet = "UN Country Info"
country_names_dict_sheet = "country names dicionary"
bri_countries_sheet = "BRI countries"
joining_info_sheet = "joining"
commissioning_year_thresholds_info_sheet = "commissioning_year_thresholds"

# the tables we are creating here
tables_to_create = ["Power Plant", "City", "Country"]

# names of the databases used here
wepp_name = "WEPP"
gppd_name = "GPPD"

"""## 1. Read data

#### 1.1 Read info table
"""

# this table contains the name of the data sources
file_names = map_table[files_names_sheet]
file_names

"""#### 1.2 Read data"""

wepp = my_read_excel(input_folder, file_names.loc[file_names['Database'] == wepp_name]["File name"].values[0])
print(f"WEEEEPPPP , {wepp.shape}")
# safe copy
wepp_full = wepp.copy(deep=True)

wepp.head(2)

gppd = my_read_excel(input_folder, file_names.loc[file_names['Database'] == gppd_name]["File name"].values[0])
print(f"GGPPPP , {gppd.shape}")
# safe copy
gppd_full = gppd.copy(deep=True)

gppd.head(2)

"""## 2. Vertical slicing and Cleaning

#### 2.1 Vertical slicing
"""

[print(key) for key in map_table.keys()]



print(vertical_slicing_info_sheet == "variables to use") 

vertical_slicing = map_table[vertical_slicing_info_sheet]
print(vertical_slicing.shape)
print(vertical_slicing.head(5))

# read only the variables that refer to the tables that we are creating here & that refer to wepp
wepp_variables = get_variables(vertical_slicing, wepp_name, tables_to_create )
wepp_variables

# read only the variables that refer to the tables that we are creating here & that refer to wepp
gppd_variables = get_variables(vertical_slicing, gppd_name, tables_to_create )
gppd_variables

wepp = wepp[wepp_variables]
wepp.head(2)

gppd = gppd[gppd_variables]
gppd.head(2)

"""#### 2.2 Cleaning columns"""

# all the IDs are there
wepp.isna().sum()

"""##### installed_capacity (shouldn't have been already dealt with at the cleaning step)"""

gppd['installed_capacity']

# TODO: I shouldn't do this here

gppd['installed_capacity'] = gppd['installed_capacity'].apply(lambda x: x.replace("MW", "").strip())
gppd['installed_capacity'] = gppd['installed_capacity'].astype("float64")

gppd['installed_capacity']

wepp['installed_capacity']

# TODO: I shouldn't do this here

wepp['installed_capacity'] = wepp['installed_capacity'].apply(lambda x: x.replace("MW", "").strip())
wepp['installed_capacity'] = wepp['installed_capacity'].astype("float64")

wepp['installed_capacity']

"""##### Primary fuel"""

wepp['primary_fuel'].unique()

# make conversion
wepp = clean_primary_fuel_column(wepp)

wepp['primary_fuel'].unique()

gppd['primary_fuel'].unique()

# make conversion
gppd = clean_primary_fuel_column(gppd)

gppd['primary_fuel'].unique()

"""Since we later need to aggregate based on the primary fuel, we need to keep the empty fuels seperate. Therefore we now create a placeholder for the primary fuels and then we will remove it after the aggregation."""

for i, row in wepp.loc[wepp['primary_fuel'].isna()].iterrows():
    wepp.at[i, "primary_fuel"] = create_placeholder(row, "primary_fuel", prefix = "missing_fuel_wepp", unique_number = i)

for i, row in gppd.loc[gppd['primary_fuel'].isna()].iterrows():
    gppd.at[i, "primary_fuel"] = create_placeholder(row, "primary_fuel", prefix = "missing_fuel_gppd_", unique_number = i)

"""##### Country"""

# needed to run the functions to do the converting
country_names_dict_df = map_table[country_names_dict_sheet]
un_data = map_table[un_country_info_sheet]

# pre-process
wepp['country'] = wepp['country'].apply(lambda x: x.lower().strip())
gppd['country'] = gppd['country'].apply(lambda x: x.lower().strip())

# check if there are names that are not standard or that are not in the dictionary to convert the country names
for db in [wepp, gppd]:
    print(check_if_new_country_names(list(db['country'].unique()), un_data, country_names_dict_df))

# convert the names
wepp = update_country_names(country_names_dict_df, wepp)
gppd = update_country_names(country_names_dict_df, gppd)

"""##### Fix those entries in GPPD that don't have a wepp_id

There are some columns in GPPD that don't have a wepp_id. Since we want to aggregate the GPPD file, we need to have a placeholder for them because otherwise they will be lost.
"""

print("Empty wepp_ids: "+ str(gppd['wepp_id'].isna().sum()))
print("%: " + str(gppd['wepp_id'].isna().sum() / gppd.shape[0] * 100))

# reset index just to be sure
gppd = my_reset_index(gppd)

# apply
for index, row in gppd.iterrows():
    gppd.at[index, "wepp_id"] = create_placeholder(row, "wepp_id", "unknown_GPPD_wepp_id_", index)

gppd.loc[gppd['wepp_id'].str.contains("_")].head()

# proof that the newly created wepp_ids are unique
gppd.loc[gppd['wepp_id'].str.contains("_")]['wepp_id'].nunique() == len(gppd.loc[gppd['wepp_id'].str.contains("_")])

"""#### 2.3 Create temporary placeholders for the year"""

wepp['commissioning_year'] = wepp['commissioning_year'].fillna("")
wepp['commissioning_year'] = wepp['commissioning_year'].astype("string")

gppd['commissioning_year'] = gppd['commissioning_year'].fillna("")
gppd['commissioning_year'] = gppd['commissioning_year'].astype("string")

print(f"Missing year in WEPP: {wepp.loc[wepp['commissioning_year'] == ''].shape[0]}")
print(f"Missing year in GPPD: {gppd.loc[gppd['commissioning_year'] == ''].shape[0]}")

for i, row in wepp.loc[wepp['commissioning_year'] == ""].iterrows():
    wepp.at[i, "commissioning_year"] = create_placeholder(row, "commissioning_year", prefix = "missing_year_wepp_", unique_number = i)

for i, row in gppd.loc[gppd['commissioning_year'] == ""].iterrows():
    gppd.at[i, "commissioning_year"] = create_placeholder(row, "commissioning_year", prefix = "missing_year_gppd_", unique_number = i)

print(f"Missing year in WEPP but now with placeholder: {wepp.loc[wepp['commissioning_year'].str.contains('missing')].shape[0]}")
print(f"Missing year in GPPD but now with placeholder: {gppd.loc[gppd['commissioning_year'].str.contains('missing')].shape[0]}")

for db in [wepp, gppd]:
    all_missing = db.loc[db['commissioning_year'].str.contains("missing")]
    print(f"All unique placeholders: {all_missing.shape[0] == all_missing['commissioning_year'].nunique()}")

wepp['commissioning_year']

# functions to check that in the dbs there are missing nans in the same location_id+primary_fuel group when there are other valid values

def check_commissioning_year_wepp(loc_id, pr_fuel):
    group = wepp.loc[(wepp['location_id'] == loc_id) & (wepp['primary_fuel'] == pr_fuel)]
    if group.shape[0] == 1:
        return False
    if group['commissioning_year'].nunique() > 0:
        return True
    return False

def check_commissioning_year_gppd(loc_id, pr_fuel):
    group = gppd.loc[(gppd['wepp_id'] == loc_id) & (gppd['primary_fuel'] == pr_fuel)]
    if group['commissioning_year'].nunique() > 0:
        return True
    return False

"""#### 2.4 Fix dtypes

Since now there are wepp_ids in GPPD that are a string (the unknowns wepp_ids have been transformed in unique "unknown_id_XX"), we covert the WEPP's "location_id" in strings so that we can do the join
"""

wepp['location_id'] = wepp['location_id'].astype("string")

gppd['wepp_id'] = gppd['wepp_id'].astype("string")

gppd[gppd['wepp_id'].str.contains(r"\|")].head(10)

"""## 3. Groupby"""

wepp.loc[wepp['location_id'] == "1114791"]

gppd.loc[gppd['wepp_id'] == "1010536"]

"""#### 3.1 Do groupby

Steps:
1. Get the ids that need special fixing.
    * <i><b>function to use: get_pairs_to_fix(db, loc_id_col_name)</i></b>
2. Seperate these ids. We will get:
    * SPECIAL: the IDs that need the special grouping.
        * Of these still remove the rows that have "missing" in the commissioning year and put them in the other group.
    * NORMAL: the IDs that need the normal grouping (so directly groupby on location_id, primary_fuel, and then commissioning_year as we did before).
    * <i><b>function to use: seperate_rows(db, pairs_to_fix, loc_id_col_name)</i></b>
3. Apply the grouping
    * NORMAL: do the normal grouping (directly groupby on location_id, primary_fuel, and then commissioning_year as we did before)
        * <i><b>function to use: do_groupby_new( db, groupby_columns, name,agg_info)</i></b>
    + SPECIAL:
        1. use the function that creates a new copy of the rows to do the clustering
            * <i><b>function to use: do_clustering</i></b>
        1. then do the groupby on the clusters instead of the normal commissioning_year
            * make sure to put the lowest year as cluster representative
                * this can be achieved by putting in Variables.xlsx's aggregation sheet min for commissioning_year.
            * <i><b>function to use. do_groupby_new( db, groupby_columns, name,agg_info)</i></b>
4. Put everything back together.


Ways to improve:
* do the normal group on all data first -> therefore we can already get together those rows that have the same year and do not bring extra rows in the clustering.
    * e.g., instead of having "2020, 2021, 2021, 2021" we only have "2020, 2021". Less rows -> less time to do the clustering
* we can have custom rows to do the merging of the loc_id+primary_fuel pairs that only have two rows because we do not need to do a cluster analysis but just compare directly the two years together by looking at the difference in year:
    * e.g., if the difference > threshold, then do not cluster, if less then cluster.
* Therefore, the new steps would be:
    1. Do normal grouping
    2. Idetnify the pairs to fix, seperate those with 2 rows vs 2+ rows
    3. Seperate the rows that need fixing
    4. Fix these rows:
        1. cluster:
            * custom function: 2 rows
            * clustering: 2+ rows
        2. then do the grouping
    5. put them back together
"""

commissioning_year_thresholds_info = map_table[commissioning_year_thresholds_info_sheet]
commissioning_year_thresholds_info

from functions.cleaning import get_comm_year_thr

YEAR_THRESHOLD_AGG_GPPD = get_comm_year_thr(commissioning_year_thresholds_info, "Aggregating GPPD")
YEAR_THRESHOLD_AGG_GPPD

YEAR_THRESHOLD_AGG_WEPP = get_comm_year_thr(commissioning_year_thresholds_info, "Aggregating WEPP")
YEAR_THRESHOLD_AGG_WEPP

"""##### A. Study"""

from functions.groupby import get_pairs_to_fix

gppd_pairs_to_fix = get_pairs_to_fix(gppd, "wepp_id")
gppd_pairs_to_fix

wepp_pairs_to_fix = get_pairs_to_fix(wepp, "location_id")
wepp_pairs_to_fix

"""##### B. Seperate"""

from functions.groupby import seperate_rows

wepp_special, wepp_normal, wepp_pairs_to_fix = seperate_rows(wepp, wepp_pairs_to_fix, "location_id")

gppd_special, gppd_normal, gppd_pairs_to_fix = seperate_rows(gppd, gppd_pairs_to_fix, "wepp_id")

"""##### C. Do the actual groupby"""

from functions.groupby import do_groupby_new

aggregation_info = map_table[aggregation_info_sheet]

"""<b>Normal</b>"""

# 13.5s
gppd_normal_agg = do_groupby_new(gppd_normal, ["wepp_id", "primary_fuel", "commissioning_year"], gppd_name, aggregation_info)
print(f"Check: the rows have been aggregated correctly: {gppd_normal_agg.shape[0] <= gppd_normal.shape[0]}")

# 1m 29.6s
wepp_normal_agg = do_groupby_new(wepp_normal, ["location_id", "primary_fuel", "commissioning_year"], wepp_name, aggregation_info)
print(f"Check: the rows have been aggregated correctly: {wepp_normal_agg.shape[0] <= wepp_normal.shape[0]}")

# show example of the aggregation of the installed capacity
example = "1116575 solar"
wepp_normal.loc[wepp_normal['full_index'] == example]

wepp_normal_agg.loc[wepp_normal_agg['full_index'] == example]

# show example of the aggregation of the installed capacity
example = "1094293 wind"
gppd_normal.loc[gppd_normal['full_index'] == example]

gppd_normal_agg.loc[gppd_normal_agg['full_index'] == example]

gppd_normal_agg['installed_capacity']

wepp_normal_agg['installed_capacity']

"""<b>Special</b>"""

from functions.groupby import apply_dbscan_full, do_clustering

"""First, cluster."""

# 5m 53.5s, 8m 51.8s
wepp_special_clustered = do_clustering(wepp_special, wepp_pairs_to_fix, YEAR_THRESHOLD_AGG_WEPP)

# 3.3 s
gppd_special_clustered = do_clustering(gppd_special, gppd_pairs_to_fix, YEAR_THRESHOLD_AGG_GPPD)

"""Then, do the groupby on the clusters as well."""

gppd_special_agg = do_groupby_new(gppd_special_clustered, ["wepp_id", "primary_fuel", "year_cluster"], gppd_name, aggregation_info)

# 26.9s, 1m 0.5s
wepp_special_agg = do_groupby_new(wepp_special_clustered, ["location_id", "primary_fuel", "year_cluster"], wepp_name, aggregation_info)

"""##### D. Put everything back together."""

gppd_agg = pd.concat([gppd_normal_agg, gppd_special_agg])
gppd_agg = my_reset_index(gppd_agg)

wepp_agg = pd.concat([wepp_normal_agg, wepp_special_agg])
wepp_agg = my_reset_index(wepp_agg)

"""#### 3.3 Clean up the concat columns

The issue is that for the concat columns the could be replicated numbers which are not nice to see.
"""

wepp_agg.loc[wepp_agg['unit_id'].str.contains("; ")].head()

# all the compids have the same number within
len([x for x in wepp_agg['compid'].unique() if len(set(x.split("; "))) == 1]) == wepp_agg['compid'].nunique()

# make the change
for col in ['compid', "unit_id"]:
    wepp_agg[col] = wepp_agg[col].apply(lambda text: "; ".join(set(text.split("; "))))

# since all the original compid had just one number duplicated, then now there is no compid with more than one number
wepp_agg.loc[wepp_agg['compid'].str.contains("; ")].shape[0] == 0

"""#### 3.4 Save"""

# clean up index
wepp_agg = my_reset_index(wepp_agg)
gppd_agg = my_reset_index(gppd_agg)

# save these results
# wepp_agg.to_excel(back_up_folder + wepp_name + "_new_method_agg.xlsx")
# gppd_agg.to_excel(back_up_folder + gppd_name + "_new_method_agg.xlsx")
wepp_agg.to_csv(back_up_folder + wepp_name + "_new_method_agg.csv")
gppd_agg.to_csv(back_up_folder + gppd_name + "_new_method_agg.csv")

"""## 4. Joining

#### 4.1 Preparation: extract the year range limits

First, create the range for all the rows just to facilitate the process.
"""

from functions.cleaning import prepare_for_joining

wepp_agg = prepare_for_joining(wepp_agg)
gppd_agg = prepare_for_joining(gppd_agg)

"""#### 4.2 Do the actual merging

First, do all the possible combinations.
"""

pp_merged = pd.merge(wepp_agg, gppd_agg, left_on=['location_id', "primary_fuel"], right_on = ['wepp_id', "primary_fuel"], how="outer", suffixes=('_wepp', "_gppd"))

matches_no = pp_merged.loc[(~pp_merged['lower_limit_wepp'].isna()) & (~pp_merged['lower_limit_gppd'].isna())].shape[0]
print(f"Matches made #: {matches_no}")
print(f"Matches made %: {matches_no / pp_merged.shape[0] * 100}")

pp_merged.loc[(~pp_merged['lower_limit_wepp'].isna()) & (~pp_merged['lower_limit_gppd'].isna())][['lower_limit_wepp', "upper_limit_wepp", 'lower_limit_gppd', "lower_limit_gppd",]]

"""Then, compute if the single combinations are a match by looking if the ranges overlap."""

YEAR_THRESHOLD_JOIN_WEPP_GPPD = get_comm_year_thr(commissioning_year_thresholds_info, "Joining WEPP & GPPD")
YEAR_THRESHOLD_JOIN_WEPP_GPPD

from functions.joining import is_close_overlap

pp_merged['match'] = pp_merged.apply(lambda row: is_close_overlap(row['lower_limit_wepp'], row['upper_limit_wepp'], row['lower_limit_gppd'], row['upper_limit_gppd'], tolerance=YEAR_THRESHOLD_JOIN_WEPP_GPPD) if np.isnan(row['lower_limit_gppd']) == False and np.isnan(row['lower_limit_wepp']) == False else False, axis=1)

# check how many valid pairs we have and how many are True vs False
# valid pairs: they have both information on WEPP and GPPD in their row
valid_matches_made = pp_merged.loc[(~pp_merged['lower_limit_wepp'].isna()) & (~pp_merged['lower_limit_gppd'].isna())]
print(f"Valid matches #: {valid_matches_made.shape[0]}")
print(f"Valid matches %: {valid_matches_made.shape[0] / pp_merged.shape[0] * 100}")
for value in [True, False]:
    print(f"{value} matches #: {valid_matches_made.loc[valid_matches_made['match'] == value].shape[0]}")
    print(f"{value} matches %: {valid_matches_made.loc[valid_matches_made['match'] == value].shape[0] / valid_matches_made.shape[0] * 100}")


# visual check
valid_matches_made[['lower_limit_wepp', "upper_limit_wepp", 'lower_limit_gppd', "lower_limit_gppd", "match"]]

# check that all those rows that have at least one year missing false as a match
missing_years = pp_merged.loc[(pp_merged['commissioning_year_wepp'].str.contains("missing") ) | (pp_merged['commissioning_year_gppd'].str.contains("missing"))]
print(f"Check: all the rows with a missing year are not matched: {missing_years.loc[missing_years['match'] == False].shape[0] == missing_years.shape[0]}")

# check that all those rows that have at least a year has NaN have false as a match
missing_years = pp_merged.loc[(pp_merged['lower_limit_wepp'].isna()) | (pp_merged['lower_limit_gppd'].isna())]
print(f"Check: all the rows with a missing year are not matched: {missing_years.loc[missing_years['match'] == False].shape[0] == missing_years.shape[0]}")

"""#### 4.3 Clean up the results

So, now:
* we need to drop those rows that are not a match (note: those rows that have both the gppd and wepp values valid).
    * because these rows were not a match.
* but of these rows we need to check if they had a match otherwise we are dropping information
"""

# first create a conveninient new index with the year
# also in pp_merged
pp_merged['full_index_year_wepp'] = pp_merged['full_index_wepp'] + " " + ['year_range_wepp']
pp_merged['full_index_year_gppd'] = pp_merged['full_index_gppd'] + " " + pp_merged['year_range_gppd']
# also in wepp_agg and gppd_agg
wepp_agg['full_index_year'] = wepp_agg['full_index'] + " " + wepp_agg['year_range']
gppd_agg['full_index_year'] = gppd_agg['full_index'] + " " + gppd_agg['year_range']

# these rows were VALID PAIRS BUT WERE NOT A MATHCH
pp_merged = my_reset_index(pp_merged)
to_drop = pp_merged.loc[(~pp_merged['full_index_gppd'].isna()) & (~pp_merged['full_index_wepp'].isna()) & (pp_merged['match'] == False)]
to_drop[['full_index_gppd', "full_index_wepp", "year_range_wepp", "year_range_gppd", "match"]]

pp_merged = pp_merged.drop(to_drop.index.to_list())

# there are still "False" matches in pp_merged
pp_merged['match'].value_counts()

# but these all come from those rows that only have information from one dataset
still_false = pp_merged.loc[(pp_merged['full_index_gppd'].isna()) | (pp_merged['full_index_wepp'].isna())]
print(f"All False matches still here are from those rows with info only from one dataset: {still_false.shape[0] == pp_merged.loc[pp_merged['match'] == False].shape[0]}")
still_false [['full_index_gppd', "full_index_wepp", "year_range_wepp", "year_range_gppd", "match"]]

"""But these rows in to_drop are still information that needs to go in the final dataset as these are power plants (they just not were in both dataset). So, first we need to check if these rows are not in pp_merged already: if they aren't, we need to add them back."""

# get the WEPP ones that are not in pp_merged
# 1m 21.1s, 58.2s
to_check_in = pp_merged['full_index_year_wepp'].unique()
to_add_back_wepp = [x for x in to_drop["full_index_year_wepp"].unique() if x not in to_check_in]

# check
pp_merged.loc[pp_merged['full_index_year_wepp'].isin(to_add_back_wepp)]

# show statistics
print(f"Entries NOT in pp_merged: {len(to_add_back_wepp)}")
print(f"Entries IN pp_merged: {len(to_drop['full_index_year_wepp'].unique()) - len(to_add_back_wepp)}")

# show example
# get the IDs that are already in pp_merged
already_in_wepp = list(set(to_drop["full_index_year_wepp"].unique()) & set(pp_merged['full_index_year_wepp'].unique()))
example_no = 0
# in to_drop
to_drop.loc[to_drop['full_index_year_wepp'] == already_in_wepp[example_no]]

# in pp_merged: this row is in pp_merged because it had a good match!
pp_merged.loc[pp_merged['full_index_year_wepp'] == already_in_wepp[example_no]]

# get the GPPD ones that are not in pp_merged
to_check_in = pp_merged['full_index_year_gppd'].unique()
to_add_back_gppd = [x for x in to_drop["full_index_year_gppd"].unique() if x not in to_check_in]

# check
pp_merged.loc[pp_merged['full_index_year_gppd'].isin(to_add_back_wepp)]

# show statistics
print(f"Entries NOT in pp_merged: {len(to_add_back_gppd)}")
print(f"Entries IN pp_merged: {len(to_drop['full_index_year_gppd'].unique()) - len(to_add_back_gppd)}")

# show example
# get the IDs that are already in pp_merged
already_in_gppd = list(set(to_drop["full_index_year_gppd"].unique()) & set(pp_merged['full_index_year_gppd'].unique()))
example_no = 0
# in to_drop
to_drop.loc[to_drop['full_index_year_gppd'] == already_in_gppd[example_no]][pp_merged.columns.to_list()[::-1]]

# in pp_merged: this row is in pp_merged because it had a good match!
pp_merged.loc[pp_merged['full_index_year_gppd'] == already_in_gppd[example_no]][pp_merged.columns.to_list()[::-1]]

"""Now that we have them, we can create dataframes with these rows and put them back in pp_merged. Note: to put them in pp_merged we need to change the columns name."""

# create a dataframe for these rows - WEPP
to_add_back_wepp_rows = wepp_agg.loc[wepp_agg['full_index_year'].isin(to_add_back_wepp)].copy(deep=True)
print(f"Check: we got all the rows needed: {len(to_add_back_wepp) == to_add_back_wepp_rows.shape[0]}")
# create a dataframe for these rows - GPPD
to_add_back_gppd_rows = gppd_agg.loc[gppd_agg['full_index_year'].isin(to_add_back_gppd)].copy(deep=True)
print(f"Check: we got all the rows needed: {len(to_add_back_gppd) == to_add_back_gppd_rows.shape[0]}")

# to concatenate properly the GPPD rows rename the columns to be aligned with the columns that come from GPPD
columns_to_change_gpppd = [x for x in to_add_back_gppd_rows.columns if x not in pp_merged.columns] # there are rows that we do not have to change
to_add_back_gppd_rows = to_add_back_gppd_rows.rename(columns={col: col+"_gppd" for col in columns_to_change_gpppd})
to_add_back_gppd_rows.head(2)

# to concatenate properly the WEPP rows rename the columns to be aligned with the columns that come from WEPP
columns_to_change_wepp = [x for x in to_add_back_wepp_rows.columns if x not in pp_merged.columns]
to_add_back_wepp_rows = to_add_back_wepp_rows.rename(columns={col: col+"_wepp" for col in columns_to_change_wepp})
to_add_back_wepp_rows.head(2)

# merge all together
pp_merged = pd.concat([pp_merged, to_add_back_gppd_rows, to_add_back_wepp_rows])

# chcek that we got all the unique indexes in the end: the IDs that we had to add back (to_add_back_X) are now in pp_merged
# 1m 34.4s
to_check_in = pp_merged['full_index_year_gppd'].unique()
print(f"Check: we have all the GPPD indexes in pp_merged: {len([x for x in to_add_back_gppd if x not in to_check_in]) == 0}")
# chcek that we got all the unique indexes in the end
to_check_in = pp_merged['full_index_year_wepp'].unique()
print(f"Check: we have all the WEPP indexes in pp_merged: {len([x for x in to_add_back_wepp if x not in to_check_in]) == 0}")

# other checks
print(f"Check: all the missing WEPP commissioning_years are still here: {pp_merged.loc[pp_merged['commissioning_year_wepp'].str.contains('missing')]['commissioning_year_wepp'].nunique() == wepp_agg.loc[wepp_agg['commissioning_year'].str.contains('missing')]['commissioning_year'].nunique()}")
print(f"Check: all the missing WEPP commissioning_years are still here: {pp_merged.loc[pp_merged['commissioning_year_gppd'].str.contains('missing')]['commissioning_year_gppd'].nunique() == gppd_agg.loc[gppd_agg['commissioning_year'].str.contains('missing')]['commissioning_year'].nunique()}")
print(f"Check: all the missing WEPP and GPPD primary_fuel are still here: {pp_merged.loc[pp_merged['primary_fuel'].str.contains('missing')]['primary_fuel'].nunique() == wepp_agg.loc[wepp_agg['primary_fuel'].str.contains('missing')]['primary_fuel'].nunique() + gppd_agg.loc[gppd_agg['primary_fuel'].str.contains('missing')]['primary_fuel'].nunique()}")

# check that the original IDs are again in pp_merged
print(f"Check: all the original GPPD ids are in pp_merged: {len(set(pp_merged['full_index_year_gppd']) & set(gppd_agg['full_index_year'])) == len(set(gppd_agg['full_index_year']))}")
print(f"Check: all the original WEPP ids are in pp_merged: {len(set(pp_merged['full_index_year_wepp']) & set(wepp_agg['full_index_year'])) == len(set(wepp_agg['full_index_year']))}")

"""#### 4.4 Aggregating columns"""

# fix the indexes
pp_merged = my_reset_index(pp_merged)

"""First, we need to aggregate the location_id and wepp_id columns since we will only preserve the location_id when creating the Power Plant table, but there is information in the wepp_id as well."""

print(f"Number of matches: {pp_merged.loc[(~pp_merged['location_id'].isna()) & (~pp_merged['wepp_id'].isna())].shape[0]}")

# visual check
pp_merged.loc[pp_merged['location_id'].isna()][['location_id', 'wepp_id']]

print(f"Missing location_id BEFORE: {pp_merged['location_id'].isna().sum()}")
for i, row in pp_merged.iterrows():
    if pd.isna(row["location_id"]):
        pp_merged.at[i, "location_id"] = row["wepp_id"]
print(f"Missing location_id AFTER: {pp_merged['location_id'].isna().sum()}")

# visual check
pp_merged.loc[pp_merged['location_id'].isna()][['location_id', 'wepp_id']]

"""We did an "outer" join so to keep the information from both dataframes. But since both database share information (e.g., the year of commission) and WEPP has the most reliable information, then we need to drop the columns that are from GPPD. But before doing that we need to keep the information that GPPD has for those rows that are not in WEPP."""

# these are the columns that both datasets have (some are to be kept - e.g., "installed_capacity" - some were just needed in the previous steps - e.g., "year_range")
[col.replace("_wepp", "") for col in pp_merged.columns if "_wepp" in col]

# visual check
pp_merged[['commissioning_year_wepp', 'commissioning_year_gppd', "year_range_wepp", "year_range_gppd"]]

# visual check
pp_merged[['installed_capacity_wepp', 'installed_capacity_gppd', "country_wepp", "country_gppd", "power_plant_name_wepp", "power_plant_name_gppd"]]

# these rows have information from both dataabse: check that in the "_x" columns we do not add the "_y" information
pp_merged.loc[(~pp_merged['power_plant_name_wepp'].isna()) & (~pp_merged['power_plant_name_gppd'].isna())][['installed_capacity_wepp', 'installed_capacity_gppd', "country_wepp", "country_gppd", "power_plant_name_wepp", "power_plant_name_gppd"]]

# TODO: why is this needed to make the following cell run?
pp_merged['commissioning_year_wepp'] = pp_merged['commissioning_year_wepp'].fillna("")

# columsn that we want to merge
columns_to_merge = ['installed_capacity', 'country', 'power_plant_name', "commissioning_year", "year_range"]

# TODO: use the lambdas here to make it much, much faster pls
# 1m 33.5s
for col in columns_to_merge:
# for col in  ["commissioning_year"]:
    print(f"Missing {col} BEFORE: {pp_merged.loc[(pp_merged[col + '_wepp'].isna()) | (pp_merged.loc[pp_merged[col + '_wepp'] == ''].shape[0])].shape[0]}")
    for i, row in pp_merged.iterrows():
        if row[col + "_wepp"] == "" or pd.isna(row[col + "_wepp"]):
            pp_merged.at[i, col + "_wepp"] = row[col + "_gppd"]
    print(f"Missing {col} AFTER: {pp_merged.loc[(pp_merged[col + '_wepp'].isna()) | (pp_merged.loc[pp_merged[col + '_wepp'] == ''].shape[0])].shape[0]}")
    print()

# visual check
pp_merged[list(itertools.chain.from_iterable([[x + "_wepp", x+"_gppd"] for x in columns_to_merge]))][['installed_capacity_wepp', 'installed_capacity_gppd', "country_wepp", "country_gppd", "power_plant_name_wepp", "power_plant_name_gppd"]]

# check that the missing values now are in the same amount as before the merging
for col in columns_to_merge:
    print(f"{col}: {pp_merged.loc[(pp_merged[col + '_wepp'].isna()) | (pp_merged[col + '_wepp'] == '')].shape[0] == wepp_agg.loc[(wepp_agg[col].isna()) | (wepp_agg[col] == '')].shape[0] + gppd_agg.loc[(gppd_agg[col].isna()) | (gppd_agg[col] == '')].shape[0]}")

# drop the GPPD columns: these are the ones that end with a "_y" because gppd_agg was on the right in the merging function
cols_to_drop = [x+"_gppd" for x in columns_to_merge]
pp_merged = pp_merged.drop(columns=cols_to_drop)

# rename the columns that are from WEPP because they have the "_x"
pp_merged = pp_merged.rename(columns={x+"_wepp":x for x in columns_to_merge})

pp_merged.head()

"""#### 4.3 Reclean the primary fuel and commissioning year

For aggregating and merging purposes we put placeholders, when there was a missing commmissioning year or primary fuel. Now we can remove them.
"""

# to check after making the change
before_year = pp_merged.loc[pp_merged['commissioning_year'].str.contains("missing")].shape[0]
before_fuel = pp_merged.loc[pp_merged['primary_fuel'].str.contains("missing")].shape[0]

# make the change
pp_merged['commissioning_year'] = pp_merged['commissioning_year'].apply(lambda x: np.nan if "missing" in x else x)
pp_merged['primary_fuel'] = pp_merged['primary_fuel'].apply(lambda x: "" if "missing" in x else x)

# check
print(f"Check: all the missing years were cleaned correctly: {pp_merged['commissioning_year'].isna().sum() == before_year}")
print(f"Check: all the missing fuels were cleaned correctly: {pp_merged.loc[pp_merged['primary_fuel'] == ''].shape[0] == before_fuel}")

# fix the type
# pp_merged['commissioning_year'] = pp_merged['commissioning_year'].astype("float64")
pp_merged['primary_fuel'] = pp_merged['primary_fuel'].astype("string")

"""#### 4.4 Save copy"""

# save safe copy
# pp_merged.to_excel(back_up_folder + "WEPP_GPPD_merged.xlsx")
pp_merged.to_csv(back_up_folder + "WEPP_GPPD_merged.csv")

"""## 5. Create final tables"""

# read data from copy
# pp_merged = pd.read_excel(back_up_folder + "WEPP_GPPD_merged.xlsx")
# pp_merged.head(2)

pp_merged = my_reset_index(pp_merged)

"""#### 5.1 Create the empty tables"""

db_structure = map_table[db_structure_sheet]
tables_df = get_tables_to_create_empty(db_structure)

"""#### 5.2 Create PP_keys

These keys facilitate the joining especially if we want to fix the city names (e.g., if there are duplicates because of different spellings) in a second moment. With these keys we don't have to worry (too much) about how the changes in City can impact the Power Plant (see below).
"""

pp_merged = create_unique_id(pp_merged, "PP_key", "PLANTKEY_WEPPGPPD_")
pp_merged.head()

pp_merged.head()

"""#### 5.3 Do creation"""

tables_to_create

# fill the db that can already be filled
for entity in ['Power Plant', "Country"]:
    for column in tables_df[entity].columns:
        if column in pp_merged.columns:
            tables_df[entity][column] = pp_merged[column]

entity = "City"
for column in tables_df[entity].columns:
    if column in pp_merged.columns:
        tables_df[entity][column] = pp_merged[column]
    elif column == "city_key":
        tables_df[entity][column] = pp_merged["PP_key"]

"""#### 5.4 check that all the data that we could extract is in the final tables"""

tables_df["Power Plant"]

for col in ['latitude', "longitude", "gppd_id"]:
    print(f"Valid {col}: {tables_df['Power Plant'].loc[~tables_df['Power Plant'][col].isna()].shape[0]}")

# check that all the entries that originally missed a wepp_id are still present
tables_df["Power Plant"].loc[tables_df["Power Plant"]['location_id'].str.contains("_")].shape[0] == gppd.loc[gppd['wepp_id'].str.contains("_")].shape[0]

tables_df['Power Plant'].isna().sum()

tables_df["City"]

tables_df["City"].isna().sum()

tables_df["Country"]

"""#### 5.5 Save"""

print_final_statistics(tables_df, tables_to_create)

# save copies
# for entity in tables_to_create:
#     tables_df[entity].to_excel(intermediate_folder + entity + "_WEPPGPPD.xlsx", index=False)
[print(x) for x in tables_df["Power Plant"].columns]
for entity in tables_to_create:
    tables_df[entity].to_csv(intermediate_folder + entity + "_WEPPGPPD.csv", index=False)