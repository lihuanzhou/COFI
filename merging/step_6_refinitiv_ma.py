# -*- coding: utf-8 -*-
"""Step 6 - REFINITIV_MA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rog90z2sqwPr5Pund1DWRuSiNGNQM8wi
"""

import pandas as pd
import numpy as np

# import custom-made functions
from functions.cleaning import get_variables, create_placeholder, create_unique_id, get_variables_advanced, preprocess_text
from functions.cleaning import  check_if_new_country_names, update_country_names, convert_primary_fuel
from functions.joining import join_pp_until_country, join_equity_transaction
from functions.final_tables_creation import get_tables_to_create_empty

# other useful libraries
import itertools
import statistics

# for the matching of power plant names
from thefuzz import process
from thefuzz import fuzz

from functions.joining import join_pp_full_equity_full
from functions.cleaning import my_read_excel
from functions.cleaning import get_thresholds_methods_dicts, my_reset_index
from functions.matching import find_matches_equity_and_debt_complete
from functions.cleaning import print_final_statistics
from functions.matching import find_matches_power_plant_or_fill_PP_key
from functions.cleaning import fix_investor_names, clean_primary_fuel_column
from functions.cleaning import count_sources_in_id
from functions.matching import prepare_pp_full_for_matching_commissioning_year

# needed folder paths
input_folder = "Cleaned Data/" # or we can put this info in the "files name" sheet
intermediate_folder = "Intermediate Results/" # to save some intermediate results
current_final_tables_folder = "Current Final Tables/"

# load the excel containing all the mapping information
map_table_name = "Variables.xlsx"
map_table = pd.read_excel(map_table_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(map_table))

# load the excel containing all the mapping information
dictionary_fuel_name = "Dictionaries.xlsx"
dictionary_fuel = pd.read_excel(dictionary_fuel_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(dictionary_fuel))


# the sheet where there is the information that the client can change
files_names_sheet = "file names"
groupby_info_sheet = "formatting"
vertical_slicing_info_sheet = "variables to use"
aggregation_info_sheet = "aggregation"
db_structure_sheet = "db structure"
un_country_info_sheet = "UN Country Info"
country_names_dict_sheet = "country names dicionary"
bri_countries_sheet = "BRI countries"
filtering_sheet = "filtering"
merging_sheet = "merging"
matching_threshold_sheet = "matching thersholds"
matching_method_sheet = "matching methods"
matching_paremeter_sheet = "matching parameters"
# we will need this later
thresholds_df = map_table[matching_threshold_sheet]
methods_df = map_table[matching_method_sheet]
parameters_df = map_table[matching_paremeter_sheet]


# the tables we are creating here
tables_to_create = ["Power Plant", "City", "Country", "Equity", "Transaction", "Investor"]
investments_tables = ["Equity", "Transaction", "Investor"]

# already exising final data<<
# debt_file = "Debt.xlsx"
# power_plant_file = "Power Plant.xlsx"
# city_file = "City.xlsx"
# country_file = "Country.xlsx"
# bridge_file = "CITYKEY_BRIDGE_PP_C.xlsx"
# equity_file = "Equity.xlsx"
debt_file = "Debt.csv"
power_plant_file = "Power Plant.csv"
city_file = "City.csv"
country_file = "Country.csv"
bridge_file = "CITYKEY_BRIDGE_PP_C.csv"
equity_file = "Equity.csv"

# names of the databases used here
r_ma_name = "REFINITIV_MA"

# names of the final tables to be used here
equity_name = "Equity"

"""## 1. Read data

#### 1.1 Load data
"""

# this table contains the name of the data sources
file_names = map_table[files_names_sheet]
file_names

r_ma = my_read_excel(input_folder , file_names.loc[file_names['Database'] == r_ma_name]["File name"].values[0])

r_ma_copy = r_ma.copy(deep=True)

r_ma.head()

"""#### 1.2 Load data for exising Power Plants and Debt data for matching purposes"""

# remember that we want to keep the keys so that we can use them if there is a match with exisitng data
columns_to_keep_pp = ["PP_key", "power_plant_name", "primary_fuel", "country"]
columns_to_keep_equity = ["equity_id", "PP_key", "equity_investment_type", "equity_investment_year", "investor_name", "amount"]
# TODO: re-check that these are right

pp_full, eq_pp = join_pp_full_equity_full(columns_to_keep_pp, columns_to_keep_equity, get_commissioning_year=True)

count_sources_in_id(pp_full, "PP_key", "pp_full")

pp_full.head()

eq_pp.head()

count_sources_in_id(eq_pp, "equity_id", "pp_full")

"""#### 1.3 Load Equity table"""

equity = my_read_excel(current_final_tables_folder, equity_file)

equity.head()

"""## 2. Clean the data

#### 2.1 Vertical slicing
"""

vertical_slicing = map_table[vertical_slicing_info_sheet]
vertical_slicing.head()

r_ma_variables = get_variables(vertical_slicing, r_ma_name, tables_to_create)
r_ma_variables

r_ma = r_ma[r_ma_variables]
r_ma.head(2)

# check
print(len(r_ma_variables) == len(r_ma.columns))

"""#### 2.2 Clean if needed"""

r_ma.isna().sum()

"""##### Country"""

# needed to run the functions to do the converting
country_names_dict_df = map_table[country_names_dict_sheet]
un_data = map_table[un_country_info_sheet]

# check if there are new names that are needs to be added to the conversion list
# TODO: in the final product this can also be a feature when they load the db
check_if_new_country_names(list(r_ma['country'].unique()), un_data, country_names_dict_df)

# convert the names
r_ma = update_country_names(country_names_dict_df, r_ma)

"""##### Primary_fuel

If the primary_fuel is nan or "other" then we keep/put it to nan because, respectively, it is not known and it may still match something in WEPP that we don't know.
* so when we try to find matches we do an equal join based on the primary_fuel. If the primary_fuel is nan, then we consider all the potential plants (in the same country etc.)
"""

r_ma['primary_fuel'].unique()

# make conversion
r_ma = clean_primary_fuel_column(r_ma)

r_ma['primary_fuel'].unique()

"""##### Equity_investment_type

Add the equity_investment_type: since r_ma is about M&A, then we can already set them up correctly.
"""

r_ma['equity_investment_type'] = "m&a"

"""##### investor_name"""

r_ma['equity_investor_name'] = r_ma['equity_investor_name'].fillna("")

r_ma = fix_investor_names(r_ma, "equity_investor_name", False, None, drop_duplicates=False)

"""#### 2.3 Pre-process REFINTIV_MA and exisitng Power Plant and Equity data for matching"""

pp_full.columns

for col in pp_full.columns:
    if col in ["power_plant_name", "city", "province"]:
        pp_full[col] = pp_full[col].fillna("")
        pp_full[col] = pp_full[col].apply(preprocess_text)
        pp_full[col] = pp_full[col].astype("string")
    elif col in ["country", "primary_fuel"]:
        pp_full[col] = pp_full[col].fillna("")
        pp_full[col] = pp_full[col].astype("string")
    elif col in ['installed_capacity']:
        pp_full[col] = pp_full[col].astype("float64")
    elif col ==  "commissioning_year":
        # pp_full[col] = pp_full[col].apply(lambda x: np.nan if (isinstance(x, str)) else x)
        pp_full[col] = pp_full[col].astype("float64")

pp_full.dtypes

eq_pp.columns

for col in eq_pp.columns:
    if col in ["investor_name", "power_plant_name", "city", "province"]:
        eq_pp[col] = eq_pp[col].fillna("")
        eq_pp[col] = eq_pp[col].apply(lambda x: preprocess_text(x, True))
        eq_pp[col] = eq_pp[col].astype("string")
    elif col in ["country", "primary_fuel"]:
        eq_pp[col] = eq_pp[col].fillna("")
        eq_pp[col] = eq_pp[col].astype("string")
    elif (col == 'equity_investment_year') or (col == "amount"):
        eq_pp[col] = eq_pp[col].astype("float64")

eq_pp.dtypes

r_ma.columns

for col in r_ma.columns:
    if col in ["equity_investment_year", "equity_investor_amount"]:
        r_ma[col] = r_ma[col].astype("float64")
    elif col in ["city", "power_plant_name", "equity_investor_name", "province"]:
        r_ma[col] = r_ma[col].fillna("")
        r_ma[col] = r_ma[col].apply(lambda x: preprocess_text(x, True))
        r_ma[col] = r_ma[col].astype("string")
    elif col in ["country", "primary_fuel"]:
        r_ma[col] = r_ma[col].fillna("")
        r_ma[col] = r_ma[col].astype("string")
    elif col in ['equity_investment_type']:
        r_ma[col] = r_ma[col].astype("string")
        r_ma[col] = r_ma[col].apply(lambda x: preprocess_text(x, False))

"""## 3. Match with existing Equity data

#### 3.1 Load thresholds and methods to use
"""

thrs_dict_eq, mtds_dict_eq, prmts_dict_eq = get_thresholds_methods_dicts(thresholds_df, methods_df, parameters_df, r_ma_name, "Equity")

thrs_dict_eq

mtds_dict_eq

prmts_dict_eq

"""#### 3.2 Find the matches with the info in equity"""

r_ma = my_reset_index(r_ma)

# no. of matches in defualt settings: 5
matches = find_matches_equity_and_debt_complete(r_ma, eq_pp, equity_or_debt="Equity",
                                               thresholds_dict=thrs_dict_eq,
                                                methods_dict=mtds_dict_eq, parameters_dict=prmts_dict_eq)
matches

"""equity_investment_type: we should not include the matches that are known to be a mistake (e.g., r_ma is "M&A" and should not be matched with "greenfield" investments). BUT since all the matches are to IJGLOBAL and this ones doens't have a clear investment type, then all the matches are fine."""

index = 14
r_ma.head(index + 1).tail(1)

eq_pp.loc[eq_pp['equity_id'] == matches[index]]

"""#### 3.3 Merge what needs to be merged"""

equity.head(0)

# UPDATE DEBT: put the j_id there
equity['rma_id'] = equity['rma_id'].astype("string") # since there could be multiple j_ids then we want to keep them all
equity['rma_id'] = equity['rma_id'].fillna("")

equity = my_reset_index(equity)

for index_rma in matches:
    # get the j_id of the db_ij's row
    rma_id = r_ma.iloc[index_rma]['rma_id']
    # get the index of the matched debt row (we use the index to then assign to the row the j_id)
    index_equity = equity.loc[equity['equity_id'] == matches[index_rma]].index.to_list()[0]
    # get the debt row just for convinience
    equity_row = equity.iloc[index_equity]
    # assign the j_id to the debt row
    # note: multiple IJ_G rows could match the same debt row so we need to take track of all their j_id
    # but different IJ_G could have the same j_id so we also don't want to repeat
    if equity_row['rma_id'] == "": # add the first one
        equity.at[index_equity, "rma_id"] = str(rma_id)
    elif not str(rma_id) in equity_row['rma_id']: # if the j_id is not already there then add this id
        equity.at[index_equity, "rma_id"] = str(rma_id) + "; " + equity_row['rma_id']

# check that the number of  rows changed == number of matches
print(f"Check: {len(matches) == equity.loc[equity['rma_id'] != ''].shape[0]}")
print(f"Rows changed: {equity.loc[equity['rma_id'] != ''].shape[0]}")

# CREATE THE TRANSACTION AND INVESTOR ROWS: from the db_ij rows
transaction_rows = []
investor_rows = []
for index_rma in matches:
    rma_row = r_ma.iloc[index_rma]
    tr_row = [matches[index_rma], rma_row['equity_investor_name'], rma_row['equity_investor_amount'], "N", "N", np.nan]
    in_row = [rma_row['equity_investor_name'], ""]
    transaction_rows.append(tr_row)
    investor_rows.append(in_row)
print(f"There are as many new transaction rows as matches: {len(transaction_rows) == len(matches)}")
print(f"There are as many new investor rows as matches: {len(investor_rows) == len(matches)}")

# REMOVE FROM DB_IJ: the rows that were matched
new_equity = r_ma.drop(list(matches.keys()))
print(f"Dropping went fine: {new_equity.shape[0] == r_ma.shape[0] - len(matches)}")

"""## 4. Find matches with Power Plant"""

thrs_dict_pp, mtds_dict_pp, prmts_dict_pp = get_thresholds_methods_dicts(thresholds_df, methods_df, parameters_df, r_ma_name, "Power Plant")

thrs_dict_pp

mtds_dict_pp

prmts_dict_pp

new_equity = my_reset_index(new_equity)

# prepare the pp_full dataset for the matching (it fixes up the year_range and then creates the upper and lower limit columns which are needed)
pp_full = prepare_pp_full_for_matching_commissioning_year(pp_full)

# 26.9s
# no. of matches in default settings: 10
matched_db = find_matches_power_plant_or_fill_PP_key(new_equity, pp_full, "PLANTKEY_REFINITIVMA_",
                                                     equity_or_debt="Equity",
                                                      thresholds_dict = thrs_dict_pp, methods_dict=mtds_dict_pp, parameters_dict=prmts_dict_pp
                                                            )

pp_keys_matched_list = matched_db.loc[~matched_db['PP_key'].str.contains("PLANTKEY_REFINITIVMA_")]['PP_key'].unique()
pp_keys_matched_list

# show example of the commissioning year filtering
list_el = 5
matched_db.loc[matched_db['PP_key'] == pp_keys_matched_list[list_el]]

# show the original power plants
pp_full.loc[pp_full['PP_key'] == pp_keys_matched_list[list_el]]

# check that the matching went fine
# do some checks
matched = matched_db
newly_created_name = "REFINITIVMA"

# check that all the entries now have a proper PP_key

# no. of matched power_plants
key_wepp = matched.loc[matched['PP_key'].str.contains("WEPPGPPD")].shape[0]
key_bucgp = matched.loc[matched['PP_key'].str.contains("BUCGP")].shape[0]
key_bucgef = matched.loc[matched['PP_key'].str.contains("BUCGEF")].shape[0]
key_sais = matched.loc[matched['PP_key'].str.contains("SAIS")].shape[0]
key_ijg = matched.loc[matched['PP_key'].str.contains("IJG")].shape[0]
# key_fdi = matched.loc[matched['PP_key'].str.contains("FDIMARKETS")].shape[0]


# no. of newly created keys, these are new entries
key_new = matched.loc[matched['PP_key'].str.contains(newly_created_name)].shape[0]

# check that matched keys got the PP_key from Power Plant
print(f"All matched keys got the PP_key from Power Plant: {key_wepp + key_bucgp + key_bucgef + key_sais + key_ijg == matched.loc[~(matched['PP_key'].str.contains(newly_created_name))].shape[0]}")

# check that each row has a PP_key
print(f"Each row has a PP_key: {key_wepp + key_bucgp + key_bucgef + key_sais + key_ijg + key_new == matched.shape[0]}")

pd.DataFrame(data=[["WEPP + GPPD", key_wepp], ["BUCGP", key_bucgp], ["BUCGEF", key_bucgef], ["SAIS_CLA + IAD_GEGI", key_sais],["IJGLOBAL", key_ijg], ["REFINITIVMA", key_new]], columns=['Data source', "Count"])

"""## 5. Create tables

#### 5.0 Create the empty tables
"""

db_structure = map_table[db_structure_sheet]
tables_df = get_tables_to_create_empty(db_structure)
# used as support for the creation
tables_df_tmp = get_tables_to_create_empty(db_structure)

"""#### 5.0 Deal with duplicates Power Plant and City info"""

from functions.cleaning import fix_duplicated_power_plants_keys

examples_duplicated = ['eocell inc', 'nec energy devices ltd', 'calient technologies inc']
matched_db.loc[matched_db['power_plant_name'].isin(examples_duplicated)].sort_values(by="power_plant_name")

matched_db, diff = fix_duplicated_power_plants_keys(matched_db,list(tables_df['Power Plant'].columns) + list(tables_df['City'].columns), "PLANTKEY_REFINITIVMA_")

matched_db.loc[matched_db['power_plant_name'].isin(examples_duplicated)].sort_values(by="power_plant_name")

"""#### 5.1 Add equity_id"""

# create unique ids with custom function
matched_db = create_unique_id(matched_db, "equity_id", "EQUITYID_REFINITIVMA_")
matched_db.head()

"""#### 5.2 Adding to the Equity table

Since before we updated the equity data then here we can just concat the updated data with the new data (from matched_db).

Note: in this notebook we havent' updated Equity because there was no need. In the future we may still need to plug in the information so we leave this code below.
"""

# get the new entries
entity = equity_name
for column in tables_df_tmp[entity].columns:
    if column in matched_db.columns:
        tables_df_tmp[entity][column] = matched_db[column]

# concat
tables_df['Equity'] = pd.concat([equity, tables_df_tmp['Equity']])
# check
tables_df['Equity'].shape[0]  == equity.shape[0]  +  tables_df_tmp['Equity'].shape[0]

"""#### 5.3 Creating Transaction and Investor"""

# we need to rename some columns to match the columns in Investor and Transactio
renamings = get_variables_advanced(vertical_slicing, r_ma_name, ['Investor', "Transaction"])
# the following two are not in the vertical_slicing (which comes from Variables.xlsx) because we created these two columns not from data coming from the sources (they are based
# on the index of the dataframes....)
renamings["equity_id"] = "investment_id"
renamings

# do renaming
# Note: we need to filter because some renaimings rules only apply to eq_agg_full and some only to dt_agg_full
matched_db_v2 = matched_db.rename(columns=dict(filter(lambda x: x[0] in matched_db.columns, renamings.items())))
matched_db_v2.head(2)

# create table now that all is set
for entity in ["Transaction", "Investor"]:
    for column in tables_df[entity].columns:
        if column in matched_db_v2.columns:
            tables_df[entity][column] = matched_db_v2[column]

tables_df['Transaction']["investment_averaged"] = "N"
tables_df['Transaction']["investment_weighted"] = "N"

# concat the newly created rows
tables_df['Transaction'] = pd.concat([tables_df['Transaction'], pd.DataFrame(transaction_rows, columns=tables_df['Transaction'].columns)])
tables_df['Transaction'].tail()

# concat the newly created rows
tables_df['Investor'] = pd.concat([tables_df['Investor'], pd.DataFrame(investor_rows , columns=tables_df['Investor'].columns)])
tables_df['Investor'].tail()

"""#### 5.4 Make Power Plant and City and Country for new Power Plants found here!"""

missing_power_plants = matched_db.loc[matched_db['PP_key'].str.contains("PLANTKEY_REFINITIVMA_")]
print("Missing power plant: " + str(missing_power_plants.shape[0]))
print(f"check: {matched_db.shape[0] - matched_db.loc[~matched_db['PP_key'].str.contains('PLANTKEY_REFINITIVMA_')].shape[0] == missing_power_plants.shape[0]}")
# 90 was the number that the matching function printed as the number of matches

# get the data from equity
for entity in ['Power Plant', "Country"]:
    for column in tables_df[entity].columns:
        if column in missing_power_plants.columns:
            tables_df[entity][column] = missing_power_plants[column]

entity = "City"
for column in tables_df[entity].columns:
    if column in missing_power_plants.columns:
        tables_df[entity][column] = missing_power_plants[column]
    elif column == "city_key":
        tables_df[entity][column] = missing_power_plants["PP_key"]

# drop the duplicates that we fixed before
before_dropping = tables_df['Power Plant'].shape[0]
tables_df['Power Plant'] = tables_df['Power Plant'].drop_duplicates()
rows_dropped =  before_dropping - tables_df['Power Plant'].shape[0]
print(f"Rows dropped: {rows_dropped}")
print(f"Check: the rows we are dropping now are the same as the rows that we found as duplicated before: {rows_dropped == diff}")

before_dropping = tables_df['City'].shape[0]
tables_df['City'] = tables_df['City'].drop_duplicates()
rows_dropped =  before_dropping - tables_df['City'].shape[0]
print(f"Rows dropped: {rows_dropped}")
print(f"Check: the rows we are dropping now are the same as the rows that we found as duplicated before: {rows_dropped == diff}")

"""#### 5.5 Visual check"""

tables_df['Power Plant']

for col in ['power_plant_name', "primary_fuel"]:
    print(f"Valid {col} #: {tables_df['Power Plant'].loc[tables_df['Power Plant'][col] != ''].shape[0]}")
    print(f"Valid {col} %: {tables_df['Power Plant'].loc[tables_df['Power Plant'][col] != ''].shape[0] / tables_df['Power Plant'].shape[0] * 100}")

tables_df['City']

tables_df['Country']

tables_df['Equity']

print(f"The previous Equity table was updated with the rma_id of the matches: {tables_df['Equity'].loc[(tables_df['Equity']['rma_id'] != '') & (~tables_df['Equity']['equity_id'].str.contains('REFINITIV'))].shape[0] == len(matches)}")
print(f"All rma_ids are here: {tables_df['Equity'].loc[(tables_df['Equity']['rma_id'] != '')].shape[0] == r_ma.shape[0]}")

tables_df['Transaction']

print(f"All investment_ids are there: {tables_df['Transaction']['investment_id'].isna().sum() == 0}")

tables_df['Investor']

"""#### 5.6 Save"""

print_final_statistics(tables_df, tables_to_create)

# all entities but Debt get saved in the intermediate folder
for entity in ['Power Plant', 'City', 'Country', 'Transaction', 'Investor']:
    tables_df[entity].to_csv(intermediate_folder + entity + "_REFINITIVMA.csv", index=False)

# since Equity is ready and doesn't need any further post-processing, we can save it here
entity = "Equity"
current_final_tables_folder + entity + ".csv"

tables_df['Equity'].to_csv(current_final_tables_folder + entity + ".csv", index=False)

