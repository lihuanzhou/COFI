# -*- coding: utf-8 -*-
"""Step 4 - SAIS-CLA & IAD-GEGI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FwKP7PUDqAQb2atSJ4am3x61rWG62xUf
"""

import pandas as pd
import numpy as np

# import custom-made functions
from functions.cleaning import get_variables, create_placeholder, create_unique_id, get_variables_advanced, preprocess_text
from functions.joining import join_pp_until_country
from functions.final_tables_creation import get_tables_to_create_empty

# other useful libraries
import itertools
import statistics

# for the matching of power plant names
from thefuzz import process
from thefuzz import fuzz

from functions.joining import join_pp_full_debt_full
from functions.cleaning import check_if_new_country_names, update_country_names, my_reset_index
from functions.matching import find_matches_power_plant_or_fill_PP_key
from functions.cleaning import print_final_statistics
from functions.cleaning import convert_primary_fuel
from functions.cleaning import count_sources_in_id
from functions.cleaning import my_read_excel
from functions.cleaning import fix_investor_names
from functions.cleaning import clean_primary_fuel_column, get_thresholds_methods_dicts
from functions.matching import prepare_pp_full_for_matching_commissioning_year

# needed folder paths
input_folder = "Cleaned Data/" # or we can put this info in the "files name" sheet
intermediate_folder = "Intermediate Results/" # to save some intermediate results
current_final_tables_folder = "Current Final Tables/"

# load the excel containing all the mapping information
map_table_name = "Variables.xlsx"
map_table = pd.read_excel(map_table_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(map_table))

# load the excel containing all the mapping information
dictionary_fuel_name = "Dictionaries.xlsx"
dictionary_fuel = pd.read_excel(dictionary_fuel_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(dictionary_fuel))

# the sheet where there is the information that the client can change
files_names_sheet = "file names"
groupby_info_sheet = "formatting"
vertical_slicing_info_sheet = "variables to use"
aggregation_info_sheet = "aggregation"
db_structure_sheet = "db structure"
un_country_info_sheet = "UN Country Info"
country_names_dict_sheet = "country names dicionary"
bri_countries_sheet = "BRI countries"
filtering_sheet = "filtering"
merging_sheet = "merging"
matching_threshold_sheet = "matching thersholds"
matching_method_sheet = "matching methods"
matching_paremeter_sheet = "matching parameters"

# values needed to read the right information from the the matching sheets in Variables.xlsx
# these needs to match what's in Variables.xlsx
capcity_thr_description = "matching with Power Plants on installed_capacity"
pp_thr_description = "matching with Power Plants on power_plant_name"
method_description = "matching with Power Plants"


# the tables we are creating here
tables_to_create = ["Power Plant", "City", "Country", "Debt", "Transaction", "Investor"]
investments_tables = ["Debt", "Transaction", "Investor"]

# already exising final data<<
# debt_file = "Debt.xlsx"
# power_plant_file = "Power Plant.xlsx"
# city_file = "City.xlsx"
# country_file = "Country.xlsx"
# bridge_file = "CITYKEY_BRIDGE_PP_C.xlsx"
debt_file = "Debt.csv"
power_plant_file = "Power Plant.csv"
city_file = "City.csv"
country_file = "Country.csv"
bridge_file = "CITYKEY_BRIDGE_PP_C.csv"

# names of the databases used here
sais_cla_name = "SAIS_CLA"
iad_gegi_name = "IAD_GEGI"
# the name of the two datasets when concatenated together
db_name = sais_cla_name + " + " + iad_gegi_name

# names of the final tables to be used here
debt_name = "Debt"

# saving information
# saving_suffix = "_SAISCLA_IADGEGI.xlsx"
saving_suffix = "_SAISCLA_IADGEGI.csv"

"""## 1. Read Data

#### 1.1 Load data
"""

# this table contains the name of the data sources
file_names = map_table[files_names_sheet]
file_names

sais_cla = my_read_excel(input_folder , file_names.loc[file_names['Database'] == sais_cla_name]["File name"].values[0])

sais_cla_full = sais_cla.copy(deep=True)

sais_cla.head()

iad_gegi = my_read_excel(input_folder , file_names.loc[file_names['Database'] == iad_gegi_name]["File name"].values[0])

iad_gegi_full = iad_gegi.copy(deep=True)

iad_gegi.head()

# TODO: I shouldn't do this here
for db in [sais_cla, iad_gegi]:
    print(f"Missing NaNs BEFORE fixing issue: {db['installed_capacity'].isna().sum()}")
    db['installed_capacity'] = db['installed_capacity'].astype("string")
    db['installed_capacity'] = db['installed_capacity'].fillna("")
    db['installed_capacity'] = db['installed_capacity'].apply(lambda x: x.replace("MW", "").strip().replace(",", ""))
    db['installed_capacity'] = db['installed_capacity'].apply(lambda x: np.nan if x == "" else x)
    db['installed_capacity'] = db['installed_capacity'].astype("float64")
    print(f"Missing NaNs AFTER fixing issue: {db['installed_capacity'].isna().sum()}")

"""#### 1.2 Load data for exising Power Plants and Debt data for matching purposes"""

# remember that we want to keep the keys so that we can use them if there is a match with exisitng data
columns_to_keep_pp = [ "PP_key", "power_plant_name", "installed_capacity", "primary_fuel", "country"]
columns_to_keep_debt = ["debt_id", "PP_key", "bu_id", "debt_investment_year", "investor_name", "amount"]

pp_full, db_pp = join_pp_full_debt_full(columns_to_keep_pp, columns_to_keep_debt, "Current Final Tables/", get_commissioning_year=True)

count_sources_in_id(pp_full, "PP_key", "Power Plant full")

count_sources_in_id(db_pp, "debt_id", "Debt full")

pp_full.head()

"""#### 1.3 Load Debt"""

debt = my_read_excel(current_final_tables_folder, debt_file)
debt.head()

"""## 2. Clean the data

#### 2.1 Vertical slicing
"""

vertical_slicing = map_table[vertical_slicing_info_sheet]
vertical_slicing.head()

sais_cla_variables = get_variables(vertical_slicing, sais_cla_name, tables_to_create)
sais_cla_variables

sais_cla = sais_cla[sais_cla_variables]
sais_cla.head(2)

iad_gegi_variables = get_variables(vertical_slicing, iad_gegi_name, tables_to_create)
iad_gegi_variables

iad_gegi = iad_gegi[iad_gegi_variables]
iad_gegi.head(2)

"""#### 2.2 Clean SAIS_ClA and IAD_GEGI columns"""

sais_cla.isna().sum()

iad_gegi.isna().sum()

"""Only the capacity has some NaNs. That's fine when we will need to use the capacity (e.g., to find the matches with the already existing power plants) we will make sure to deal with the NaNs accordingly.

##### Rename columns

Also, "iad_id" and "sais_id" are the same and are called "bu_id". So we change their names.
"""

# TODO: this may not be right (e.g., sais_id is not the bu_id) or may not be needed
# (e.g., if it is correct, the cleaning step of the whole process will already call these columns as bu_id)
# so update this step accordingly

sais_cla = sais_cla.rename(columns={"sais_id": "bu_id"})
iad_gegi = iad_gegi.rename(columns={"iad_id": "bu_id"})

"""##### Country"""

# needed to run the functions to do the converting
country_names_dict_df = map_table[country_names_dict_sheet]
un_data = map_table[un_country_info_sheet]

# pre-process
sais_cla['country'] = sais_cla['country'].apply(lambda x: x.lower().strip())
iad_gegi['country'] = iad_gegi['country'].apply(lambda x: x.lower().strip())

# check if there are names that are not standard or that are not in the dictionary to convert the country names
for db in [sais_cla, iad_gegi]:
    print(check_if_new_country_names(list(db['country'].unique()), un_data, country_names_dict_df))

sais_cla.loc[sais_cla['country'] == "regional"]

# convert the names
sais_cla = update_country_names(country_names_dict_df, sais_cla)
iad_gegi = update_country_names(country_names_dict_df, iad_gegi)

"""##### Primary fuel"""

sais_cla['primary_fuel'].unique()

# make conversion
sais_cla = clean_primary_fuel_column(sais_cla)

sais_cla['primary_fuel'].unique()

iad_gegi['primary_fuel'].unique()

# make conversion
iad_gegi = clean_primary_fuel_column(iad_gegi)

iad_gegi['primary_fuel'].unique()

"""##### investor_name

Before changing the investor_name, we need to account for those rows that have several investors on the same rows. For these, using the "fix_investor_names" function would mean losing the punctuations which is needed to keep seperate the investors. So here we just seperate them, clean them seperatly from the rest, and then put them back together. We will later process these rows to get each investor on its own.
"""

# the rows that are problmeatic
sais_cla.loc[sais_cla['Lender'].str.contains(":")]

# get the rows on another dataset and prepare to drop
sais_cla = my_reset_index(sais_cla) # needed for the dropping
special_rows = sais_cla.loc[sais_cla['Lender'].str.contains(":")].copy(deep=True)
to_drop = sais_cla.loc[sais_cla['Lender'].str.contains(":")].index.to_list()
# drop
sais_cla = sais_cla.drop(to_drop)

# fix the names of all the normal rows
iad_gegi = fix_investor_names(iad_gegi, "Lender", False, None, drop_duplicates=False)
sais_cla = fix_investor_names(sais_cla, "Lender", False, None, drop_duplicates=False)

# fix the names of these special rows (easy clean up, no removal of the punctutations)
special_rows['Lender'] = special_rows['Lender'].apply(lambda x: preprocess_text(x, False))

# concat back everything
sais_cla = pd.concat([sais_cla, special_rows])
sais_cla = my_reset_index(sais_cla)

# the rows that are problmeatic are still here
sais_cla.loc[sais_cla['Lender'].str.contains(":")]

"""#### 2.3 Concat the two datasets together.

Since they the same columns (except for the installed capcity), we can concatenate them together so that it will be easier to deal with them.
"""

# TODO; if a specific value is set for the entries that don't have a value for the
# installed capacity, then make sure to set it also for the entries in sais_cla (before
# or after concat the datasets)

# concat together
db = pd.concat([sais_cla, iad_gegi])
# check concat
db.shape[0] == sais_cla.shape[0] + iad_gegi.shape[0]

# check that NaNs values have been added correctly
db['installed_capacity'].isna().sum() == iad_gegi['installed_capacity'].isna().sum() + sais_cla['installed_capacity'].isna().sum()

"""#### 2.3 Pre-process SAIS_CLA+IAD_GEGI and exisitng Power Plant and Debt data for matching"""

pp_full.columns

for col in columns_to_keep_pp:
    # turn all the strings to text except for the numerical ones (PP_key doesn't neeed any pre-processing)
    if col == "power_plant_name":
        pp_full[col] = pp_full[col].astype("string")
        pp_full[col] = pp_full[col].fillna("")
        pp_full[col] = pp_full[col].apply(preprocess_text)
    elif col == "country" or col == "primary_fuel":
        pp_full[col] = pp_full[col].astype("string")
        pp_full[col] = pp_full[col].fillna("")
    elif col == "installed_capacity":
        pp_full[col] = pp_full[col].astype("float64")
    elif col ==  "commissioning_year":
        # pp_full[col] = pp_full[col].apply(lambda x: np.nan if (isinstance(x, str)) else x)
        pp_full[col] = pp_full[col].astype("float64")

for col in ['primary_fuel', "country"]:
    print(f"NaNs in pp_full for {col}: {pp_full.loc[pp_full[col] == ''].shape[0]}")

missing_comm_year = pp_full.loc[~pp_full["commissioning_year"].isna()].shape[0]
print(f"Valid commissioning_year in pp_full #: {missing_comm_year}")
print(f"Valid commissioning_year in pp_full %: {missing_comm_year / pp_full.shape[0] * 100}")

# no need to pre-process the country and fuels because those are already good (we made sure
# of this when processing the previous data sources)
columns_to_keep_db_pp = columns_to_keep_debt + columns_to_keep_pp
for col in columns_to_keep_db_pp:
    if col == "investor_name" or col == "power_plant_name":
        db_pp[col] = db_pp[col].astype("string")
        db_pp[col] = db_pp[col].fillna("")
        db_pp[col] = db_pp[col].apply(lambda x: preprocess_text(x))
    if col == "primary_fuel" or col == "country":
        db_pp[col] = db_pp[col].astype("string")
        db_pp[col] = db_pp[col].fillna("")
    elif (col == 'debt_investment_year') or (col == "amount") or col == "installed_capacity":
        db_pp[col] = db_pp[col].astype("float64")

# special for here: we need this id in a string format so we can use it to do the matching
db_pp['bu_id'] = db_pp['bu_id'].astype("string")

for col in ['primary_fuel', "country"]:
    print(f"NaNs in db_pp for {col}: {db_pp.loc[db_pp[col] == ''].shape[0]}")

db.columns

for col in db.columns:
    if col in ["debt_investment_year", "debt_investment_amount", "installed_capacity"]:
        db[col] = db[col].astype("float64")
    elif col in ["power_plant_name", "Lender", "Other Lenders"]:
        db[col] = db[col].astype("string")
        db[col] = db[col].fillna("")
        db[col] = db[col].apply(lambda x: preprocess_text(x, False))
        # do not remove punctuations because in "Other lenders" the different names are divided using punctatons
    elif col in ['country', "primary_fuel"]:
        db[col] = db[col].astype("string")
        db[col] = db[col].fillna("")

for col in ['primary_fuel', "country"]:
    if col in db.columns:
        print(f"NaNs in {db_name} for {col}: {db.loc[db[col] == ''].shape[0]}")

"""## 3. Match data with already existing data

#### 3.0 Anlyze the matches

Things taken into account:
* there is one ID per row both in sais_cla+iad_gegi and in db_pp
* in Debt there could be multiple rows matching the same bu_id but they all have the same power plant name so it's fine to pick just the first power plant name as a representative (when we do the .values[0])
* we need to standardize everything (lower the names, etc.) to compare the names

Conclusion:
* the same bu_id represent the same project on the differnent datasets (the names are the same basically)
"""

col = "power_plant_name"
for db_tmp in [sais_cla, iad_gegi]:
    db_tmp[col] = db_tmp[col].astype("string")
    db_tmp[col] = db_tmp[col].fillna("")
    db_tmp[col] = db_tmp[col].apply(lambda x: preprocess_text(x))

# check that for each ID in Debt there is only one value per year, plant name, capacitym fuel and country across all the rows with that ID
# check that across all the rows with the same ID there are different investors!
multiple_rows_ids = [x for x in db_pp['bu_id'].unique() if db_pp.loc[db_pp['bu_id'] == x].shape[0] > 1]
print(f"IDs in db_pp that have more than one row: {len(multiple_rows_ids)}\n")
for col in ['debt_investment_year', 'power_plant_name', 'installed_capacity', 'primary_fuel','country', "debt_id", "PP_key"]:
    print(f"All of these IDs have only one matching {col}: {len([x for x in multiple_rows_ids if db_pp.loc[db_pp['bu_id'] == x][col].nunique() == 1]) == len(multiple_rows_ids)}")

print(f"\nAll the rows with the same ID have unique investors: {len([x for x in multiple_rows_ids if db_pp.loc[db_pp['bu_id'] == x]['investor_name'].nunique() == db_pp.loc[db_pp['bu_id'] == x].shape[0]]) == len(multiple_rows_ids)}")

# shared bu_ids between sais_cla & iad_gegi
print(f"Shared bu_ids between SAIS_CLA and IAD_GEGI: {len([x for x in db['bu_id'].value_counts() if x > 1])}")

# shared bu_ids between sais_cla+iad_gegi & Debt
print(f"Shared bu_ids between SAIS_CLA+IAD_GEGI and Debt: {len(set(db['bu_id']) & set(db_pp['bu_id']))}")
print(f"Shared bu_ids between SAIS_ClA and Debt: {len(set(sais_cla['bu_id']) & set(db_pp['bu_id']))}")
print(f"Shared bu_ids between IAD_GEGI and Debt: {len(set(iad_gegi['bu_id']) & set(db_pp['bu_id']))}")

# SAIS_CLA

# compute average similarity score
shared_debt_sais = list(set(sais_cla['bu_id']) & set(db_pp['bu_id']))
sim_scores_sais = []
to_show_sais = []
for id in shared_debt_sais:
    pp_name_sais = sais_cla.loc[sais_cla['bu_id'] == id]['power_plant_name'].values[0]
    pp_name_debt = db_pp.loc[db_pp['bu_id'] == id]['power_plant_name'].values[0]
    score = fuzz.partial_ratio(pp_name_sais, pp_name_debt)
    sim_scores_sais.append(score)
    to_show_sais.append([pp_name_sais, pp_name_debt, score])

print(f"Mean: {statistics.mean(sim_scores_sais)}")
print(f"Min value: {min(sim_scores_sais)}")
print(f"Max value: {max(sim_scores_sais)}")
for thr in [80, 90, 95, 100]:
    print(f"% over {thr}: {len([x for x in sim_scores_sais if x >= thr]) / len(sim_scores_sais) * 100}")

# shows those values that are not exaclty matching
res_sais_df = pd.DataFrame(to_show_sais, columns=["SAIS_CLA", "Debt", "Score"])
res_sais_df.drop(res_sais_df.loc[res_sais_df['SAIS_CLA'] == res_sais_df['Debt']].index)

# IAD_GEGI

# compute average similarity score
shared_debt_iad = list(set(iad_gegi['bu_id']) & set(db_pp['bu_id']))
sim_scores_iad = []
to_show_iad = []
for id in shared_debt_iad:
    pp_name_iad= iad_gegi.loc[iad_gegi['bu_id'] == id]['power_plant_name'].values[0]
    pp_name_debt = db_pp.loc[db_pp['bu_id'] == id]['power_plant_name'].values[0]
    score = fuzz.partial_ratio(pp_name_iad, pp_name_debt)
    sim_scores_iad.append(score)
    to_show_iad.append([pp_name_iad, pp_name_debt, score])

# shows those values that are not exaclty matching
print(f"Mean: {statistics.mean(sim_scores_iad)}")
print(f"Min value: {min(sim_scores_iad)}")
print(f"Max value: {max(sim_scores_iad)}")
for thr in [80, 90, 95, 100]:
    print(f"% over {thr}: {len([x for x in sim_scores_iad if x >= thr]) / len(sim_scores_iad) * 100}")

res_iad_df = pd.DataFrame(to_show_iad, columns=["IAD_GEGI", "Debt", "Score"])
res_iad_df.drop(res_iad_df.loc[res_iad_df['IAD_GEGI'] == res_iad_df['Debt']].index)

"""So, we can declare that the shared IDs do refer to the same thing so they can be used to do the matching.

Note: at the beginning we thought of using a threshold to eliminate those matches that are (very) dissimilar in the "power_plant_name" but, by manually verifying the matches, it seems that all the matches are good ones so there is no need for the filtering.
"""

# example: the lowest match score is in iad_gegi
worst = res_iad_df.loc[res_iad_df['Score'] == min(res_iad_df['Score'])]
print(f"Score: {worst['Score'].values[0]}")
print(f"IAD_GEGI name: {worst['IAD_GEGI'].values[0]}")
print(f"Debt name: {worst['Debt'].values[0]}")
# this is a good match because: "hydrolelectric" and "hydropower" are synonyms; "sopladora" from the IAD_GEGi name is in the "pautesopladora" from Debt

"""#### 3.1 Get the entries in SAIS_CLA/IAD_GEGI that needs to be merged"""

ids_to_merge = list(set(db['bu_id']) & set(db_pp['bu_id']))
print(f"IDs to merge #: {len(ids_to_merge)}")
print(f"IDs to merge % / IDs in sais_cla+iad_gegi: {len(ids_to_merge) / db['bu_id'].nunique() * 100}")
print(f"IDs to merge % / IDs already in Debt: {len(ids_to_merge) / db_pp['bu_id'].nunique() * 100}")

"""#### 3.2 Merge

The information to merge is only "debt_investment_year".

All the others columns (but the IDs) are either to be determined at the consolidation stage ("debt_investment_amount", "number_of_lenders" are computed once we have all the matching transactions).

The rest of the columns are IDs that are used to merge ("bu_id"), that belong to Debt which were previously computed ("debt_id", "PP_key") or that are not in sais_cla+iad_gegi ("j_id", "r_id")
"""

debt.columns

# let's see how different the IDs are in terms of the matching debt_investment_year

# compute average similarity score
diffs_year = []
to_show = []
for id in ids_to_merge:
    year_sais_iad = db.loc[db['bu_id'] == id]['debt_investment_year'].values[0]
    year_debt = db_pp.loc[db_pp['bu_id'] == id]['debt_investment_year'].values[0]
    score = abs(year_sais_iad - year_debt)
    diffs_year.append(score)
    to_show.append([year_sais_iad, year_debt, score])

print(f"Mean: {statistics.mean(diffs_year)}")
print(f"Min value: {min(diffs_year)}")
print(f"Max value: {max(diffs_year)}")

pd.DataFrame(to_show, columns=['SAIS_CLA+IAD_GEGI', "Debt", "Difference"])

"""So, there is no need to merge the info as of _now_ because they have the same information.

But for the future make sure to still implement in case there is a difference in the data:
* get the year that comes out of SAIS_CLA+IAD_GEGI as this one is the more accurate
"""

# TODO: do so

# get the rows in Debt that have already a match and seperate from those that don't

# those that have a match
already_matched_db = db.loc[db['bu_id'].isin(ids_to_merge)]
already_matched_db = my_reset_index(already_matched_db)
len(already_matched_db) == len(ids_to_merge)

# those that dont
not_matched_db = db.loc[~db['bu_id'].isin(ids_to_merge)]
not_matched_db = my_reset_index(not_matched_db)
len(not_matched_db) == len(db) - len(ids_to_merge)

# get the debt_id and PP_key for those that have a match
already_matched_db['debt_id'] = ""
already_matched_db['PP_key'] = ""
for i, row in already_matched_db.iterrows():
    # get the information regarding PP_key and debt_id from Debt
    id = row['bu_id']
    debt_row = db_pp.loc[db_pp['bu_id'] == id]
    # save (remember: if there are multiple rows in debt_row these have all the same debt_id and PP_key, see section 3.0)
    already_matched_db.at[i, "debt_id"] = debt_row['debt_id'].values[0]
    already_matched_db.at[i, "PP_key"] = debt_row['PP_key'].values[0]

"""#### 3.4 Fix matched rows that have multiple investors per row"""

# check if the investors in SAIS_CLA+IAD_GEGI are already in Debt

counter = 0
ids_no_match_investor = []
for id in ids_to_merge:
    # get the investor name in SAIS_CLA+IAD_GEGI
    investor_sais_iad = db.loc[db['bu_id'] == id]['Lender'].values[0]
    # get the investor names alaredy in Debt
    investors_debt = db_pp.loc[db_pp['bu_id'] == id]['investor_name'].values
    # see if the investor name from SAIS_CLA+IAD_GEGI are in the list of investor names in Debt
    if investor_sais_iad in investors_debt:
        counter += 1
    else:
        # save the ID for those that don't have a match already in Debt for the investor_name
        ids_no_match_investor.append(id)

print(f"IDs with investor already in Debt #: {counter}")
print(f"IDs with investor already in Debt %: {counter / len(ids_to_merge) * 100}")
print(f"IDs without investor already in Debt #: {len(ids_no_match_investor)}")

ids_no_match_investor

"""Of these three that don't have a match:
* for 'SD.002' and 'SD.063' this is because they have investors that are indeed different than what already in Debt
* for the last, 'ZM.025' and 'AO.061', it is because there are two names in the "Lender" cell. These two banks are actually alredy in Debt (with slighly different names).
    * we will need to manually deal with this
"""

# get the row to modify
ids_to_fix_list  = ['AO.061',  'ZM.025']

already_matched_db.loc[already_matched_db['bu_id'].isin(ids_to_fix_list)].sort_values(by="bu_id")['Lender']

"""Solution:
* create copies of the these rows so that there is a row per each investor
* distribute the investors and the amounts across the rows
* concat the newly created rows
"""

for id_to_fix in ids_to_fix_list:
    # get the row of this ID
    old_row = already_matched_db.loc[already_matched_db['bu_id'] == id_to_fix]
    # get its index
    old_row_index = list(old_row.index)[0]

    # get the investors
    old_lender_value = old_row['Lender'].values[0].replace(".,", ".") # we remove this unneeded comma because it comes from this "ping an bank co., ltd."
    # which is just one investor: if we keep the "," then when we split based on "," then we will get "ltd" separeated from the rest
    investors = [x.replace("mixedcn:", "").strip() for x in old_lender_value.split(",")]

    # create a new row for each new investor - 1 (we will also re-use the row in db_matched to distribute the investors and amount)
    new_row = old_row.copy(deep=True)
    new_rows = pd.concat([new_row] * (len(investors) - 1))
    new_rows = my_reset_index(new_rows)

    # make subsitution
    # add one investor in the old row
    already_matched_db.at[old_row_index, "Lender"] = investors[0]
    # add the other investors in the newly created rows
    investor_index = 1
    for i, row in new_rows.iterrows():
        new_rows.at[i, "Lender"] = investors[investor_index]
        investor_index += 1

    # distribute investor amount
    total_amount = old_row['debt_investment_amount'].values[0]
    # make subsitution
    already_matched_db.at[old_row_index, "debt_investment_amount"] = total_amount / len(investors)
    new_rows["debt_investment_amount"] = total_amount / len(investors)

    # concat
    already_matched_db = pd.concat([already_matched_db, new_rows])

# since we concatenate we need to fix the index
already_matched_db = my_reset_index(already_matched_db)

# check
already_matched_db.loc[already_matched_db['bu_id'].isin(ids_to_fix_list)].sort_values(by="bu_id")

# check (I knwo there must be 8 resulting things)
already_matched_db.loc[already_matched_db['bu_id'].isin(ids_to_fix_list)].shape[0] == 8

"""Now that these problematic rows are fixed, we then need to clean their investors names."""

# seperate them the rows that stemmed from the problematic rows from the rest with dropping
tmp = already_matched_db.loc[already_matched_db['bu_id'].isin(ids_to_fix_list)].copy(deep=True)
to_drop = already_matched_db.loc[already_matched_db['bu_id'].isin(ids_to_fix_list)].index.to_list()
already_matched_db = already_matched_db.drop(to_drop)

# fix the investor names
tmp = fix_investor_names(tmp, "Lender", False, None, drop_duplicates=False)

# check
tmp['Lender']

# re-concatenate back together
already_matched_db = pd.concat([already_matched_db, tmp])
already_matched_db = my_reset_index(already_matched_db)

# check they are still there
already_matched_db.loc[already_matched_db['bu_id'].isin(ids_to_fix_list)].shape[0] == 8

"""## 4. Match with Power Plants"""

not_matched_db = my_reset_index(not_matched_db)

not_matched_db.columns

# load the thresholds and methods and parameters to use
thresholds_df = map_table[matching_threshold_sheet]
methods_df = map_table[matching_method_sheet]
parameters_df = map_table[matching_paremeter_sheet]
thrs_dict_pp, mtds_dict_pp, prmts_dict_pp = get_thresholds_methods_dicts(thresholds_df, methods_df, parameters_df, db_name, "Power Plant")

# prepare the pp_full dataset for the matching (it fixes up the year_range and then creates the upper and lower limit columns which are needed)
pp_full = prepare_pp_full_for_matching_commissioning_year(pp_full)

# no. of matches in default settings: 13
db_matched_plant = find_matches_power_plant_or_fill_PP_key(not_matched_db, pp_full, "PLANTKEY_SAISCLA_IADGEGI_",
                                                            thresholds_dict=thrs_dict_pp,
                                                            methods_dict = mtds_dict_pp, equity_or_debt="Debt", parameters_dict=prmts_dict_pp)

# show matches in SAIS_CLA+IAD_GEGI data
db_matched_plant.loc[~db_matched_plant['PP_key'].str.contains("SAIS")].sort_values(by="PP_key").head()

# show the original power plants
pp_full.loc[pp_full['PP_key'].isin(db_matched_plant.loc[~db_matched_plant['PP_key'].str.contains("SAIS")]['PP_key'].values)].sort_values(by="PP_key").head()

examples_with_comm_year = pp_full.loc[(pp_full['PP_key'].isin(db_matched_plant.loc[~db_matched_plant['PP_key'].str.contains("SAIS")]['PP_key'].values)) & (~pp_full['commissioning_year'].isna())]['PP_key'].unique()
db_matched_plant.loc[db_matched_plant['PP_key'].isin(examples_with_comm_year)].sort_values(by="PP_key")

pp_full.loc[pp_full['PP_key'].isin(examples_with_comm_year)].sort_values(by="PP_key")

# show example of the commissioning_year
example = "PLANTKEY_WEPPGPPD_45612"
db_matched_plant.loc[db_matched_plant['PP_key'] == example]

pp_full.loc[pp_full['PP_key'] == example]

# check that the matching went fine
# do some checks
matched = db_matched_plant
newly_created_name = "SAIS"

# check that all the entries now have a proper PP_key

# no. of matched power_plants
key_wepp = matched.loc[matched['PP_key'].str.contains("WEPPGPPD")].shape[0]
key_bucgp = matched.loc[matched['PP_key'].str.contains("BUCGP")].shape[0]
key_bucgef = matched.loc[matched['PP_key'].str.contains("BUCGEF")].shape[0]
# key_sais = matched.loc[matched['PP_key'].str.contains("SAIS")].shape[0]
# key_fdi = matched.loc[matched['PP_key'].str.contains("FDIMARKETS")].shape[0]
# key_ijg = matched.loc[matched['PP_key'].str.contains("IJG")].shape[0]

# no. of newly created keys, these are new entries
key_new = matched.loc[matched['PP_key'].str.contains(newly_created_name)].shape[0]

# check that matched keys got the PP_key from Power Plant
print(f"All matched keys got the PP_key from Power Plant: {key_wepp + key_bucgp + key_bucgef == matched.loc[~(matched['PP_key'].str.contains(newly_created_name))].shape[0]}")

# check that each row has a PP_key
print(f"Each row has a PP_key: {key_wepp + key_bucgp + key_bucgef + key_new == matched.shape[0]}")

pd.DataFrame(data=[["WEPP + GPPD", key_wepp], ["BUCGP", key_bucgp], ["BUCGEF", key_bucgef], ["SAIS_CLA + IAD_GEGI", key_new]], columns=['Data source', "Count"])

# # check that the scores are above the increased threshold (threshold * 1.1) for power_plant_name
# # since we are matching when there is no primary_fuel

# This doesn't apply anymore because we have more primary fuels!
# scores = []
# for i, row in matched.loc[~(matched['PP_key'].str.contains(newly_created_name))].iterrows():
#     sais_iad_name = row['power_plant_name']
#     pp_full_name = pp_full.loc[pp_full['PP_key'] == row['PP_key']]['power_plant_name'].values[0]
#     scores.append(fuzz.partial_ratio(sais_iad_name, pp_full_name))

# print(f"Mean: {statistics.mean(scores)}")
# print(f"Min value: {min(scores)}")
# print(f"Max value: {max(scores)}")

"""## 5. Create tables

#### 5.0 Create the empty tables
"""

db_structure = map_table[db_structure_sheet]
tables_df = get_tables_to_create_empty(db_structure)

# used as support for the creation
tables_df_tmp = get_tables_to_create_empty(db_structure)

"""#### 5.0 Deal with duplicates Power Plant and City info"""

examples = ['volta lake resettlement township electrification project', 'solar energy project in northern kordofan', 'tanzania petroleum development corporationnatural gas processing plants']
db_matched_plant.loc[db_matched_plant['power_plant_name'].isin(examples)].sort_values(by="power_plant_name")

from functions.cleaning import fix_duplicated_power_plants_keys

db_matched_plant, diff = fix_duplicated_power_plants_keys(db_matched_plant,list(tables_df['Power Plant'].columns) + list(tables_df['City'].columns), "PLANTKEY_SAISCLA_IADGEGI_")

db_matched_plant.loc[db_matched_plant['power_plant_name'].isin(examples)].sort_values(by="power_plant_name")

"""#### 5.1 Add debt_id"""

# create unique ids with custom function
db_matched_plant = create_unique_id(db_matched_plant, "debt_id", "DEBTID_SAISCLA_IADGEGI_")
db_matched_plant.head()

db_matched_plant.loc[~db_matched_plant['PP_key'].str.contains("SAIS")].shape[0]

"""#### 5.2 Adding to the Debt table

Since before we updated the Debt table, then we can just get the new debt information and then concatenate them here.
"""

# fill the db that can already be filled
# TODO: put "Equity" in a variable (not called equity_name)
entity = "Debt"
for column in tables_df[entity].columns:
    if column in db_matched_plant.columns:
        tables_df[entity][column] = db_matched_plant[column]

tables_df['Debt'].head()

# make sure that the number_of_lenders column is NaN always (number_of_lenders is filled only with REF_LOAN data)
tables_df["Debt"]['number_of_lenders'] = np.nan

tables_df['Debt'] = pd.concat([tables_df['Debt'], debt])

"""#### 5.3 Add to Transaction and Investor

##### Deal with the rows that were matched at the debt level
"""

# we need to rename some columns to match the columns in Investor and Transaction
# Note: IAD_GEGI and SAIS_CLA do have the same columns names so we can just use one of the two to get the renaimings for the concatenated db
renamings = get_variables_advanced(vertical_slicing, iad_gegi_name, ['Investor', "Transaction"])
# the following is not in the vertical_slicing (which comes from Variables.xlsx) because we created these two columns not from data coming from the sources
renamings['debt_id'] = "investment_id"
renamings

# do renaming
already_matched_db = my_reset_index(already_matched_db)
already_matched_db_v2 = already_matched_db.rename(columns=renamings)
already_matched_db_v2.head(2)

# create table now that all is set
for entity in ["Investor"]:
    for column in tables_df_tmp[entity].columns:
        if column in already_matched_db_v2.columns:
            tables_df_tmp[entity][column] = already_matched_db_v2[column]

# create table now that all is set
for entity in ["Transaction"]:
    for column in tables_df_tmp[entity].columns:
        if column in already_matched_db_v2.columns:
            tables_df_tmp[entity][column] = already_matched_db_v2[column]

    # we can also add the "investment_averaged" and "investment_weighted" columns
    # all at once since we didn't average nor weighted as there was only one
    # Lender per entry, so all the investment amount goes to the only Lender in the entry
    tables_df_tmp[entity]['investment_averaged'] = "N"
    tables_df_tmp[entity]['investment_weighted'] = "N"

# manually fix the id that was an issue
# get the debt_id of this id
print(ids_to_fix_list)
debt_ids_to_fix = already_matched_db.loc[already_matched_db['bu_id'].isin(ids_to_fix_list)]['debt_id'].values

# add that for these rows the debt amount was averaged
for i, row in tables_df_tmp['Transaction'].loc[tables_df_tmp['Transaction']['investment_id'].isin(debt_ids_to_fix)].iterrows():
    tables_df_tmp['Transaction'].at[i, "investment_averaged"] = "Y"
# show changes
tables_df_tmp['Transaction'].loc[tables_df_tmp['Transaction']['investment_id'].isin(debt_ids_to_fix)]

# check: the only rows with "Y" as investment_averaged are the two of the id_to_fix (debt_id_to_fix)
tables_df_tmp['Transaction'].loc[tables_df_tmp['Transaction']['investment_averaged'] == "Y"].shape[0] == tables_df_tmp['Transaction'].loc[tables_df_tmp['Transaction']['investment_id'].isin(debt_ids_to_fix)].shape[0]

"""##### Deal with the rows that were not matched at the debt level and put everything together.

As for Debt, since we already have some data for Transaction and Investor, we concatenate this data with the data in matched.
"""

# we need to rename some columns to match the columns in Investor and Transactio
renamings = get_variables_advanced(vertical_slicing, iad_gegi_name, ['Investor', "Transaction"])
# the following two are not in the vertical_slicing (which comes from Variables.xlsx) because we created these two columns not from data coming from the sources (they are based
# on the index of the dataframes....)
renamings["debt_id"] = "investment_id"
renamings

# do renaming
# Note: we need to filter because some renaimings rules only apply to eq_agg_full and some only to dt_agg_full
db_matched_plant_v2 = db_matched_plant.rename(columns=dict(filter(lambda x: x[0] in db_matched_plant.columns, renamings.items())))
db_matched_plant_v2.head(2)

# create table now that all is set
for entity in ["Transaction", "Investor"]:
    for column in tables_df[entity].columns:
        if column in db_matched_plant_v2.columns:
            tables_df[entity][column] = db_matched_plant_v2[column]

tables_df['Transaction']["investment_averaged"] = "N"
tables_df['Transaction']["investment_weighted"] = "N"

# concat
for entity in ["Transaction", "Investor"]:
    tables_df[entity] = pd.concat([tables_df[entity], tables_df_tmp[entity]])

"""There are still the rows with multiple investor per rows. So here we deal with them: for the Transaction table, seperate the investors, distributing equally the investment equally and copying the investment_id and also clean the investor names. For Investor just get the cleaned investor_names from the fixed Transaction table."""

tables_df['Transaction'].loc[tables_df['Transaction']['investor_name'].str.contains(":")]

tables_df['Investor'].loc[tables_df['Investor']['investor_name'].str.contains(":")]

# seperate the rows to fix via dropping
tables_df['Transaction'] = my_reset_index(tables_df['Transaction']) # need for the dropping
rows_to_fix = tables_df['Transaction'].loc[tables_df['Transaction']['investor_name'].str.contains(":")].copy(deep=True)
# drop
to_drop = tables_df['Transaction'].loc[tables_df['Transaction']['investor_name'].str.contains(":")].index.to_list()
tables_df['Transaction'] = tables_df['Transaction'].drop(to_drop)
# check: the rows to fix are not in the table anymore
tables_df['Transaction'].loc[tables_df['Transaction']['investor_name'].str.contains(":")].shape[0] == 0

rows_to_fix

new_tr_rows = []
for i, row in rows_to_fix.iterrows():
    # seperate the investor names
    investor_names = [x.strip() for x in row['investor_name'].replace("mixedcn:", "").split("&")]
    print(investor_names)
    # distribute the investment
    shared_investment = row['amount'] / len(investor_names)
    # create the new rows keeping the original invesmtent_id
    for investor in investor_names:
        new_tr_rows.append([row['investment_id'], investor, shared_investment, "Y", "N", np.nan])

# put the new rows in a DataFrame for convenience
new_tr_rows_df = pd.DataFrame(new_tr_rows, columns=tables_df['Transaction'].columns)
new_tr_rows_df

# clean the investor_name of these rows
new_tr_rows_df = fix_investor_names(new_tr_rows_df, "investor_name", False, None, drop_duplicates=False)

# put the new rows in the Transaction table
tables_df['Transaction'] = pd.concat([tables_df['Transaction'], new_tr_rows_df])

tables_df['Transaction']

# drop these rows in the Investor table
to_drop = tables_df['Investor'].loc[tables_df['Investor']['investor_name'].str.contains(":")].index.to_list()
tables_df['Investor'] = tables_df['Investor'].drop(to_drop)
# check: the rows have been removed
tables_df['Investor'].loc[tables_df['Investor']['investor_name'].str.contains(":")].shape[0] == 0

# get the new investor names from new_tr_rows_df
new_tr_rows_df['parent_company_of_investor'] = np.nan # create the parent company column to facilitate this step
tables_df['Investor'] = pd.concat([tables_df['Investor'], new_tr_rows_df[['investor_name', "parent_company_of_investor"]].copy(deep=True)])

tables_df['Investor']

"""#### 5.4 Make Power Plant and City and Country for new Power Plants found here!

These are the new entries for the Power Plant & co. tables since for these entries we have found no match in the Power Plant table nor in the Debt table.
"""

missing_power_plants = db_matched_plant_v2.loc[db_matched_plant_v2['PP_key'].str.contains("PLANTKEY_SAISCLA_IADGEGI_")]
print("Missing power plant: " + str(missing_power_plants.shape[0]))

# get the data from equity
for entity in ['Power Plant', "Country"]:
    for column in tables_df[entity].columns:
        if column in missing_power_plants.columns:
            tables_df[entity][column] = missing_power_plants[column]

entity = "City"
for column in tables_df[entity].columns:
    if column in missing_power_plants.columns:
        tables_df[entity][column] = missing_power_plants[column]
    elif column == "city_key":
        tables_df[entity][column] = missing_power_plants["PP_key"]

# drop the duplicates that we fixed before
before_dropping = tables_df['Power Plant'].shape[0]
tables_df['Power Plant'] = tables_df['Power Plant'].drop_duplicates()
rows_dropped =  before_dropping - tables_df['Power Plant'].shape[0]
print(f"Rows dropped: {rows_dropped}")
print(f"Check: the rows we are dropping now are the same as the rows that we found as duplicated before: {rows_dropped == diff}")

before_dropping = tables_df['City'].shape[0]
tables_df['City'] = tables_df['City'].drop_duplicates()
rows_dropped =  before_dropping - tables_df['City'].shape[0]
print(f"Rows dropped: {rows_dropped}")
print(f"Check: the rows we are dropping now are the same as the rows that we found as duplicated before: {rows_dropped == diff}")

# show that we dropped by showing the examples of before: now we only have one row per key!
keys_examples = tables_df['Power Plant'].loc[tables_df['Power Plant']['power_plant_name'].isin(examples)]['PP_key'].unique()
tables_df['Power Plant'].loc[tables_df['Power Plant']['power_plant_name'].isin(examples)]

tables_df['City'].loc[tables_df['City']['city_key'].isin(keys_examples)]

"""#### 5.6 Visual check"""

tables_df['Power Plant'].head()

# rows that do have a power_plant_name, primary_fuel, capacity
for col in ["power_plant_name", "primary_fuel"]:
    print(f"Valid {col}: {tables_df['Power Plant'].loc[tables_df['Power Plant'][col] != ''].shape[0]}")
col = "installed_capacity"
print(f"Valid {col}: {tables_df['Power Plant'].loc[~tables_df['Power Plant'][col].isna()].shape[0]}")

tables_df['City'].head()

tables_df['Country'].head()

tables_df['Debt']

tables_df['Transaction']

# count the rows that were matched and not at the investment level
matched_invst_no = tables_df['Transaction'].loc[~tables_df['Transaction']['investment_id'].str.contains("SAISCLA")].shape[0]
not_matched_invst_no = tables_df['Transaction'].loc[tables_df['Transaction']['investment_id'].str.contains("SAISCLA")].shape[0]
print(f"Matched at the investment level: {matched_invst_no}")
print(f"Not matched at the investment level: {not_matched_invst_no}")
print(f"All rows have an investment ID: {matched_invst_no + not_matched_invst_no == tables_df['Transaction'].shape[0] and tables_df['Transaction']['investment_id'].isna().sum() == 0}")

for col in ["investment_averaged", 	"investment_weighted"]:
    print(f"Missing {col}: {tables_df['Transaction'].loc[tables_df['Transaction'][col].isna()].shape[0] + tables_df['Transaction'].loc[tables_df['Transaction'][col] == ''].shape[0] }")

# the only rows that have "Y" as averaged are the eight that we manually fixed in step 3 and the 4 resulting from the fix above
tables_df['Transaction'].loc[tables_df['Transaction']['investment_averaged'] == "Y"].shape[0] == 8 + 4

tables_df['Investor']

# there are no values for "parent_company_of_investor" at all
tables_df['Investor'].loc[tables_df['Investor']['parent_company_of_investor'].isna()].shape[0] == tables_df['Investor'].shape[0]

"""#### 5.5 Save"""

print_final_statistics(tables_df, tables_to_create)

intermediate_folder

# all entities but Debt get saved in the intermediate folder
for entity in ['Power Plant', 'City', 'Country', 'Transaction', 'Investor']:
    tables_df[entity].to_csv(intermediate_folder + entity + "_SAISCLA_IADGEGI.csv", index=False)

# since Debt is ready and doesn't need any further post-processing, we can save it here
entity = "Debt"
tables_df['Debt'].to_csv(current_final_tables_folder + entity + ".csv", index=False)

