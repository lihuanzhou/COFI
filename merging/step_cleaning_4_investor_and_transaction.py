# -*- coding: utf-8 -*-
"""Step CLEANING - 4 - Investor and Transaction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FXziNvEvNfUNpFSNEKdsE2WHM8cQpguk

<font color=red><i><b>Since Debt and Equity are constantly updated, we don't have always have duplicates in Debt to remove (that's for Step 5 when we deal with the issues with BU_CPG).

TODO:
* Step 5:
    * FDI: do we discard the BUCGP value if there is an FDI entry on the same plant? isn't FDI also very unreliable?
</i></b></font>
"""

import pandas as pd
from functions.cleaning import preprocess_text
from functions.cleaning import get_all_files, my_reset_index, extract_between_underscore_period
from functions.cleaning import my_read_excel
from functions.cleaning import count_sources_in_id

from functions.cleaning import change_parent_company

# needed folder paths
input_folder = "Cleaned Data/" # or we can put this info in the "files name" sheet
intermediate_folder = "Intermediate Results/" # to save some intermediate results
output_folder = "Current Final Tables/"

# load the excel containing all the mapping information
map_table_name = "Variables.xlsx"
map_table = pd.read_excel(map_table_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(map_table))


# the sheet where there is the information that the client can change
files_names_sheet = "file names"
groupby_info_sheet = "formatting"
vertical_slicing_info_sheet = "variables to use"
aggregation_info_sheet = "aggregation"
db_structure_sheet = "db structure"
un_country_info_sheet = "UN Country Info"
country_names_dict_sheet = "country names dicionary"
bri_countries_sheet = "BRI countries"

# the name of the Country table
investor_name = "Investor"
transaction_name = "Transaction"
debt_name = "Debt"

# list of the names of the various Country tables
# TODO: uniform all the names of these files
#investor_files_list = ["Investor_need_post_processing_BU_CGP_.xlsx", "BU_CGEF_embedding_power_generation/Investor_BUCGEF.xlsx", "BU_CGEF_embedding_multipurpose/Investor_BUCGEF.xlsx"]
#transaction_files_list = ["Transaction_need_post_processing_BU_CGP_.xlsx", "BU_CGEF_embedding_power_generation/Transaction_BUCGEF.xlsx", "BU_CGEF_embedding_multipurpose/Transaction_BUCGEF.xlsx"]
# investor_files_list = ["Investor_need_post_processing_BU_CGP_.xlsx", "BU_CGEF_fuzzy_matching - fuzz.partial_ratio/Investor_BUCGEF.xlsx", "Investor_SAISCLA_IADGEGI.xlsx", "Investor_FDI_RMA.xlsx"]
# transaction_files_list = ["Transaction_need_post_processing_BU_CGP_.xlsx", "BU_CGEF_fuzzy_matching - fuzz.partial_ratio/Transaction_BUCGEF.xlsx", "Transaction_SAISCLA_IADGEGI.xlsx", "Transaction_FDI_RMA.xlsx"]
# investor_files_list = ["Investor_need_post_processing_BU_CGP_.xlsx", "BU_CGEF_fuzzy_matching - fuzz.partial_ratio/Investor_BUCGEF.xlsx", "Investor_SAISCLA_IADGEGI.xlsx", "Investor_FDI_RMA.xlsx", "Investor_IJ_Global.xlsx"]
# transaction_files_list = ["Transaction_need_post_processing_BU_CGP_.xlsx", "BU_CGEF_fuzzy_matching - fuzz.partial_ratio/Transaction_BUCGEF.xlsx", "Transaction_SAISCLA_IADGEGI.xlsx", "Transaction_FDI_RMA.xlsx", "Transaction_IJ_Global.xlsx"]

"""List of sources to clean and merge together after processing each data source:
* after BU_CGP:
    * investor: ['Investor_BUCGP.xlsx']
    * transaction: ['Transaction_BUCGP.xlsx']
* after BU_CGEF:
    * investor: ['Investor_BUCGEF.xlsx', 'Investor_BUCGP.xlsx']
    * transaction: ['Transaction_BUCGEF.xlsx', 'Transaction_BUCGP.xlsx']
* after SAIS_CLA+IAD_GEGI:
    * investor: ['Investor_BUCGEF.xlsx', 'Investor_BUCGP.xlsx', 'Investor_SAISCLA_IADGEGI.xlsx']
    * transaction: ['Transaction_BUCGEF.xlsx', 'Transaction_BUCGP.xlsx', 'Transaction_SAISCLA_IADGEGI.xlsx']
* after IJ_Global:
    * investor: ['Investor_IJ_Global.xlsx', 'Investor_SAISCLA_IADGEGI.xlsx', 'Investor_BUCGEF.xlsx', 'Investor_BUCGP.xlsx']
    * transaction: ['Transaction_IJ_Global.xlsx', 'Transaction_SAISCLA_IADGEGI.xlsx', 'Transaction_BUCGEF.xlsx', 'Transaction_BUCGP.xlsx']
* after REF_MA:
    * investor:['Investor_REFINITIVMA.xlsx',
 'Investor_IJ_Global.xlsx',
 'Investor_SAISCLA_IADGEGI.xlsx',
 'Investor_BUCGEF.xlsx',
 'Investor_BUCGP.xlsx']
    * transaction: ['Transaction_REFINITIVMA.xlsx',
 'Transaction_IJ_Global.xlsx',
 'Transaction_SAISCLA_IADGEGI.xlsx',
 'Transaction_BUCGEF.xlsx',
 'Transaction_BUCGP.xlsx']
* after FDI_markets:
    * investor:['Investor_FDIMARKETS.xlsx',
 'Investor_REFINITIVMA.xlsx',
 'Investor_IJ_Global.xlsx',
 'Investor_SAISCLA_IADGEGI.xlsx',
 'Investor_BUCGEF.xlsx',
 'Investor_BUCGP.xlsx']
    * transaction: ['Transaction_FDIMARKETS.xlsx',
 'Transaction_REFINITIVMA.xlsx',
 'Transaction_IJ_Global.xlsx',
 'Transaction_SAISCLA_IADGEGI.xlsx',
 'Transaction_BUCGEF.xlsx',
 'Transaction_BUCGP.xlsx']
* after REF_LOAN:
    * investor:['Investor_REFINITIVLOAN.xlsx',
 'Investor_FDIMARKETS.xlsx',
 'Investor_REFINITIVMA.xlsx',
 'Investor_IJ_Global.xlsx',
 'Investor_SAISCLA_IADGEGI.xlsx',
 'Investor_BUCGEF.xlsx',
 'Investor_BUCGP.xlsx']
    * transaction: ['Transaction_REFINITIVLOAN.xlsx',
 'Transaction_FDIMARKETS.xlsx',
 'Transaction_REFINITIVMA.xlsx',
 'Transaction_IJ_Global.xlsx',
 'Transaction_SAISCLA_IADGEGI.xlsx',
 'Transaction_BUCGEF.xlsx',
 'Transaction_BUCGP.xlsx']

"""

# get the files
investor_files_list = get_all_files(intermediate_folder, "Investor")
investor_files_list

transaction_files_list =  get_all_files(intermediate_folder, "Transaction")
transaction_files_list

"""#### 0. Load data

Concatanate the data from the list of files.
"""

# read all files
datasets_inv = []
for file in investor_files_list:
    datasets_inv.append(my_read_excel(intermediate_folder , file))

# concatanate
investor = pd.concat(datasets_inv)
if "Unnamed: 0" in investor.columns:
    investor = investor.drop(columns=['Unnamed: 0'])
print("Shape: " + str(investor.shape))

# check that there were no problems with concat: all entries have been added
print("Concat went smoothly: " + str(investor.shape[0] == sum([x.shape[0] for x in datasets_inv])))

investor.head()

datasets_inv[0].shape

# we will use this function to extract from the file name where the data comes from
for file in transaction_files_list:
    print(f"{file}: {extract_between_underscore_period(file)}")

# read all files
datasets_tr = []
for file in transaction_files_list:
    # load dataset
    tmp = my_read_excel(intermediate_folder , file)
    # add where the data is from
    datasource_name = extract_between_underscore_period(file)
    tmp['original_source'] = datasource_name
    # save dataset
    datasets_tr.append(tmp)

# concatanate
transaction = pd.concat(datasets_tr)
if "Unnamed: 0" in transaction.columns:
    transaction = transaction.drop(columns=['Unnamed: 0'])
print("Shape: " + str(transaction.shape))

# check that there were no problems with concat: all entries have been added
print("Concat went smoothly: " + str(transaction.shape[0] == sum([x.shape[0] for x in datasets_tr])))

transaction.head()

datasets_tr[0].shape

"""#### 1. Do some string manipulation"""

investor.isna().sum()

transaction.isna().sum()

# fill NaNs just to make it easire to process these files
investor = investor.fillna("")
transaction = transaction.fillna("")

# fix the types
investor['investor_name'] = investor['investor_name'].astype("string")
investor['parent_company_of_investor'] = investor['parent_company_of_investor'].astype("string")
transaction['investor_name'] = transaction['investor_name'].astype("string")

# pre-process the text
investor['investor_name'] = investor['investor_name'].apply(lambda x: preprocess_text(x))
transaction['investor_name'] = transaction['investor_name'].apply(lambda x: preprocess_text(x))

investor.head()

transaction.head()

# TODO: fix these manually most likely
transaction.loc[transaction['investor_name'].str.contains("cofinancing")]

"""#### 2. Drop duplicates in Investor

This is where we can update the notebook: we can see if there are similar company names (e.g.,  because of something as simple misspellings to synonyms) and then group the entries based on their similarities and so on.
"""

before = investor.shape[0]
print("Entries in Investor before: " + str(before))
investor = investor.drop_duplicates()
after = investor.shape[0]
print("Entries in Investor after: " + str(after))
perc = after / before * 100
print("Current entries % over original entries: " + str(perc))

"""#### 3. Drop duplicates in Transaction

Since when merging the information in Debt we didn't check if the information in Transaction was already present, then we can drop.
"""

# TODO: a more thorough merging would given the same investment_id and investor_name, take the max investment information

# before = transaction.shape[0]
# print("Entries in Transaction before: " + str(before))
# transaction = transaction.drop_duplicates(subset=["investment_id", "investor_name", "amount"])
# after = transaction.shape[0]
# print("Entries in Transaction after: " + str(after))
# perc = after / before * 100
# print("Current entries % over original entries: " + str(perc))

transaction.head()

"""#### 4. Merge info for all entries with parent company"""

# these are all the company that do have a parent company
parents = investor.loc[investor['parent_company_of_investor'] != ""]
parents

"""The idea is:
* I work on each company name that has at least one entry with the parent entry and if there are multiple entries for that same company
* i collect the index of all these entries for the same company. Then I drop in Investor all these entries for all these companies
* I get the names of the parent company
* I create a new dataframe of Investor with the names of the comapnies and their matchin parents company names and then concat it to the original Investor
"""

if len(parents) == 0:
    # there is no consolidation needed
    print("There is no consolidation needed for the parent companies!")
else:
    # consolidation is needed
    index_to_drop = []
    new_parents_row = []

    investor = my_reset_index(investor)

    for i, row in parents.iterrows():
        # get all the entries for the same company
        multiples = investor.loc[investor['investor_name'] == row['investor_name']]
        multiples_no = multiples.shape[0]

        # this is applicable only for those company that are present multiple times in the Investor df
        if multiples_no > 1:
            print(f"{row['investor_name']}: {multiples_no}")

            # collect the index so that we can remove these entries later
            index_to_drop.extend(list(multiples.index))

            # get all the parents names
            parents_names = list(multiples["parent_company_of_investor"].values)
            while "" in parents_names:
                # since some may not have a parent, then they return an empty value
                parents_names.remove("")

            # create the rows for the new dataframe
            new_parents_row.append([row['investor_name'], ", ".join(parents_names)])

    # drop the entries that are now summarized in the new dataframe
    investor = investor.drop(index_to_drop)

    # creete the new dataframe
    new_parents = pd.DataFrame(new_parents_row, columns=investor.columns)

    # drop duplicates: there are duplicates if the same company has multiple rows with different parent companies
    new_parents = new_parents.drop_duplicates()

    # concat
    investor_shape_before = investor.shape[0]
    investor = pd.concat([investor, new_parents])
    # check
    print(f"Consolidation went correctly: {investor_shape_before + new_parents.shape[0] == investor.shape[0]}")

investor.loc[investor['parent_company_of_investor'] != ""]

with_parent = investor.loc[investor['parent_company_of_investor'] != ""].shape[0]
without_parent = investor.loc[investor['parent_company_of_investor'] == ""].shape[0]
print(f"Investors WITH parent company: {with_parent}, {with_parent / investor.shape[0] * 100} %")
print(f"Investors WITHOUT parent company: {without_parent}, {without_parent / investor.shape[0] * 100} %")

"""##### Add the parent company from the equity investor directory"""

investor = my_reset_index(investor)
investor = change_parent_company(mapping_df=None, db=investor, investor_column="investor_name", parent_column="parent_company_of_investor")

investor

"""#### 5. Consolidate transaction info

##### Functions to use
"""

# return True if the exact same company has multiple entries in the same debt_id (at least one company like this is needed to trigger the True)
def check_if_multiple_rows_for_same_investor_same_investment_id(x):
    investment = transaction.loc[transaction['investment_id'] == x]
    for company in investment['investor_name'].unique():
        if investment.loc[investment['investor_name'] == company].shape[0] > 1:
            return True
    return False

# TODO: here is when there is the exact match, but what about fuzzy matches? E.g., in the example above instead of having "chexim" we have "Export-Import Bank of China - chexim"
# or what if we have nicknames or the parent company issue that Ziyi mentioned in one email (there is a dictioanry of the company names to use) -> so there are probably more
# ids that need merging

# examples: DEBTID_BUCGEF_160

# but if the issues is just with a few rows then a dictionary approach is enoguh (maybe using what Ziyi sent is enough)

# TODO: we also need to remove the BU_CGP entry if there one entry for the same power plant with the same investor (but a different debt_id because we didn't match the BU_CGP debt
# rows as they don't have information for year nor amount)

# return True if the exact same company has multiple entries in the same PP_key and one is BU_CGP
def check_if_multiple_rows_for_same_investor_different_investment_id(x, debt_tr_local):
    plant = debt_tr_local.loc[debt_tr_local['PP_key'] == x]
    for company in plant['investor_name'].unique():
        company_rows = plant.loc[plant['investor_name'] == company]
        if company_rows.shape[0] > 1 and "BUCGP" in company_rows['original_source'].unique() and company_rows['original_source'].nunique() > 1 :
            return True
    return False

# for "same company, same investment_id, multiple rows" case

def get_highest_ranked_source(sources_list, debt_or_equity):
    # functions that returns the most reliable source in the given sources_list based
    # on whether we are dealing with data from "Debt" or "Equity"
    ranked_sources_debt = ['SAISCLA_IADGEGI', "BUCGEF", "IJ_Global", "REFINITIVLOAN", "BUCGP"]
    ranked_sources_equity = ["IJ_Global", "REFINITIVMA", "FDIMARKETS", "BUCGP"]
    # TODO: read this order from the Variables.xlsx file!

    # TODO: this function has been tested on actual Debt data, test it on actual Equity data (
    # that is, the rows that needed merging were all from Debt, there was no row from equity
    # that needed merging)

    # determine the ranked sources list to use based on whether it is "Debt" or "Equity" data
    if debt_or_equity == "Debt":
        ranked_sources = ranked_sources_debt
    elif debt_or_equity == "Equity":
        ranked_sources = ranked_sources_equity
    else:
        # if we get here there is an issue with the input
        print("ERROR: Transactions can either be \"Debt\" or \"Equity\"")
        return "ERROR"

    # iterate through the datasources name from most relevant to least relevant
    for datasource in ranked_sources:
        # if present then return it
        if datasource.strip() in sources_list:
            return datasource
    # if none of the datasources was here then there is a problem
    print(f"ERROR: original datasource not present! {datasource}")
    return "ERROR"

def pick_most_valuable_rows(id, investment = None):
    # function that given an ID it returns the rows that needs to be kept for that ID
    # it removes those whose information is already covered by some other more reliable
    # information

    # only for testing purposes: we can already send the pandas dataframe
    if investment is None:
        # get the rows for this transaction
        investment = transaction.loc[transaction['investment_id'] == id]
        investment = my_reset_index(investment) # reset the index so that the dropping is accurate leter on

    to_drop = [] # to take track of the rows that we need to drop in investment
    # we analyze each company
    for company_name in investment['investor_name'].unique():
        # get the rows in investment that belong to that company
        company = investment.loc[investment['investor_name'] == company_name]
        # if there is only one row per company then there is no need to process this company
        if company.shape[0] == 1:
            continue
        else:
            # if there is more than one company, then we need merging
            # determine if it is a debt or equity entry
            if "DEBTID" in id:
                debt_or_equity = "Debt"
            else:
                debt_or_equity = "Equity"
            # get the name of the most reliable source, this depends on whether it is a debt or equity entry
            # print(list([x.strip() for x in company['original_source'].unique()]))
            highest_ranked_source = get_highest_ranked_source(list([x.strip() for x in company['original_source'].unique()]), debt_or_equity)
            # print(highest_ranked_source)
            # get rows that we are NOT going to keep: these are the ones that don't belong to the best ranked source
            to_drop.extend(company.loc[company['original_source'] != highest_ranked_source].index.to_list())

    # now that we have collected the rows to drop for ALL companies, we can drop the results
    investment = investment.drop(to_drop)

    return investment

def consolidate_bucgp_rows(transaction, debt_equity, debt_equity_tr, debt_or_equity):
    # 0. Re-set indexes
    transaction = my_reset_index(transaction)
    debt_equity = my_reset_index(debt_equity)
    debt_equity_tr = my_reset_index(debt_equity_tr)
    # set up variables
    if debt_or_equity == "Debt":
        id_col = "debt_id"
    elif debt_or_equity == "Equity":
        id_col = "equity_id"
    else:
        print("ERROR!")
        return None

    # 1. Get IDs to consolidate
    pp_ids_to_consolidate_bucgp = [x for x in debt_equity_tr['PP_key'].unique() if check_if_multiple_rows_for_same_investor_different_investment_id(x, debt_equity_tr) == True]

    if len(pp_ids_to_consolidate_bucgp) == 0:
        print("There are NO IDs to consolidate!")
    else: # thereare IDs to consolidate so do that

        # 2. Drop in Transaction the BU_CGP rows
        # get the investment IDs to drop: these are the investmetn_id matchted with the PP IDs to consolidate and whose original source is BUCGP
        investment_id_to_drop = debt_equity_tr.loc[(debt_equity_tr['PP_key'].isin(pp_ids_to_consolidate_bucgp)) & (debt_equity_tr['original_source'] == "BUCGP")]['investment_id'].to_list()
        # get the matching indexes in transaction
        indexes_to_drop = transaction.loc[transaction['investment_id'].isin(investment_id_to_drop)].index.to_list()
        # drop
        transaction_before_dropping_bucgp = transaction.shape[0]
        print(f"Transaction rows # BEFORE consolidating: {transaction_before_dropping_bucgp}")
        transaction = transaction.drop(indexes_to_drop)
        print(f"Transaction rows # AFTER consolidating: {transaction.shape[0]}")
        print(f"Rows were dropped correctly in Transaction: {transaction.shape[0] == transaction_before_dropping_bucgp - len(indexes_to_drop)}")

        # 3. Transfer info from BUCGP to rows from more reliable sources and get debt_equity_ids of row to drop
        debt_equity_ids_to_drop = []
        # for each PP_key
        for id in pp_ids_to_consolidate_bucgp:
            # 3.1: get the info of the BUCGP row whose info we need to transfer to debt_equity and which will we drop

            # get the rows about this plant from debt_equity_tr: this is needed to understand which row in debt_equity to update
            # because we need to use the investor name
            plant_tr = debt_equity_tr.loc[debt_equity_tr['PP_key'] == id]
            # get from debt_equity_tr the row from BUCGP whose unique info we will transfer to
            # the others in debt_equity. For the assumption listed above there is only one row per PP_key
            bu_cgp_row = plant_tr.loc[plant_tr[id_col].str.contains("BUCGP")]
            # get the debt_equity_id that we will drop in debt_equity
            bucpg_id_to_drop = bu_cgp_row[id_col].values[0]
            # take track of this value
            debt_equity_ids_to_drop.append(bucpg_id_to_drop)

            # 3.2: get the IDs of the debt_equity rows that needs to updated

            # let's use the company in the BUCGP row to find the rows from the more reliable sources
            company = bu_cgp_row['investor_name'].values[0]
            # get the debt_equity_ids of all the rows that have this investor (this includes the BUCGP row as well)
            ids_to_update = plant_tr.loc[plant_tr['investor_name'] == company][id_col].to_list()
            # get the IDs of the other sources (not BUCGP): remove the debt_equity_id of the BUCGP row
            while bucpg_id_to_drop in ids_to_update:
                ids_to_update.remove(bucpg_id_to_drop)

            # 3.3: update the debt_equity rows with the IDs found in step 3.2 with the information from the row found in step 3.1

            # we have the rows to update (ids_to_update) and where to get the information (bu_cgp_row)
            # we update debt_equity directly
            # for each row in debt_equity that has the IDs found in step 3.2
            for i, row in debt_equity.loc[debt_equity[id_col].isin(ids_to_update)].iterrows():
                # we assign the information from the row found in step 3.1
                # the only information that the BU_CGP rows have that the other rows don't is the
                # bu_id_bu_cgp
                debt_equity.at[i, "bu_id_bu_cgp"] = bu_cgp_row["bu_id_bu_cgp"].values[0]

        # technically the investment_id_to_drop (used in Step 2) and debt_equity_ids_to_drop (used in step 3) contain the same elements because
        # both contain the IDs of the investments from BUCGP that needs to be dropped (in the first we get them from the "investment_id" column
        # whereas in the second we get it from "debt_equity_id" but because these two columns share the same information as these values are unique identifiers
        # of the investments across the tables! - we indeed use these values to join the two tables)
        # to test that they hava the same elements we look at the elements that are shared by using "set" and the "&" operator
        set_len = len(set(investment_id_to_drop) & set(debt_equity_ids_to_drop))
        print(f"We got the right variables: {set_len == len(investment_id_to_drop) and set_len == len(debt_equity_ids_to_drop)}")

        # 4. Drop in debt_equity
        # check
        print(f"All the debt_equity_ids we are dropping are from BUCGP: {len([x for x in debt_equity_ids_to_drop if 'BUCGP' in x]) == len(debt_equity_ids_to_drop)}")
        # print useful information
        debt_equity_before_dropping = debt_equity.shape[0]
        print(f"debt_equity rows # BEFORE dropping: {debt_equity_before_dropping}")
        # drop
        indexes_debt_equity_to_drop = debt_equity.loc[debt_equity[id_col].isin(debt_equity_ids_to_drop)].index.to_list()
        debt_equity = debt_equity.drop(indexes_debt_equity_to_drop)
        # print useful information
        print(f"debt_equity rows # AFTER dropping: {debt_equity.shape[0]}")
        print(f"Rows were dropped correctly: {debt_equity.shape[0] == debt_equity_before_dropping - len(indexes_debt_equity_to_drop)}")

        # print all statistics
        transaction_rows_dropped = transaction_before_dropping_bucgp - transaction.shape[0]
        debt_equity_rows_dropped = debt_equity_before_dropping - debt_equity.shape[0]

        print(f"# rows dropped in Transaction: {transaction_rows_dropped}")
        print(f"# rows dropped in debt_equity: {debt_equity_rows_dropped}")
        print(f"% rows dropped in Transaction: {transaction_rows_dropped / transaction_before_dropping_bucgp * 100}")
        print(f"% rows dropped in debt_equity: {debt_equity_rows_dropped / debt_equity_before_dropping * 100}\n")


        original_bucgp_rows = debt_equity_tr.loc[debt_equity_tr['original_source'] == "BUCGP"].shape[0]
        current_bucgp_rows = debt_equity.loc[debt_equity[id_col].str.contains("BUCGP")].shape[0]
        diff = original_bucgp_rows - current_bucgp_rows
        print(f"Original rows from BUCGP: {original_bucgp_rows}")
        print(f"Current rows from BUCGP: {current_bucgp_rows}")
        print(f"Dropped BUCGP rows: {diff}")
        print(f"% or rows dropped over original # BUCGP rows: {diff / original_bucgp_rows * 100}")
        print(f"% or rows dropped over original # debt_equity rows: {diff / debt_equity_before_dropping * 100}")

        # check that the BUCGP rows that are still in debt_equity are because there is no other row that can cover them
        # first get the new version of the debt_equity_tr table (since now we dropped rows)
        debt_equity_tr_new = debt_equity.merge(transaction, left_on=id_col, right_on="investment_id")
        # let's re-use the function we used before to determine if a PP_key needed to be consolidated: now we should get no results!
        print(f"PP_keys that still needs consolidation: {len([x for x in debt_equity_tr_new['PP_key'].unique() if check_if_multiple_rows_for_same_investor_different_investment_id(x, debt_equity_tr_new) == True])}")

    return transaction, debt_equity

def check_same_company_same_source_same_investment(x):
    # Check if in the given investment_id there is at least one company that has multiple rows
    # that come from the same data source

    # get the rows of the investment_id
    invest = transaction.loc[transaction['investment_id'] == x]
    # check for each company
    for company_name in invest['investor_name'].unique():
        # get the company's rows
        company = invest.loc[invest['investor_name'] == company_name]
        # if there is more than one row and there is only one source
        if company.shape[0] > 1 and company['original_source'].nunique() == 1:
            # then this is what we were looking for
            return True
    return False

"""##### Do merging: same company, same investment_id, multiple rows"""

# get some statistics on the data to merge
ids_to_consolidate_tmp = [x for x in transaction['investment_id'].unique() if check_if_multiple_rows_for_same_investor_same_investment_id(x) == True]
only_one_investor = [x for x in ids_to_consolidate_tmp if transaction.loc[transaction['investment_id'] == x].shape[0] == 2]
print(f"IDs with only one investor: {len(only_one_investor)}")
print(f"Examples: {only_one_investor[0:4]}\n")
more_than_one_investor = [x for x in ids_to_consolidate_tmp if transaction.loc[transaction['investment_id'] == x].shape[0] > 2]
print(f"IDs with more than one investor: {len(more_than_one_investor)}")
print(f"Examples: {more_than_one_investor[0:4]}")

equity_rows = [x for x in ids_to_consolidate_tmp if "EQUITY" in x]
print(f"Rows to consolidate that are equity rows: {len(equity_rows)}")
print(f"Examples: {equity_rows[0:5]}")

for source in ['IJGLOBAL', "REFINITIVMA","REFINITIVLOAN"]:
    print(f"IDs to consolidate in {source}: {[x for x in ids_to_consolidate_tmp if (source in x) and transaction.loc[transaction['investment_id'] == x].shape[0] > 0]}")

# example with more than one investor
transaction.loc[transaction['investment_id'] == "DEBTID_BUCGEF_1"]

# example with more than one investor
transaction.loc[transaction['investment_id'] == "DEBTID_BUCGEF_161"]

transaction.loc[transaction['investment_id'] == "DEBTID_IJGLOBAL_163"]

transaction.loc[transaction['investment_id'] == "DEBTID_IJGLOBAL_244"]

transaction.loc[transaction['investment_id'] == 'EQUITYID_IJGLOBAL_54']

transaction.loc[transaction['investment_id'] == 'EQUITYID_IJGLOBAL_6']

# transaction = transaction_safe_copy.copy(deep=True)

"""Idea:
1. we get all the IDs to consolidate with a function (as done above)
1. we use a function that given an ID it returns the rows to keep for all the IDs
1. we remove the original rows of these IDs from transaction
    * we remove after getting the consolidation step because the function we use gets the rows to consolidate from Transaction
1. we concatenate the resulting rows with the original rows that didn't need to be consolidated
"""

transaction_safe_copy = transaction.copy(deep=True)

# transaction = transaction_safe_copy.copy(deep=True)

# 0. Reset index for transaction
transaction = my_reset_index(transaction)

# 1. Get the IDs to consolidate
print("Step 1: Get the IDs to consolidate")
ids_to_consolidate = [x for x in transaction['investment_id'].unique() if check_if_multiple_rows_for_same_investor_same_investment_id(x) == True]

consolidated_rows = [] # to save the consolidated rows

if len(ids_to_consolidate) == 0:
    # there is nothing to consolidate
    print("No IDs to consoliddate!")
else:
    # there are IDs to consolidate

    # consolidate each ID at one time
    for id in ids_to_consolidate:
        # 2. Get the consolidated rows for the the given ID
        consolidated_rows.append(pick_most_valuable_rows(id))
    # check
    print(f"All the IDs have consolidated rows (no result is an empty table): {len([x for x in consolidated_rows if x.shape[0] > 0]) == len(ids_to_consolidate)}")

    before_rows_no = transaction.shape[0]
    print(f"Transaction rows # BEFORE consolidating: {before_rows_no}\n")

    # 2. Remove the original rows of the IDs to consolidate
    print("Step 2: Remove the original rows of the IDs to consolidate")
    rows_to_remove_indexes = transaction.loc[transaction['investment_id'].isin(ids_to_consolidate)].index.to_list()
    print(f"Rows to drop: {len(rows_to_remove_indexes)}")
    transaction = transaction.drop(rows_to_remove_indexes)
    # check
    after_rows_no = transaction.shape[0]
    print(f"Transaction rows # after dropping: {after_rows_no}")
    print(f"Rows dropped correctly: {before_rows_no - len(rows_to_remove_indexes) == after_rows_no}\n")

    # 3. Concat the new rows with those that didn't need any consolidation
    print("Step 3: Concat the new rows with those that didn't need any consolidation")
    consolidated_rows_df = pd.concat(consolidated_rows)
    transaction = pd.concat([transaction, consolidated_rows_df])
    print(f"Transaction rows # AFTER consolidating: {transaction.shape[0]}")
    print(f"Rows were concatenated correctly: {after_rows_no + len(consolidated_rows_df) == transaction.shape[0]}\n")

    # Print statistics
    removed_rows_all_process_no = before_rows_no - transaction.shape[0]
    print(f"\nRows that were removed during the whole process: {removed_rows_all_process_no}")
    print(f"% over all initial rows: {removed_rows_all_process_no / before_rows_no * 100}")

# TODO: finire qui (check the equity rows and why the counting function I did for ijg local and such doesn't work)

transaction_after_first_consol = transaction.copy(deep=True)

# the ones that are still left to fix are those that have the same company coming from the same dataset, we will fix these in the next section
[x for x in transaction['investment_id'].unique() if check_if_multiple_rows_for_same_investor_same_investment_id(x) == True]

# this will be consolidated in the next sections
transaction.loc[transaction['investment_id'] == "DEBTID_BUCGEF_108"]

# check
transaction.loc[transaction['investment_id'] == "DEBTID_IJGLOBAL_244"]

transaction.loc[transaction['investment_id'] == "DEBTID_BUCGEF_1"]

transaction.loc[transaction['investment_id'] == "DEBTID_BUCGEF_161"]

transaction.loc[transaction['investment_id'] == "DEBTID_IJGLOBAL_163"]

transaction.loc[transaction['investment_id'] == 'DEBTID_IJGLOBAL_244']

transaction.loc[transaction['investment_id'] == 'EQUITYID_IJGLOBAL_54']

transaction.loc[transaction['investment_id'] == 'EQUITYID_IJGLOBAL_6']

"""Note: the IDs that we still get to fix are those that have the same company in different rows but these rows come from the same datasource....

##### Same investment_id, same company, same source.

We need to merge together the rows that are about the same company in a unique investment_id if these rows come from the same data source. The previous algorithm doesn't deal with them (it makes sure that in each investment_id there is only one source per company, but these could be multiple rows).

To combine the rows together we just sum up the investments.
"""

# safe copy
tr_safe_copy_before_same_comp_source_fix = transaction.copy(deep=True)

# get IDs to fix
ids_to_fix_same_source_same_company = [x for x in transaction['investment_id'].unique() if check_same_company_same_source_same_investment(x)]
ids_to_fix_same_source_same_company

# show example
transaction.loc[transaction['investment_id'] == "DEBTID_BUCGEF_108"]

# show example
transaction.loc[transaction['investment_id'] == "DEBTID_IJGLOBAL_244"]

"""The issue is with the "industrial and commerical bank of china": there are four rows that take track of the same company investing in the same investment opportunity. Thus, we can combine them together."""

# for checking purposes
before_fixing = transaction['investment_id'].nunique()

# since we need to concat the values in r_id we need to prepare the column for it
transaction['r_id'] = transaction['r_id'].fillna("")
transaction['r_id'] = transaction['r_id'].astype("string")

# only run the algorithm if there are IDs to fix
# and only run the algorithm on the IDs that are to fix
if len(ids_to_fix_same_source_same_company) == 0:
    print("No IDs to consolidate")
else:
    # we know that these IDs do have at least one company that has multiple rows from the same data source
    for id in ids_to_fix_same_source_same_company:
        # 0. Let's do a reset index because we will need to drop later
        transaction = my_reset_index(transaction)

        # 1. get the problematic ID's row and its indexes
        to_fix = transaction.loc[transaction['investment_id'] == id]
        to_fix_indexes = to_fix.index.to_list()

        # 2. Fix the issue:
        # combine the rows of the same company in the same investment
        new_rows = to_fix.groupby(by=['investment_id', "investor_name"]).agg(
                                                        {"amount": "sum", # sum up all the different investments of the same company
                                                         "investment_averaged": "max", "investment_weighted": "max", # keep track of the original values: if there is at least a "Y"
                                                                # then this will get preserved (max("Y", "N") = "Y")
                                                         "r_id":'; '.join, # concatenate all the r_ids if they are present
                                                         "original_source": "max"} # if there are multiple rows these are the same source since we checked this ID before
                                                        ).reset_index()

        # 3. Update transaction
        # drop the old rows
        transaction = transaction.drop(to_fix_indexes)
        # add the new rows
        transaction = pd.concat([transaction, new_rows])

# show example
transaction.loc[transaction['investment_id'] == "DEBTID_BUCGEF_108"]

# show example
transaction.loc[transaction['investment_id'] == "DEBTID_IJGLOBAL_244"]

# check that all the fixes have been dealt with
print(f"All fixes have been dealt with: {len([x for x in transaction['investment_id'].unique() if check_same_company_same_source_same_investment(x)]) == 0}")
print(f"All unique_ids have been preserved: {before_fixing == transaction['investment_id'].nunique()}")

"""##### Do merging: same company, same PP_key, multiple rows and one is BUCGP

Steps:
1. get all the PP IDs that needs to be consolidated with a function as done above
1. get the investment_ids of the BUCGP rows that needs to be dropped and drop them in Transaction
1. transfer the information that the BUCGP uniquely has (bu_id_bu_cgp) to the other rows of the same company in Debt or Equity (as of now let's only do Debt because only BUCGP has data about equity) and get the debt_ids of the BUCGP to drop
1. then drop the rows that come out of BUCGP in Debt/Equity

Ideas behind this:
* to go from one dataset to the other (from Transaction to debt_tr to Debt we can use the debt_id and PP_key values)
* there is only one Transaction row per BUCGP's debt_id because we made sure not to match anything, yet, with BUCGP (and in the future since the BUCGP investment has no year then it won't be matched with anything else).
"""

# transaction = transaction_after_first_consol.copy(deep=True)

# load Equity
equity = my_read_excel(output_folder , "Equity.csv")
# join it with transaction
equity_tr = equity.merge(transaction, left_on="equity_id", right_on="investment_id")
count_sources_in_id(equity, "equity_id", "Equity")

# load Debt
debt = my_read_excel(output_folder , "Debt.csv")
# join it with transaction
debt_tr = debt.merge(transaction, left_on="debt_id", right_on="investment_id")
count_sources_in_id(debt, "debt_id", "Debt")

transaction, debt = consolidate_bucgp_rows(transaction, debt, debt_tr, debt_or_equity="Debt")

# check again that there is no need to consolidate anymore
debt_tr_new = debt.merge(transaction, left_on="debt_id", right_on="investment_id")
_, _ = consolidate_bucgp_rows(transaction, debt, debt_tr_new, debt_or_equity="Debt")

transaction, equity = consolidate_bucgp_rows(transaction, equity, equity_tr, debt_or_equity="Equity")

# check again that there is no need to consolidate anymore
equity_tr_new = equity.merge(transaction, left_on="equity_id", right_on="investment_id")
_, _ = consolidate_bucgp_rows(transaction, equity, equity_tr_new, debt_or_equity="Equity")

"""#### 3. Save"""

output_folder

# investor.to_excel(output_folder + investor_name + ".xlsx", index=False)
# transaction.to_excel(output_folder + transaction_name + ".xlsx", index=False)
investor.to_csv(output_folder + investor_name + ".csv", index=False)
transaction.to_csv(output_folder + transaction_name + ".csv", index=False)

# debt.to_excel(output_folder + debt_name + ".xlsx", index=False)
debt.to_csv(output_folder + debt_name + ".csv", index=False)

# equity.to_excel(output_folder + "Equity.xlsx", index=False)
equity.to_csv(output_folder + "Equity.csv", index=False)

