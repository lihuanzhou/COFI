# -*- coding: utf-8 -*-
"""Step CLEANING - 3 - Power Plant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nXPo9WxyWpWnMSaHQaNhR7Yz0hEQbD6x

Thoughts:
* If we group stuff here based on the name (e.g., because the location_id is missing), then we need to make sure to update the bridging table with City accordingly.
"""

import pandas as pd

from functions.cleaning import get_all_files, my_reset_index, my_read_excel

# needed folder paths
input_folder = "Cleaned Data/" # or we can put this info in the "files name" sheet
intermediate_folder = "Intermediate Results/" # to save some intermediate results
# output_folder = "Final tables/" # where we store the final tables (e.g. Power Plant)
output_folder = "Current Final Tables/"

# load the excel containing all the mapping information
map_table_name = "Variables.xlsx"
map_table = pd.read_excel(map_table_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(map_table))


# the sheet where there is the information that the client can change
files_names_sheet = "file names"
groupby_info_sheet = "formatting"
vertical_slicing_info_sheet = "variables to use"
aggregation_info_sheet = "aggregation"
db_structure_sheet = "db structure"
un_country_info_sheet = "UN Country Info"
country_names_dict_sheet = "country names dicionary"
bri_countries_sheet = "BRI countries"

# the name of the Country table
power_plant_name = "Power Plant"

# list of the names of the various Country tables
# TODO: uniform all the names of these files
# power_plant_files_list = ["Power Plant_WEPPGPPD.xlsx", "Power Plant_BUCGP.xlsx", "BU_CGEF_fuzzy_matching - fuzz.partial_ratio/Power Plant_BUCGEF.xlsx", "Power Plant_SAISCLA_IADGEGI.xlsx", "Power Plant_FDI_RMA.xlsx"]
# power_plant_files_list = ["Power Plant_WEPPGPPD.xlsx", "Power Plant_BUCGP.xlsx", "BU_CGEF_fuzzy_matching - fuzz.partial_ratio/Power Plant_BUCGEF.xlsx", "Power Plant_SAISCLA_IADGEGI.xlsx", "Power Plant_FDI_RMA.xlsx", "Power Plant_IJ_Global.xlsx"]

"""List of sources to clean and merge together after processing each data source:
* after WEPP & GPPD: ['Power Plant_WEPPGPPD.xlsx']
* after BU_CGP: ['Power Plant_BUCGP.xlsx', 'Power Plant_WEPPGPPD.xlsx']
* after BU_CGEF: ['Power Plant_BUCGEF.xlsx', 'Power Plant_BUCGP.xlsx', 'Power Plant_WEPPGPPD.xlsx']
* after SAIS_CLA+IAD_GEGI: ['Power Plant_SAISCLA_IADGEGI.xlsx', 'Power Plant_BUCGEF.xlsx', 'Power Plant_BUCGP.xlsx', 'Power Plant_WEPPGPPD.xlsx']
* after IJ_Globlal: ['Power Plant_IJ_Global.xlsx', 'Power Plant_SAISCLA_IADGEGI.xlsx', 'Power Plant_BUCGEF.xlsx', 'Power Plant_BUCGP.xlsx',
 'Power Plant_WEPPGPPD.xlsx']
* after REF_MA: ['Power Plant_REFINITIVMA.xlsx',
 'Power Plant_IJ_Global.xlsx',
 'Power Plant_SAISCLA_IADGEGI.xlsx',
 'Power Plant_BUCGEF.xlsx',
 'Power Plant_BUCGP.xlsx',
 'Power Plant_WEPPGPPD.xlsx']
* after FDI_Markets: ['Power Plant_FDIMARKETS.xlsx',
 'Power Plant_REFINITIVMA.xlsx',
 'Power Plant_IJ_Global.xlsx',
 'Power Plant_SAISCLA_IADGEGI.xlsx',
 'Power Plant_BUCGEF.xlsx',
 'Power Plant_BUCGP.xlsx',
 'Power Plant_WEPPGPPD.xlsx']
* after REF_LOAN: ['Power Plant_REFINITIVLOAN.xlsx',
 'Power Plant_FDIMARKETS.xlsx',
 'Power Plant_REFINITIVMA.xlsx',
 'Power Plant_IJ_Global.xlsx',
 'Power Plant_SAISCLA_IADGEGI.xlsx',
 'Power Plant_BUCGEF.xlsx',
 'Power Plant_BUCGP.xlsx',
 'Power Plant_WEPPGPPD.xlsx']
"""

# get the files
power_plant_files_list = get_all_files(intermediate_folder, "Power Plant")
power_plant_files_list

"""Concatanate the data from the list of files."""

# read all files
datasets = []
for file in power_plant_files_list:
    datasets.append(my_read_excel(intermediate_folder , file))

datasets[0].shape[0]

# concatanate
power_plant = pd.concat(datasets)
if "Unnamed: 0" in power_plant.columns:
    power_plant = power_plant.drop(columns=['Unnamed: 0'])
print("Shape: " + str(power_plant.shape))

# check that there were no problems with concat: all entries have been added
print("Concat went smoothly: " + str(power_plant.shape[0] == sum([x.shape[0] for x in datasets])))

power_plant.head()

datasets[-1].shape[0]

from functions.cleaning import extract_between_outside_underscores

# check that there are no duplicated rows
tmp = power_plant.copy(deep=True) # create a copy so we do not mess up with the original
# check if each row is a copy of another
tmp['duplicated'] = tmp.duplicated(subset=[x for x in tmp.columns if x != "PP_key"])
duplicated_rows = tmp.loc[tmp['duplicated'] == True]
relevant_duplicated_rows = tmp.loc[(tmp['duplicated'] == True) & (~tmp['power_plant_name'].isna()) & (tmp['power_plant_name'] != "")]

if relevant_duplicated_rows.shape[0] == 0 and tmp.loc[(tmp['duplicated'] == False) | (~tmp['power_plant_name'].isna()) | (tmp['power_plant_name'] != "")].shape[0] == tmp.shape[0]:
    print(f"There are no duplicated rows!")
else:
    print(f"There are {relevant_duplicated_rows.shape[0]} duplicated rows....")
    print(f"The duplicated rows come from: {set([extract_between_outside_underscores(x) for x in relevant_duplicated_rows['PP_key']])}")
relevant_duplicated_rows.sort_values(by='power_plant_name')

# "mali gouina hydropower project": the duplicated row is actually not a duplicated row because they have then different countries
tmp['power_plant_name'] = tmp['power_plant_name'].fillna("")
tmp.loc[tmp['power_plant_name'].str.contains("mali go")]

# counter = 0
# for i, row in relevant_duplicated_rows.iterrows():
#     if "IJGLOBAL_DT" in row['PP_key']:
#         other_row = power_plant.loc[(power_plant['power_plant_name'] == row['power_plant_name']) & (power_plant['PP_key'].str.contains("IJGLOBAL_EQ"))]
#         if other_row.shape[0] == 0:
#             print("Weird")
#         else:
#             counter+=1

power_plant['commissioning_year'].isna().sum()

# check: if this works then there are no commissioning years with placeholders!
power_plant['commissioning_year'] = power_plant['commissioning_year'].astype("float64")

"""#### 1. Save"""

output_folder

# power_plant.to_excel(output_folder + power_plant_name + ".xlsx", index=False)
power_plant.to_csv(output_folder + power_plant_name + ".csv", index=False)

