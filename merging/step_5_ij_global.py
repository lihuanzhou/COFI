# -*- coding: utf-8 -*-
"""Step 5 - IJ Global.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Re8k7wwoV2MiSFqlyhR0r25jxNyglg0O

Things to remember:
* the equity_investor_name is also the debt_investor_name
* we can't use the exact join on the investment type as the type here is no greenfield, m&a, fdi, etc. as in the previous ones
"""

import pandas as pd
import numpy as np

# import custom-made functions
from functions.cleaning import get_variables, create_placeholder, create_unique_id, get_variables_advanced, preprocess_text
from functions.cleaning import  check_if_new_country_names, update_country_names
from functions.joining import join_pp_until_country, join_equity_transaction, join_debt_transaction
from functions.final_tables_creation import get_tables_to_create_empty

# other useful libraries
import itertools
from statistics import mean, stdev
import statistics

# for the matching of power plant names
from thefuzz import process
from thefuzz import fuzz

from functions.joining import join_pp_full_debt_full_equity_full
from functions.cleaning import my_read_excel, my_reset_index
from functions.cleaning import get_thresholds_methods_dicts
from functions.matching import find_matches_equity_and_debt_complete
from functions.matching import find_matches_power_plant_or_fill_PP_key
from functions.cleaning import extract_between_outside_underscores
from functions.cleaning import print_final_statistics, fix_investor_names, clean_primary_fuel_column
from functions.cleaning import count_sources_in_id
from functions.matching import prepare_pp_full_for_matching_commissioning_year
from functions.cleaning import fix_duplicated_power_plants_keys

# needed folder paths
input_folder = "Cleaned Data/" # or we can put this info in the "files name" sheet
intermediate_folder = "Intermediate Results/" # to save some intermediate results
current_final_tables_folder = "Current Final Tables/"

# load the excel containing all the mapping information
map_table_name = "Variables.xlsx"
map_table = pd.read_excel(map_table_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(map_table))

# load the excel containing all the mapping information
dictionary_fuel_name = "Dictionaries.xlsx"
dictionary_fuel = pd.read_excel(dictionary_fuel_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(dictionary_fuel))

# the sheet where there is the information that the client can change
files_names_sheet = "file names"
groupby_info_sheet = "formatting"
vertical_slicing_info_sheet = "variables to use"
aggregation_info_sheet = "aggregation"
db_structure_sheet = "db structure"
un_country_info_sheet = "UN Country Info"
country_names_dict_sheet = "country names dicionary"
bri_countries_sheet = "BRI countries"
filtering_sheet = "filtering"
merging_sheet = "merging"
matching_threshold_sheet = "matching thersholds"
matching_method_sheet = "matching methods"
matching_paremeter_sheet = "matching parameters"
# we will need this later
thresholds_df = map_table[matching_threshold_sheet]
methods_df = map_table[matching_method_sheet]
parameters_df = map_table[matching_paremeter_sheet]

# the tables we are creating here
tables_to_create = ["Power Plant", "City", "Country", "Debt", "Equity", "Transaction", "Investor"]
investments_tables = ["Equity", "Transaction", "Investor"]

# already exising final data<<
# debt_file = "Debt.xlsx"
# power_plant_file = "Power Plant.xlsx"
# city_file = "City.xlsx"
# country_file = "Country.xlsx"
# bridge_file = "CITYKEY_BRIDGE_PP_C.xlsx"
# equity_file = "Equity.xlsx"
# transaction_file = "Transaction.xlsx"
debt_file = "Debt.csv"
power_plant_file = "Power Plant.csv"
city_file = "City.csv"
country_file = "Country.csv"
bridge_file = "CITYKEY_BRIDGE_PP_C.csv"
equity_file = "Equity.csv"
transaction_file = "Transaction.csv"

# names of the databases used here
ijg_name = "IJ_Global"

# names of the final tables to be used here
equity_name = "Equity"
debt_name = "Debt"

# saving information
saving_suffix = "_IJ_Global.csv"

"""## 1. Read data

#### 1.1 Load data
"""

# this table contains the name of the data sources
file_names = map_table[files_names_sheet]
file_names

ijg = my_read_excel(input_folder , file_names.loc[file_names['Database'] == ijg_name]["File name"].values[0])

ijg_full = ijg.copy(deep=True)

ijg.head()

ijg.shape

"""#### 1.2 Load data for exising Power Plants and Debt data for matching purposes"""

columns_to_keep_pp = ["PP_key", "power_plant_name", "primary_fuel", "province", "country"]
columns_to_keep_equity = ["equity_id", "PP_key", "equity_investment_type",  "equity_investment_year", "investor_name", "amount"]
columns_to_keep_debt = ["debt_id", "PP_key", "debt_investment_year", "investor_name", "amount"]

pp_full, db_pp,  eq_pp = join_pp_full_debt_full_equity_full(columns_to_keep_pp, columns_to_keep_debt, columns_to_keep_equity, "Current Final Tables/", get_commissioning_year=True)
# TODO: check this function

pp_full.head()

db_pp.head()

eq_pp.head()

count_sources_in_id(pp_full, "PP_key", "PP full")

count_sources_in_id(db_pp, "debt_id")

count_sources_in_id(eq_pp, "equity_id", )

"""#### 1.3 Load Debt and Equity table"""

equity = my_read_excel(current_final_tables_folder, equity_file)
equity.head()

debt = my_read_excel(current_final_tables_folder, debt_file)
debt.head()

"""## 2. Clean the data

#### 2.1 Vertical slicing
"""

vertical_slicing = map_table[vertical_slicing_info_sheet]
vertical_slicing.head()

ijg_variables = get_variables(vertical_slicing, ijg_name, tables_to_create)
ijg_variables

"""Keep also some extra rows:
* "debt_investor_name" is a row that was created when fixing IJ Global
* "Tranche Instrument Primary Type" is needed to divide the data in equity and debt.
"""

# keep also the extra colum
ijg = ijg[ijg_variables + ["Tranche Instrument Primary Type"]]
ijg.head(2)

"""#### 2.2 Clean if needed"""

for col in ["power_plant_name", "primary_fuel", "province", "country", "equity_investment_year", "debt_investment_year", "equity_investor_name", "debt_investor_name"]:
    print(f"{col} NaNs: {ijg[col].isna().sum()}")

"""So we may be dealing with some missing data, this is relevant when doing the matching.

We can already do some converting.

##### Country
"""

ijg.loc[ijg['country'].isna()]

# TODO: is this okay to drop them? it is kinda obvious that they are from australia
ijg = ijg.dropna(subset=["country"])

# needed to run the functions to do the converting
country_names_dict_df = map_table[country_names_dict_sheet]
un_data = map_table[un_country_info_sheet]

ijg['country'] = ijg['country'].apply(lambda x: x.lower())

# check if there are new names that are needs to be added to the conversion list
# TODO: in the final product this can also be a feature when they load the db
missing = check_if_new_country_names(list(ijg['country'].unique()), un_data, country_names_dict_df)
missing

# TODO: we should have aone country per entry! For now i will drop but this needs to be addressed
to_drop = list(ijg.loc[ijg['country'].isin(missing)].index)
len_before = ijg.shape[0]
print(f"percentage dropping: {len(to_drop) / ijg.shape[0] * 100}")
ijg = ijg.drop(to_drop)
print(f"check: {len_before - len(to_drop) == ijg.shape[0]}")

# convert the names
ijg = update_country_names(country_names_dict_df, ijg)

"""##### Primary fuel

If the primary_fuel is nan or "other" then we keep/put it to nan because, respectively, it is not known and it may still match something in WEPP that we don't know.
* so when we try to find matches we do an equal join based on the primary_fuel. If the primary_fuel is nan, then we consider all the potential plants (in the same country etc.)
"""

ijg['primary_fuel'].unique()

ijg = clean_primary_fuel_column(ijg)

ijg['primary_fuel'].unique()

"""##### investment_type

So, the investment type is also for the debt rows (as there are no missing values for the investment type) but only in the Equity table we are interested in the investment type. So here we can already rename the column.

Moreover, the values of the "investment_type" can't already be matched with the usual "m&a", "greenfield", "fdi", so we can't use this column for the exact matching when doing the investment matching (so set the "use_equity_investment_type" parameter to False in the matching function).
"""

# let's already change the name as we need
ijg = ijg.rename(columns={"investment_type": "equity_investment_type"})

ijg['equity_investment_type'].value_counts()

"""##### Rows that don't have equity nor debt data

In the original IJ Global dataset there were some rows that didn't specify whether they were about debt or equity. So, now they don't have either information. So here we drop them.
"""

indexes_to_drop = list(ijg.loc[ijg['Tranche Instrument Primary Type'].isna()].index)
print(f"Rows without any investment info: {len(indexes_to_drop)}")

# drop them
# TODO: is it fine to drop them?
print(f"Rows before dropping: {ijg.shape[0]}")
ijg = ijg.drop(indexes_to_drop)
print(f"Rows after dropping: {ijg.shape[0]}")

ijg['Tranche Instrument Primary Type'].value_counts()

# TODO: deal with this problem when fixing IJ global (in notebook "Step 6.0 - Fix IJ Global")

"""##### investor_name"""

ijg['debt_investor_name'] = ijg['debt_investor_name'].fillna("")

ijg = fix_investor_names(ijg, "debt_investor_name", False, None, drop_duplicates=False)

ijg['equity_investor_name'] = ijg['equity_investor_name'].fillna("")

ijg = fix_investor_names(ijg, "equity_investor_name", False, None, drop_duplicates=False)

"""#### 2.3 Separte data in equity and debt data"""

ijg.columns

debt_columns = ['power_plant_name', 'primary_fuel', 'province', 'country',
                  'debt_investment_year', 'j_id' ,
                  'debt_investor_name', 'debt_investment_amount',
                 'parent_company_of_investor']

equity_columns = ['power_plant_name', 'primary_fuel', 'province', 'country',
                  'equity_investment_type', 'equity_investment_year', 'j_id' ,
                  'equity_investor_name', 'equity_investor_amount',
                 'parent_company_of_investor']

eq_ijg = ijg.loc[ijg['Tranche Instrument Primary Type'] == "Equity"][equity_columns].copy(deep=True)
print(f"Equity rows: {eq_ijg.shape[0]}")
print(f"Missing equity investors: {eq_ijg['equity_investor_name'].isna().sum()}")

db_ijg = ijg.loc[ijg['Tranche Instrument Primary Type'] == "Debt"][debt_columns].copy(deep=True)
print(f"Debt rows: {db_ijg.shape[0]}")
print(f"Missing debt investors: {db_ijg['debt_investor_name'].isna().sum()}")

print(f"All rows are in a dataset: {eq_ijg.shape[0] + db_ijg.shape[0] == ijg.shape[0]}")

"""#### 2.4 Pre-process the IJ_Global datasets and existing Power Plant, Debt, and Equity data for matching"""

pp_full.columns

# check
for col in ['province']:
    print(f"Rows with unnamed {col}: {pp_full.loc[pp_full[col].str.contains('unnamed')].shape[0]}")

for col in pp_full.columns:
    if col in ["power_plant_name"]:
        pp_full[col] = pp_full[col].fillna("")
        pp_full[col] = pp_full[col].apply(preprocess_text)
        pp_full[col] = pp_full[col].astype("string")
    elif col in ["country", "primary_fuel"]:
        pp_full[col] = pp_full[col].fillna("")
        pp_full[col] = pp_full[col].astype("string")
    elif col in ['installed_capacity']:
        pp_full[col] = pp_full[col].astype("float64")
    elif col in ["city", "province"]:
        pp_full[col] = pp_full[col].fillna("")
        pp_full[col] = pp_full[col].apply(preprocess_text)
        pp_full[col] = pp_full[col].apply(lambda x: "" if "unnamed" in x else x)
        pp_full[col] = pp_full[col].astype("string")
    elif col ==  "commissioning_year":
        # pp_full[col] = pp_full[col].apply(lambda x: np.nan if (isinstance(x, str)) else x)
        pp_full[col] = pp_full[col].astype("float64")

# check
for col in ['province']:
    print(f"Rows with unnamed {col}: {pp_full.loc[pp_full[col].str.contains('unnamed')].shape[0]}")

eq_pp.columns

# check
for col in ["province"]:
    print(f"Rows with unnamed {col}: {eq_pp.loc[eq_pp[col].str.contains('unnamed')].shape[0]}")

for col in eq_pp.columns:
    if col in ["investor_name", "power_plant_name"]:
        eq_pp[col] = eq_pp[col].fillna("")
        eq_pp[col] = eq_pp[col].apply(lambda x: preprocess_text(x, True))
        eq_pp[col] = eq_pp[col].astype("string")
    elif col in ["country", "primary_fuel"]:
        eq_pp[col] = eq_pp[col].fillna("")
        eq_pp[col] = eq_pp[col].astype("string")
    elif (col == 'equity_investment_year') or (col == "amount"):
        eq_pp[col] = eq_pp[col].astype("float64")
    elif col in ["city", "province"]:
        eq_pp[col] = eq_pp[col].fillna("")
        eq_pp[col] = eq_pp[col].apply(lambda x: preprocess_text(x, True))
        eq_pp[col] = eq_pp[col].apply(lambda x: "" if "unnamed" in x else x)
        eq_pp[col] = eq_pp[col].astype("string")

# check
for col in ["province"]:
    print(f"Rows with unnamed {col}: {eq_pp.loc[eq_pp[col].str.contains('unnamed')].shape[0]}")

db_pp.columns

# check
for col in ["province"]:
    print(f"Rows with unnamed {col}: {db_pp.loc[db_pp[col].str.contains('unnamed')].shape[0]}")

for col in db_pp.columns:
    if col in ["investor_name", "power_plant_name"]:
        db_pp[col] = db_pp[col].fillna("")
        db_pp[col] = db_pp[col].apply(lambda x: preprocess_text(x, True))
        db_pp[col] = db_pp[col].astype("string")
    elif col in ["country", "primary_fuel"]:
        db_pp[col] = db_pp[col].fillna("")
        db_pp[col] = db_pp[col].astype("string")
    elif (col == 'debt_investment_year') or (col == "amount"):
        db_pp[col] = db_pp[col].astype("float64")
    elif col in ["city", "province"]:
        db_pp[col] = db_pp[col].fillna("")
        db_pp[col] = db_pp[col].apply(lambda x: preprocess_text(x, True))
        db_pp[col] = db_pp[col].apply(lambda x: "" if "unnamed" in x else x)
        db_pp[col] = db_pp[col].astype("string")

# check
for col in ["province"]:
    print(f"Rows with unnamed {col}: {db_pp.loc[db_pp[col].str.contains('unnamed')].shape[0]}")

eq_ijg.columns

for col in eq_ijg.columns:
    if col in ["equity_investor_name", "power_plant_name", "province", "parent_company_of_investor"]:
        eq_ijg[col] = eq_ijg[col].fillna("")
        eq_ijg[col] = eq_ijg[col].apply(lambda x: preprocess_text(x, True))
        eq_ijg[col] = eq_ijg[col].astype("string")
    elif col in ["country", "primary_fuel"]:
        eq_ijg[col] = eq_ijg[col].fillna("")
        eq_ijg[col] = eq_ijg[col].astype("string")
    elif (col == 'equity_investment_year') or (col == "equity_investor_amount"):
        eq_ijg[col] = eq_ijg[col].astype("float64")

db_ijg.columns

for col in db_ijg.columns:
    if col in ["debt_investor_name", "power_plant_name", "province", "parent_company_of_investor"]:
        db_ijg[col] = db_ijg[col].fillna("")
        db_ijg[col] = db_ijg[col].apply(lambda x: preprocess_text(x, True))
        db_ijg[col] = db_ijg[col].astype("string")
    elif col in ["country", "primary_fuel"]:
        db_ijg[col] = db_ijg[col].fillna("")
        db_ijg[col] = db_ijg[col].astype("string")
    elif (col == 'debt_investment_year') or (col == "debt_investment_amount"):
        db_ijg[col] = db_ijg[col].astype("float64")

"""## 3. Match with existing data

We see if the eq_ijg and db_ijg contain info that is already in Equity and Debt tables, respectively.

#### 3.1 Find matches for EQUITY
"""

# TODO: for equity in the end we won't do the matching SINCE in the final order of execution we will this before REF_MA and FDI_markets so in Equity there will only be BU_CGP
# which has no year so then no match can happen (indeed, in the matching that we see only matches to REF_MA entries -> see below)

print(f"All entries in current Equity do NOT have a valid equity_investment_year: {eq_pp['equity_investment_year'].isna().sum() == eq_pp.shape[0]}")

"""##### Load thresholds and methods to use"""

thrs_dict_eq, mtds_dict_eq, prmts_dict_eq = get_thresholds_methods_dicts(thresholds_df, methods_df, parameters_df, ijg_name, "Equity")

thrs_dict_eq

mtds_dict_eq

prmts_dict_eq

"""##### Find the matches with the info in equity"""

eq_ijg = my_reset_index(eq_ijg)

# we can't use the equity_investment type because the types here are different to what's in eq_pp
eq_ijg['equity_investment_type'].unique()

eq_pp['equity_investment_type'].unique()

matches_equity = find_matches_equity_and_debt_complete(eq_ijg, eq_pp, equity_or_debt="Equity",
                                                        thresholds_dict=thrs_dict_eq,
                                                        methods_dict=mtds_dict_eq, parameters_dict=prmts_dict_eq)
matches_equity

"""##### Merge what needs to be merged"""

# TODO: in the end there won't be anything to merge because there will be no matches for sure since we would only be matching with BUCGP....
# BUt for the final product it would be better we have a function to do the merging

# TODO: see the merging in debt, the code there can also be re-used here with a function

"""#### 3.2 Find the matches for debt

##### Load the details
"""

thrs_dict_dt, mtds_dict_dt , prmts_dict_dt = get_thresholds_methods_dicts(thresholds_df, methods_df,parameters_df,ijg_name, "Debt")

thrs_dict_dt

mtds_dict_dt

prmts_dict_dt

"""##### Find matches for debt"""

db_ijg = my_reset_index(db_ijg)

# no. of matches in default settings:  36
matches_debt = find_matches_equity_and_debt_complete(db_ijg, db_pp, equity_or_debt="Debt",
                                                    thresholds_dict=thrs_dict_dt,
                                                    methods_dict=mtds_dict_dt)
matches_debt

# show example
index = 446
db_ijg.head(index + 1).tail(1)

db_pp.loc[db_pp['debt_id'] == matches_debt[index]]

"""##### Merge what needs merging

So, since IJ_Global is ranked lower than the sources already in Debt:
* even if there is mismatch in the information (e.g., the year), then the information already in Debt is what needs to be there since it is higher ranked.
* then, we only need to add to matched rows in Equity the j_id, create the transaction and investor rows, and remove the matched IJ_Global rows from db_ij.
"""

# UPDATE DEBT: put the j_id there
# TODO: this can be a function since the mechanism used is the same when updating a new dataset
# e.g. we use the same logic (and updated code) in Step 6 with REF_MA
debt['j_id'] = debt['j_id'].astype("string") # since there could be multiple j_ids then we want to keep them all
debt['j_id'] = debt['j_id'].fillna("")
for index_ijg in matches_debt:
    # get the j_id of the db_ij's row
    j_id = db_ijg.iloc[index_ijg]['j_id']
    # get the index of the matched debt row (we use the index to then assign to the row the j_id)
    index_debt = debt.loc[debt['debt_id'] == matches_debt[index_ijg]].index.to_list()[0]
    # get the debt row just for convinience
    debt_row = debt.iloc[index_debt]
    # assign the j_id to the debt row
    # note: multiple IJ_G rows could match the same debt row so we need to take track of all their j_id
    # but different IJ_G could have the same j_id so we also don't want to repeat
    if debt_row['j_id'] == "": # add the first one
        debt.at[index_debt, "j_id"] = str(j_id)
    elif not str(j_id) in debt_row['j_id']: # if the j_id is not already there then add this id
        debt.at[index_debt, "j_id"] = str(j_id) + "; " + debt_row['j_id']

# CREATE THE TRANSACTION AND INVESTOR ROWS: from the db_ij rows
# TODO: this can be a function since the mechanism used is the same when updating a new dataset
# e.g. we use the same logic (and updated code) in Step 6 with REF_MA

transaction_rows = []
investor_rows = []
for index_ijg in matches_debt:
    ijg_row = db_ijg.iloc[index_ijg]
    tr_row = [matches_debt[index_ijg], ijg_row['debt_investor_name'], ijg_row['debt_investment_amount'], "N", "N", np.nan]
    in_row = [ijg_row['debt_investor_name'], ijg_row['parent_company_of_investor']]
    transaction_rows.append(tr_row)
    investor_rows.append(in_row)
print(f"There are as many new transaction rows as matches: {len(transaction_rows) == len(matches_debt)}")
print(f"There are as many new investor rows as matches: {len(investor_rows) == len(matches_debt)}")

# REMOVE FROM DB_IJ: the rows that were matched
# TODO: this can be a function since the mechanism used is the same when updating a new dataset
# e.g. we use the same logic (and updated code) in Step 6 with REF_MA
new_debt = db_ijg.drop(list(matches_debt.keys()))
print(f"Dropping went fine: {new_debt.shape[0] == db_ijg.shape[0] - len(matches_debt)}")

"""## 4. Find matches with Power Plant"""

thrs_dict_pp, mtds_dict_pp, prmts_dict_pp = get_thresholds_methods_dicts(thresholds_df, methods_df,parameters_df, ijg_name, "Power Plant")

thrs_dict_pp

mtds_dict_pp

prmts_dict_pp

# prepare the pp_full dataset for the matching (it fixes up the year_range and then creates the upper and lower limit columns which are needed)
pp_full = prepare_pp_full_for_matching_commissioning_year(pp_full)

# new_equity = my_reset_index(new_equity)
# TODO:: use the eq_ij that do not have the rows already matched equity (that is new_equity and likely new_debt!) if we find matches in equity (see what we do for debt)
# no. of matches in default settings: 91
matched_db_equity = find_matches_power_plant_or_fill_PP_key(eq_ijg, pp_full, "PLANTKEY_IJGLOBAL_EQ_",
                                                            thresholds_dict=thrs_dict_pp,
                                                    methods_dict=mtds_dict_pp, equity_or_debt="Equity", parameters_dict=prmts_dict_pp)

matched_db_equity.loc[~matched_db_equity['PP_key'].str.contains("IJG")].sort_values(by="PP_key").head()

pp_full.loc[pp_full['PP_key'].isin(matched_db_equity.loc[~matched_db_equity['PP_key'].str.contains("IJG")]['PP_key'].values)].sort_values(by="PP_key").head()

# show example of the commissioning_year use
example = "PLANTKEY_WEPPGPPD_8544"
matched_db_equity.loc[matched_db_equity['PP_key'] == example]

pp_full.loc[pp_full['PP_key'] == example]

new_debt = my_reset_index(new_debt)
# no. of matches in default settings: 257
matched_db_debt = find_matches_power_plant_or_fill_PP_key(new_debt, pp_full, "PLANTKEY_IJGLOBAL_DT_",
                                                          thresholds_dict=thrs_dict_pp,
                                                    methods_dict=mtds_dict_pp, equity_or_debt="Debt", parameters_dict=prmts_dict_pp)

matched_db_debt.loc[~matched_db_debt['PP_key'].str.contains("IJG")].sort_values(by="PP_key").head()

pp_full.loc[pp_full['PP_key'].isin(matched_db_debt.loc[~matched_db_debt['PP_key'].str.contains("IJG")]['PP_key'].values)].sort_values(by="PP_key").head()

# show example of the commissioning_year use
example = "PLANTKEY_WEPPGPPD_172928"
matched_db_debt.loc[matched_db_debt['PP_key'] == example]

pp_full.loc[pp_full['PP_key'] == example]

# show example of the year_range use (rows 393 and 394 with power_plant_name = "efeler geothermal plant expansion" and j_id = 47419)
example = "PLANTKEY_WEPPGPPD_180823"
matched_db_debt.loc[matched_db_debt['PP_key'] == example]

# we make the match because 2019 is in the range: if we had only used the commissioning_year then we would have not made the match
pp_full.loc[pp_full['PP_key'] == example]

for name, db in [["Equity", matched_db_equity] , ["Debt", matched_db_debt]]:
    print(name)
    print(f"Matches #: {db.loc[~(db['PP_key'].str.contains('IJG'))].shape[0]}")
    print(f"Matches %: {db.loc[~(db['PP_key'].str.contains('IJG'))].shape[0] / db.shape[0] * 100}\n")

# do some checks
for matched in [matched_db_equity, matched_db_debt]:
    # check that the matching went fine
    # do some checks
    newly_created_name = "IJGLOBAL"

    # check that all the entries now have a proper PP_key

    # no. of matched power_plants
    key_wepp = matched.loc[matched['PP_key'].str.contains("WEPPGPPD")].shape[0]
    key_bucgp = matched.loc[matched['PP_key'].str.contains("BUCGP")].shape[0]
    key_bucgef = matched.loc[matched['PP_key'].str.contains("BUCGEF")].shape[0]
    key_sais = matched.loc[matched['PP_key'].str.contains("SAIS")].shape[0]
    key_rma = matched.loc[matched['PP_key'].str.contains("REFINITIVMA")].shape[0]
    key_fdi = matched.loc[matched['PP_key'].str.contains("FDIMARKETS")].shape[0]
    # key_ijg = matched.loc[matched['PP_key'].str.contains("IJG")].shape[0]

    # no. of newly created keys, these are new entries
    key_new = matched.loc[matched['PP_key'].str.contains(newly_created_name)].shape[0]

    # check that matched keys got the PP_key from Power Plant
    print(f"All matched keys got the PP_key from Power Plant: {key_wepp + key_bucgp + key_bucgef + key_sais + key_rma + key_fdi == matched.loc[~(matched['PP_key'].str.contains(newly_created_name))].shape[0]}")

    # check that each row has a PP_key
    print(f"Each row has a PP_key: {key_wepp + key_bucgp + key_bucgef + key_sais + key_rma + key_new + key_fdi== matched.shape[0]}")

    print(pd.DataFrame(data=[["WEPP + GPPD", key_wepp], ["BUCGP", key_bucgp], ["BUCGEF", key_bucgef], ["SAIS_CLA + IAD_GEGI", key_sais], ["REFINITIVMA", key_rma], ["FDIMARKETS", key_fdi], ["IJGLOBAL", key_new]], columns=['Data source', "Count"]))
    print()

"""## 5. Create tables

#### 5.0 Create the empty tables
"""

db_structure = map_table[db_structure_sheet]
tables_df = get_tables_to_create_empty(db_structure)
# used as support for the creation
tables_df_tmp = get_tables_to_create_empty(db_structure)

"""#### 5.0 Deal with duplicates Power Plant and City info"""

match_eq_copy = matched_db_equity.copy(deep=True)

match_dt_copy = matched_db_debt.copy(deep=True)

# examples_duplicates_debt = ['kirchner 1140mw and cepernic', 'chia hui gas fired power plant', 'martel ii wind farm']
# matched_db_debt.loc[matched_db_debt['power_plant_name'].isin(examples_duplicates_debt)].sort_values(by="power_plant_name")
# matched_db_debt, diff_debt = fix_duplicated_power_plants_keys(matched_db_debt,list(tables_df['Power Plant'].columns) + list(tables_df['City'].columns), "PLANTKEY_IJGLOBAL_DT_"
# matched_db_debt.loc[matched_db_debt['power_plant_name'].isin(examples_duplicates_debt)].sort_values(by="power_plant_name")

# examples_duplicates_equity = ['plp biethanol plant', 'gullen pv solar plant', 'xiamen xiangyu nickel mine steel smelting plant thermal power plant']
# matched_db_equity.loc[matched_db_equity['power_plant_name'].isin(examples_duplicates_equity)].sort_values(by="power_plant_name")[['PP_key'] + list(matched_db_equity.columns)]
# matched_db_equity, diff_equity = fix_duplicated_power_plants_keys(matched_db_equity,list(tables_df['Power Plant'].columns) + list(tables_df['City'].columns), "PLANTKEY_IJGLOBAL_EQ")
# matched_db_equity.loc[matched_db_equity['power_plant_name'].isin(examples_duplicates_equity)].sort_values(by="power_plant_name")[['PP_key'] + list(matched_db_equity.columns)]

matched_db_debt['source_tmp'] = "Debt"
matched_db_equity['source_tmp'] = "Equity"

origina_debt_columns = matched_db_debt.columns.to_list()
origina_equity_columns = matched_db_equity.columns.to_list()

tmp = pd.concat([matched_db_debt, matched_db_equity])
tmp = my_reset_index(tmp)

from functions.cleaning import print_duplicated_power_plants_keys_info

examples = print_duplicated_power_plants_keys_info(tmp,list(tables_df['Power Plant'].columns) + list(tables_df['City'].columns), "PLANTKEY_IJGLOBAL", return_examples=True)

tmp['source_tmp'].value_counts()['Equity'] == matched_db_equity.shape[0]

tmp['source_tmp'].value_counts()['Debt'] == matched_db_debt.shape[0]

shared_duplicated_examples = ["acquisition of enersis wind portfolio", "acquisition of sao simao hydro power plant", "xiamen xiangyu nickel mine steel smelting plant thermal power plant"]
tmp.loc[tmp['power_plant_name'].isin(shared_duplicated_examples)].sort_values(by="power_plant_name")

other = ['kirchner 1140mw and cepernic', 'chia hui gas fired power plant', 'martel ii wind farm', 'plp biethanol plant']
tmp.loc[tmp['power_plant_name'].isin(other)].sort_values(by="power_plant_name")

tmp, diff_both = fix_duplicated_power_plants_keys(tmp,list(tables_df['Power Plant'].columns) + list(tables_df['City'].columns), "PLANTKEY_IJGLOBAL")

tmp.loc[tmp['power_plant_name'].isin(shared_duplicated_examples)].sort_values(by="power_plant_name")

tmp.loc[tmp['power_plant_name'].isin(other)].sort_values(by="power_plant_name")

matched_db_debt = tmp.loc[tmp['source_tmp'] == "Debt"][origina_debt_columns]
matched_db_equity = tmp.loc[tmp['source_tmp'] == "Equity"][origina_equity_columns]

matched_db_debt = my_reset_index(matched_db_debt)
matched_db_equity = my_reset_index(matched_db_equity)

"""#### 5.1 Add debt_id and equity_id"""

# create unique ids with custom function
matched_db_equity = create_unique_id(matched_db_equity, "equity_id", "EQUITYID_IJGLOBAL_")
matched_db_equity.head()

# create unique ids with custom function
matched_db_debt = create_unique_id(matched_db_debt, "debt_id", "DEBTID_IJGLOBAL_")
matched_db_debt.head()

"""#### 5.2 Adding to the Equity and Debt table"""

# get the new entries
entity = equity_name
for column in tables_df_tmp[entity].columns:
    if column in matched_db_equity.columns:
        tables_df_tmp[entity][column] = matched_db_equity[column]

# concat
tables_df['Equity'] = pd.concat([equity, tables_df_tmp['Equity']])
# check
tables_df['Equity'].shape[0]  == equity.shape[0]  +  tables_df_tmp['Equity'].shape[0]

# get the new entries
entity = debt_name
for column in tables_df_tmp[entity].columns:
    if column in matched_db_debt.columns:
        tables_df_tmp[entity][column] = matched_db_debt[column]

# concat
tables_df['Debt'] = pd.concat([debt, tables_df_tmp['Debt']])
# check
tables_df['Debt'].shape[0]  == debt.shape[0]  +  tables_df_tmp['Debt'].shape[0]

"""#### 5.3 Creating Transaction and Investor considering the fact there are already entries for these (stemming from when merging)"""

# TODO: in case I also merged the equity entries, then I would have had two sets of transaction and investor rows

# we need to rename some columns to match the columns in Investor and Transactio
renamings = get_variables_advanced(vertical_slicing, ijg_name, ['Investor', "Transaction"])
# the following two are not in the vertical_slicing (which comes from Variables.xlsx) because we created these two columns not from data coming from the sources (they are based
# on the index of the dataframes....)
renamings["equity_id"] = "investment_id"
renamings["debt_id"] = "investment_id"
renamings['debt_investor_name'] =  'investor_name' # this is added because this row I manually created when fixing IJ Global
renamings['equity_investment_amount'] =  "amount" # TODO: this should be already in the Variables.xlsx file
# because there we should have "equity_investment_amount" instead of "equity_investor_amount"
renamings

# do renaming
# Note: we need to filter because some renaimings rules only apply to eq_agg_full and some only to dt_agg_full
matched_db_equity_v2 = matched_db_equity.rename(columns=dict(filter(lambda x: x[0] in matched_db_equity.columns, renamings.items())))
matched_db_equity_v2.head(2)

# do renaming
# Note: we need to filter because some renaimings rules only apply to eq_agg_full and some only to dt_agg_full
matched_db_debt_v2= matched_db_debt.rename(columns=dict(filter(lambda x: x[0] in matched_db_debt.columns, renamings.items())))
matched_db_debt_v2.head(2)

# get the new entries, use two supporting variables to fill them
transaction_new_equity = pd.DataFrame(columns=tables_df['Transaction'].columns)
transaction_new_debt = pd.DataFrame(columns=tables_df['Transaction'].columns)

entity = "Transaction"
for column in transaction_new_debt.columns:
    if column in matched_db_debt_v2.columns:
        transaction_new_debt[column] = matched_db_debt_v2[column]
transaction_new_debt["investment_averaged"] = "N"
transaction_new_debt["investment_weighted"] = "N"

# get the new entries
entity = "Transaction"
for column in transaction_new_equity.columns:
    if column in matched_db_equity_v2.columns:
        transaction_new_equity[column] = matched_db_equity_v2[column]
transaction_new_equity["investment_averaged"] = "N"
transaction_new_equity["investment_weighted"] = "N"

# concat
transaction_dataframes = [pd.DataFrame(transaction_rows, columns=tables_df['Transaction'].columns), transaction_new_equity, transaction_new_debt]
tables_df['Transaction'] = pd.concat(transaction_dataframes)

# get the new entries, use two supporting variables to fill them
entity = "Investor"
investor_new_equity = pd.DataFrame(columns=tables_df[entity].columns)
investor_new_debt = pd.DataFrame(columns=tables_df[entity].columns)

for column in investor_new_debt.columns:
    if column in matched_db_debt_v2.columns:
        investor_new_debt[column] = matched_db_debt_v2[column]


for column in investor_new_equity.columns:
    if column in matched_db_equity_v2.columns:
        investor_new_equity[column] = matched_db_equity_v2[column]

# concat
investor_dataframes = [pd.DataFrame(investor_rows, columns=tables_df['Investor'].columns), investor_new_equity, investor_new_debt]
tables_df['Investor'] = pd.concat(investor_dataframes)

"""#### 5.4 Make Power Plant and City and Country for new Power Plants found here!"""

matched_db = pd.concat([matched_db_equity_v2, matched_db_debt_v2])
matched_db = my_reset_index(matched_db)

matched_db['PP_key']

missing_power_plants = matched_db.loc[matched_db['PP_key'].str.contains("PLANTKEY_IJGLOBAL")]
print("Missing power plant: " + str(missing_power_plants.shape[0]))
print(f"check: {matched_db.shape[0] - matched_db.loc[~(matched_db['PP_key'].str.contains('PLANTKEY_IJGLOBAL'))].shape[0] == missing_power_plants.shape[0]}")

# tables_df["City"] = tables_df["City"].head(0)
# tables_df["Power Plant"] = tables_df["Power Plant"].head(0)

# get the data from equity
for entity in ['Power Plant', "Country"]:
    for column in tables_df[entity].columns:
        if column in missing_power_plants.columns:
            tables_df[entity][column] = missing_power_plants[column]

entity = "City"
for column in tables_df[entity].columns:
    if column in missing_power_plants.columns:
        tables_df[entity][column] = missing_power_plants[column]
    elif column == "city_key":
        tables_df[entity][column] = missing_power_plants["PP_key"]

# I don't know why we are dropping 60 rows when the diff_both value we computed above is 58 but we are dropping the rows that have the power_plant_name
# we found before
p = tables_df['Power Plant'].copy(deep=True)
p['duplicated'] = p.duplicated()
len(set(examples) & set(p.loc[p['duplicated'] == True]['power_plant_name'].unique())) == len(set(examples))

# drop the duplicates that we fixed before
before_dropping = tables_df['Power Plant'].shape[0]
tables_df['Power Plant'] = tables_df['Power Plant'].drop_duplicates()
rows_dropped =  before_dropping - tables_df['Power Plant'].shape[0]
print(f"Rows dropped: {rows_dropped}")
print(f"Check: the rows we are dropping now are the same as the rows that we found as duplicated before: {rows_dropped == diff_both}")

before_dropping = tables_df['City'].shape[0]
tables_df['City'] = tables_df['City'].drop_duplicates()
rows_dropped =  before_dropping - tables_df['City'].shape[0]
print(f"Rows dropped: {rows_dropped}")
print(f"Check: the rows we are dropping now are the same as the rows that we found as duplicated before: {rows_dropped == diff_both}")

"""#### 5.5 Visual Check"""

tables_df['Power Plant']

tables_df['City']

print(f"Rows with province %: {tables_df['City'].loc[(tables_df['City']['province'] != '')].shape[0] / tables_df['City'].shape[0] * 100}")

tables_df['Country']

tables_df['Equity']

if "source" in tables_df['Equity'].columns:
    tables_df['Equity'] = tables_df['Equity'].drop(columns=['source'])

print(f"All the j_ids come from the new equity rows (no merging): {tables_df['Equity'].loc[~tables_df['Equity']['j_id'].isna()].shape[0] == tables_df['Equity'].loc[tables_df['Equity']['equity_id'].str.contains('IJGLOBAL')].shape[0]}")

tables_df['Debt']

print(f"Not all the j_ids come from the new equity rows (there was merging): {tables_df['Debt'].loc[tables_df['Debt']['j_id'] != ''].shape[0] >= tables_df['Debt'].loc[tables_df['Debt']['debt_id'].str.contains('IJGLOBAL')].shape[0]}")

tables_df['Transaction']

for text in ['EQUITY', "DEBT"]:
    print(f"Rows that are about {text} #: {tables_df['Transaction'].loc[tables_df['Transaction']['investment_id'].str.contains(text)].shape[0]}")

tables_df['Investor']

print(f"Rows with parent company %: {tables_df['Investor'].loc[tables_df['Investor']['parent_company_of_investor'] != ''].shape[0] / tables_df['Investor'].shape[0] * 100}")

"""#### 5.5 Save"""

print_final_statistics(tables_df, tables_to_create)

intermediate_folder + entity + saving_suffix

saving_suffix

for entity in ['Power Plant', 'City', 'Country', 'Transaction', 'Investor']:
    tables_df[entity].to_csv(intermediate_folder + entity + saving_suffix, index=False)

entity = "Debt"
current_final_tables_folder + entity + ".csv"

for entity in ['Debt', 'Equity']:
    tables_df[entity].to_csv(current_final_tables_folder + entity + ".csv", index=False)

