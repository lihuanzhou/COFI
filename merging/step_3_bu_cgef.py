# -*- coding: utf-8 -*-
"""Step 3 - BU_CGEF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11rOfBqbMpdCVFCKCgqb3kGdTUjiJsNjX
"""

import pandas as pd
import numpy as np
from thefuzz import fuzz

# import custom-made functions
from functions.cleaning import get_variables, create_unique_id

from functions.cleaning import check_if_new_country_names, my_reset_index, update_country_names, convert_primary_fuel

from functions.joining import join_pp_full_debt_full
from functions.cleaning import preprocess_text
from functions.matching import find_matches_power_plant_or_fill_PP_key

from functions.final_tables_creation import get_tables_to_create_empty
from functions.cleaning import print_final_statistics
from functions.matching import find_matches_equity_and_debt_complete
from functions.cleaning import count_sources_in_id
from functions.cleaning import clean_primary_fuel_column
from functions.cleaning import get_thresholds_methods_dicts
from functions.cleaning import fix_investor_names
from functions.cleaning import my_read_excel
from functions.matching import prepare_pp_full_for_matching_commissioning_year
from functions.cleaning import fix_duplicated_power_plants_keys

# needed folder paths
input_folder = "Cleaned Data/" # or we can put this info in the "files name" sheet
intermediate_folder = "Intermediate Results/" # to save some intermediate results
current_final_tables_folder = "Current Final Tables/"  # where we store the final tables (e.g. Power Plant)

# load the excel containing all the mapping information
map_table_name = "Variables.xlsx"
map_table = pd.read_excel(map_table_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(map_table))

# load the excel containing all the mapping information
dictionary_fuel_name = "Dictionaries.xlsx"
dictionary_fuel = pd.read_excel(dictionary_fuel_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(dictionary_fuel))


# the sheet where there is the information that the client can change
files_names_sheet = "file names"
groupby_info_sheet = "formatting"
vertical_slicing_info_sheet = "variables to use"
aggregation_info_sheet = "aggregation"
db_structure_sheet = "db structure"
un_country_info_sheet = "UN Country Info"
country_names_dict_sheet = "country names dicionary"
bri_countries_sheet = "BRI countries"
matching_threshold_sheet = "matching thersholds"
matching_method_sheet = "matching methods"
matching_paremeter_sheet = "matching parameters"
# we will need this later
thresholds_df = map_table[matching_threshold_sheet]
methods_df = map_table[matching_method_sheet]
parameters_df = map_table[matching_paremeter_sheet]

# the tables we are creating here
tables_to_create = ["Power Plant", "City", "Country", "Debt", "Transaction", "Investor"]
investments_tables = ["Debt", "Transaction", "Investor"]

# already exising final data
# debt_file = "Debt.xlsx"
# power_plant_file = "Power Plant.xlsx"
# city_file = "City.xlsx"
# country_file = "Country.xlsx"
# bridge_file = "CITYKEY_BRIDGE_PP_C.xlsx"
debt_file = "Debt.csv"
power_plant_file = "Power Plant.csv"
city_file = "City.csv"
country_file = "Country.csv"
bridge_file = "CITYKEY_BRIDGE_PP_C.csv"

# names of the databases used here
bu_cgef_name = "BU_CGEF"

# saving information
saving_suffix = "_BUCGEF.csv"

"""## 1. Read Data

#### 1.1 Read data source to process: BU_CGEF
"""

# this table contains the name of the data sources
file_names = map_table[files_names_sheet]
file_names

bu_cgef = my_read_excel(input_folder , file_names.loc[file_names['Database'] == bu_cgef_name]["File name"].values[0])

# safe copy
bu_cgef_full = bu_cgef.copy(deep=True)

bu_cgef.head()

# all the energy sub sectors
bu_cgef["Energy Sub-sector"].unique()

# check for important NaN values
print("bu_id NaNs: " + str(bu_cgef['bu_id'].isna().sum()))
print("country NaNs: " + str(bu_cgef['country'].isna().sum()))
print("primary_fuel NaNs: " + str(bu_cgef['primary_fuel'].isna().sum()))
print("power plant name NaNs: " + str(bu_cgef['power_plant_name'].isna().sum()))
print("installed capacity NaNs: " + str(bu_cgef['installed_capacity'].isna().sum()))

bu_cgef['installed_capacity']

bu_cgef['installed_capacity'].isna().sum()

# TODO: I shouldn't do this here
bu_cgef['installed_capacity'] = bu_cgef['installed_capacity'].astype("string")
bu_cgef['installed_capacity'] = bu_cgef['installed_capacity'].fillna("")
bu_cgef['installed_capacity'] = bu_cgef['installed_capacity'].apply(lambda x: x.replace("MW", "").strip())
bu_cgef['installed_capacity'] = bu_cgef['installed_capacity'].apply(lambda x: np.nan if x == "" else x)
bu_cgef['installed_capacity'] = bu_cgef['installed_capacity'].astype("float64")

bu_cgef['installed_capacity']

bu_cgef['installed_capacity'].isna().sum()

"""#### 1.2 Load data for exising Power Plants and Debt data for matching purposes"""

# remember that we want to keep the keys so that we can use them if there is a match with exisitng data
columns_to_keep_pp = [ "PP_key", "power_plant_name", "installed_capacity", "primary_fuel", "country"]
columns_to_keep_debt = ["debt_id", "PP_key", "debt_investment_year", "investor_name", "amount"]

pp_full, db_pp = join_pp_full_debt_full(columns_to_keep_pp, columns_to_keep_debt, "Current Final Tables/", get_commissioning_year=True)

pp_full.head()

print(f"Power Plants rows: {pp_full.shape[0]}")
print(f"Debt rows: {db_pp.shape[0]}")

# check where the data is from: only from BUCGP and WEPP+GPPD
count_sources_in_id(pp_full, "PP_key", "Power Plant full")

# check where the data is from: only from BUCGP
count_sources_in_id(db_pp, "debt_id", "Debt full")

"""#### 1.3 Load Debt"""

debt = my_read_excel(current_final_tables_folder , debt_file)
debt.head()

"""## 2. Clean the data

#### 2.1 Vertical slicing.
"""

vertical_slicing = map_table[vertical_slicing_info_sheet]
vertical_slicing.head()

bu_cgef_variables = get_variables(vertical_slicing, bu_cgef_name, tables_to_create)
bu_cgef_variables

bu_cgef = bu_cgef[bu_cgef_variables]
bu_cgef.head(2)

"""#### 2.2 Variables cleaning for BU_CGEF"""

bu_cgef_full.isna().sum()

"""##### Country

Let's convert the names of the countries to the standard ones
"""

# no missing values
bu_cgef['country'].isna().sum()

# needed to run the functions to do the converting
country_names_dict_df = map_table[country_names_dict_sheet]
un_data = map_table[un_country_info_sheet]

# pre-process
bu_cgef['country'] = bu_cgef['country'].apply(lambda x: x.lower())

# check if there are names that are not standard or that are not in the dictionary to convert the country names
missing = check_if_new_country_names(list(bu_cgef['country'].unique()), un_data, country_names_dict_df)
missing

# convert the names
bu_cgef = update_country_names(country_names_dict_df, bu_cgef)

"""##### Primary fuel"""

bu_cgef['primary_fuel'].unique()

# make conversion
bu_cgef = clean_primary_fuel_column(bu_cgef)

bu_cgef['primary_fuel'].unique()

"""##### rename columns"""

bu_cgef = bu_cgef.rename(columns={"Lender": "debt_investor_name"})

"""##### investor_name"""

# TODO: so now I am chagning the investor_naame at the end because there can be multiple ivnestors in the "Other Lenders" column. and it's fine that we don't change it now because
# there is no debt investment to make since all debt data comes from bucgp and bucgp has no amount (see later)
# but for the future we should fix the investor name here in case there are investment matches to make

"""#### 2.3 Pre-process BU_CGEF and exisitng Power Plant and Debt data for matching"""

pp_full['commissioning_year'].astype("float64")

for col in columns_to_keep_pp:
    # turn all the strings to text except for the numerical ones (PP_key doesn't neeed any pre-processing)
    if col == "power_plant_name":
        pp_full[col] = pp_full[col].astype("string")
        pp_full[col] = pp_full[col].fillna("")
        pp_full[col] = pp_full[col].apply(preprocess_text)
    elif col == "country" or col == "primary_fuel":
        pp_full[col] = pp_full[col].astype("string")
        pp_full[col] = pp_full[col].fillna("")
    elif col == "installed_capacity":
        pp_full[col] = pp_full[col].astype("float64")
    elif col ==  "commissioning_year":
        # pp_full[col] = pp_full[col].apply(lambda x: np.nan if (isinstance(x, str)) else x)
        pp_full[col] = pp_full[col].astype("float64")

for col in ['primary_fuel', "country"]:
    print(f"NaNs in pp_full for {col}: {pp_full.loc[pp_full[col] ==''].shape[0]}")

# no need to pre-process the country and fuels because those are already good (we made sure
# of this when processing the previous data sources)
columns_to_keep_db_pp = columns_to_keep_debt + columns_to_keep_pp
for col in columns_to_keep_db_pp:
    if col == "investor_name" or col == "power_plant_name":
        db_pp[col] = db_pp[col].astype("string")
        db_pp[col] = db_pp[col].fillna("")
        db_pp[col] = db_pp[col].apply(lambda x: preprocess_text(x))
    if col == "primary_fuel" or col == "country":
        db_pp[col] = db_pp[col].astype("string")
        db_pp[col] = db_pp[col].fillna("")
    elif (col == 'debt_investment_year') or (col == "amount") or col == "installed_capacity":
        db_pp[col] = db_pp[col].astype("float64")

for col in ['primary_fuel', "country"]:
    print(f"NaNs in db_pp for {col}: {db_pp.loc[db_pp[col] == ''].shape[0]}")

for col in bu_cgef.columns:
    if col in ["debt_investment_year", "debt_investment_amount", "installed_capacity"]:
        bu_cgef[col] = bu_cgef[col].astype("float64")
    elif col in ["power_plant_name", "Lender", "Other Lenders"]:
        bu_cgef[col] = bu_cgef[col].astype("string")
        bu_cgef[col] = bu_cgef[col].fillna("")
        bu_cgef[col] = bu_cgef[col].apply(lambda x: preprocess_text(x, False))
        # do not remove punctuations because in "Other lenders" the different names are divided using punctatons
    elif col in ['country', "primary_fuel"]:
        bu_cgef[col] = bu_cgef[col].astype("string")
        bu_cgef[col] = bu_cgef[col].fillna("")

for col in ['primary_fuel', "country"]:
    print(f"NaNs in bu_cgef for {col}: {bu_cgef.loc[bu_cgef[col] == ''].shape[0]}")

"""## 3. Match data with already existing Debt data"""

# TODO: now there is no match to find because all the debt data (which comes out from BU_CGP) doens't have any information regarding the year.
# for the final version, we still need to plug in the function to do the match because in case the BU_CGP gets new information regarding the data, we then have
# enough information to do the matches and also plug in the functions/code that do the merging
# NOTE: the information regarding the investors can be found both in "Lender" and "Other Lenders", so we need to think about what to match with "investor_name" column
# (I would say both).
# then when we extract data to create Transaction and Investor rows we extract also the name that has matched already and we will process it when consolidating the Transaction rows

# all debt data doesn't have a year
db_pp['debt_investment_year'].isna().sum() == db_pp.shape[0]
# NOTE: this is true if the only datasource processed so far is BU_CGP (remember: Debt is constatnly updated)

bu_cgef = my_reset_index(bu_cgef)

# load the thresholds and methods to use
thrs_dict_dt, mtds_dict_dt, prmts_dict_dt = get_thresholds_methods_dicts(thresholds_df, methods_df, parameters_df, bu_cgef_name, "Debt")

prmts_dict_dt

matches = find_matches_equity_and_debt_complete(bu_cgef, db_pp, equity_or_debt="Debt",
                                                thresholds_dict = thrs_dict_dt,
                                                methods_dict = mtds_dict_dt,
                                                parameters_dict=prmts_dict_dt
                                                )
matches

"""## 4. Find the Power Plants"""

bu_cgef = my_reset_index(bu_cgef)

# load the parameters
thrs_dict_pp, mtds_dict_pp, prmts_dict_pp = get_thresholds_methods_dicts(thresholds_df, methods_df, parameters_df,bu_cgef_name, "Power Plant")

# prepare the pp_full dataset for the matching (it fixes up the year_range and then creates the upper and lower limit columns which are needed)
pp_full = prepare_pp_full_for_matching_commissioning_year(pp_full)

# no. of matches in standard settings: 57
bu_cgef_pp_matched = find_matches_power_plant_or_fill_PP_key(bu_cgef, pp_full, "PLANTKEY_BUCGEF_",
                                                            thresholds_dict = thrs_dict_pp,
                                                            methods_dict = mtds_dict_pp, equity_or_debt="Debt", parameters_dict=prmts_dict_pp)

# check that the matching went fine
# do some checks
for matched in [bu_cgef_pp_matched]:
    newly_created_name = "BUCGEF"

    # check that all the entries now have a proper PP_ke

    # no. of matched power_plants
    key_wepp = matched.loc[matched['PP_key'].str.contains("WEPPGPPD")].shape[0]
    key_bucgp = matched.loc[matched['PP_key'].str.contains("BUCGP")].shape[0]
    # key_bucgef = matched.loc[matched['PP_key'].str.contains("BUCGEF")].shape[0]
    # key_sais = matched.loc[matched['PP_key'].str.contains("SAIS")].shape[0]
    # key_fdi = matched.loc[matched['PP_key'].str.contains("FDIMARKETS")].shape[0]
    # key_ijg = matched.loc[matched['PP_key'].str.contains("IJG")].shape[0]

    # no. of newly created keys, these are new entries
    key_new = matched.loc[matched['PP_key'].str.contains(newly_created_name)].shape[0]

    # check that matched keys got the PP_key from Power Plant
    print(f"All matched keys got the PP_key from Power Plant: {key_wepp + key_bucgp  == matched.loc[~(matched['PP_key'].str.contains(newly_created_name))].shape[0]}")

    # check that each row has a PP_key
    print(f"Each row has a PP_key: {key_wepp + key_bucgp + key_new == matched.shape[0]}")

    print(pd.DataFrame(data=[["WEPP + GPPD", key_wepp], ["BUCGP", key_bucgp], ['BUCGEF', key_new]], columns=['Data source', "Count"]))

bu_cgef_pp_matched.loc[~bu_cgef_pp_matched['PP_key'].str.contains("BUCGEF")].sort_values(by="PP_key").head()

pp_full.loc[pp_full['PP_key'].isin(bu_cgef_pp_matched.loc[~bu_cgef_pp_matched['PP_key'].str.contains("BUCGEF")]['PP_key'])].sort_values(by="PP_key").head()

# show examples with commissioning year
examples = pp_full.loc[(pp_full['PP_key'].isin(bu_cgef_pp_matched.loc[~bu_cgef_pp_matched['PP_key'].str.contains("BUCGEF")]['PP_key'])) & (~pp_full['commissioning_year'].isna())].sort_values(by="PP_key").head()
examples

bu_cgef_pp_matched.loc[bu_cgef_pp_matched['PP_key'].isin(examples['PP_key'])].sort_values(by="PP_key")

"""## 5. Create tables

#### 5.0 Create the empty tables
"""

db_structure = map_table[db_structure_sheet]
tables_df = get_tables_to_create_empty(db_structure)
# used as support for the creation
tables_df_tmp = get_tables_to_create_empty(db_structure)

"""#### 5.0 Deal with duplicates Power Plant and City info

It could happen that multiple rows have the same information regarding the Power Plant: these stems from the original dataset having multiple investments on the same power plants on different rows. The issue happens when these rows are not matched to something in pp_full.
"""

# show example
bu_cgef_pp_matched.loc[bu_cgef_pp_matched['power_plant_name'] == "bui"]

bu_cgef_pp_matched.loc[bu_cgef_pp_matched['power_plant_name'] == "kostalac coal plant"]

# show example: these rows are not the same and they should be kept seperate!
bu_cgef_pp_matched.loc[bu_cgef_pp_matched['power_plant_name'] == "mali gouina hydropower project"]

bu_cgef_pp_matched, diff = fix_duplicated_power_plants_keys(bu_cgef_pp_matched,list(tables_df['Power Plant'].columns) + list(tables_df['City'].columns), "PLANTKEY_BUCGEF_")

# show example
bu_cgef_pp_matched.loc[bu_cgef_pp_matched['power_plant_name'] == "bui"]

bu_cgef_pp_matched.loc[bu_cgef_pp_matched['power_plant_name'] == "kostalac coal plant"]

# show example: these rows are not the same and they are inded skept seperate!
bu_cgef_pp_matched.loc[bu_cgef_pp_matched['power_plant_name'] == "mali gouina hydropower project"]

"""#### 5.1 Add debt_id"""

# create unique ids with custom function
bu_cgef_pp_matched = create_unique_id(bu_cgef_pp_matched, "debt_id", "DEBTID_BUCGEF_")
bu_cgef_pp_matched.head()

# check that we have all the IDs
for id_col in ['debt_id', "PP_key"]:
    print(f"Missing {id_col}: {bu_cgef_pp_matched.loc[bu_cgef_pp_matched[id_col].isna()].shape[0] + bu_cgef_pp_matched.loc[bu_cgef_pp_matched[id_col] == ''].shape[0]}")

"""#### 5.2 Deal with lenders names: get them and count them"""

# get all the lenders in one column: this will make it easier to count them and later
bu_cgef_pp_matched['All lenders'] = bu_cgef_pp_matched.apply(lambda row: row["debt_investor_name"] + ", " + row["Other Lenders"], axis=1)

def count_lenders(row):
    # function that counts all the lenders in "All lenders"
    lenders = row['All lenders'].split(", ")

    while "" in lenders:
        lenders.remove("")

    return len(lenders)

# count the lenders. this column name is the name present in the final database structure, so do not change it!
bu_cgef_pp_matched['number_of_lenders'] = bu_cgef_pp_matched.apply(count_lenders, axis=1)

# visual check
bu_cgef_pp_matched[['debt_investor_name', "Other Lenders", "All lenders", "number_of_lenders"]]

"""#### 5.3 Adding to the Debt table"""

# fill the db that can already be filled
# TODO: put "Equity" in a variable (not called equity_name)
entity = "Debt"
for column in tables_df_tmp[entity].columns:
    if column in bu_cgef_pp_matched.columns:
        tables_df_tmp[entity][column] = bu_cgef_pp_matched[column]

# make sure that the number_of_lenders column is NaN always (number_of_lenders is filled only with REF_LOAN data)
tables_df_tmp["Debt"]['number_of_lenders'] = np.nan

# concat
tables_df['Debt'] = pd.concat([debt, tables_df_tmp['Debt']])
# check
tables_df['Debt'].shape[0]  == debt.shape[0]  +  tables_df_tmp['Debt'].shape[0]

tables_df["Debt"].tail()

"""#### 5.4 Make Transaction and Investor tables.

Investor and Transaction need some tweaking because the names of the columns don't match with those of the db structure and because we need to compute some values.
"""

# TODO: what if the clients change what column matches equity_investor_name?

"""Note: we need to attribute the total debt amount over all the lenders. So here we just do the average over all the lenders."""

# TODO: check if the average is fine with the clients / in the COFI database / in what the previous consultant did.
# TODO: we can let the users customize it: we can have a new sheet in the Variables.xlsx that deals with this.

tables_df['Transaction'] = tables_df['Transaction'].head(0)
tables_df['Investor'] = tables_df['Investor'].head(0)

# TODO: make sure that the entity name of transaction is read somewhere
transaction_entity = 'Transaction'
investor_entity = "Investor"

transactions_rows = []
investors_rows =  []
for i, row in bu_cgef_pp_matched.iterrows():
    # get the investors in a list
    investors = row['All lenders'].split(", ")
    while "" in investors:
        investors.remove("") # needed to clean up

    # get the total amount per investor: we just do an average over all the investors
    amount_per_lender = row["debt_investment_amount"] / row["number_of_lenders"]

    # for each investor, now create a row for Transaction
    for investor in investors:
        transactions_rows.append([row['debt_id'], investor.strip(), amount_per_lender, "Y", "N", np.nan])
        investors_rows.append([investor.strip(), ""])

# create final tables
tables_df[transaction_entity] = pd.DataFrame(transactions_rows, columns=tables_df[transaction_entity].columns)
tables_df[investor_entity] = pd.DataFrame(investors_rows, columns=tables_df[investor_entity].columns)

tables_df['Transaction'].head()

tables_df['Investor'].head()

"""##### Let's fix the rows that have cofinincg

There are rows that have two investors per row under the "investor_name" column because they are cofinincing together the same investment.
"""

tables_df['Transaction'].loc[tables_df['Transaction']['investor_name'].str.contains("Cof")]

tables_df['Investor'].loc[tables_df['Investor']['investor_name'].str.contains("Cof")]

"""Since in the Transaction and Investor table we want one investor per table we are going to seperate the investors and diving the amounts up equally while making sure to keep the invesmtent_id of the original row. The plan:
* for each row, create new rows that have the same information but with the amounts divided
* delete the original rows in both
* add the new rows with a concat
"""

tables_df['Transaction'] = my_reset_index(tables_df['Transaction'])
tables_df['Investor'] = my_reset_index(tables_df['Investor'])

# get the investment_id key and investor_names so that we can drop this information in the Transaction and Investor tables
rows_with_issues =  tables_df['Transaction'].loc[tables_df['Transaction']['investor_name'].str.contains("Cof")]
indexes_to_drop =rows_with_issues.index.to_list()
investors_to_drop = list(rows_with_issues['investor_name'].unique())

# create a row for each investor: keep the same investment_id but split equally the amounts
# note: this code works even if there are more than 2 investors as long as they are divided by a "-" (e.g., "CDB-CHEXIM" but also "CDB-CHEXIM-BOC")
new_rows_transaction = []
new_rows_investor = []
for i, row in tables_df['Transaction'].loc[tables_df['Transaction']['investor_name'].str.contains("Cof")].iterrows():
    # get the investors names in a list
    investors = [x.strip() for x in row['investor_name'].replace("Cofinancing", "").split("-")]
    # in case the split adds some empty lists in the strings
    while "" in investors:
        investors.remove("")
    # divide the amounts equally among all the investors
    amount_per_investor = row['amount'] / len(investors)
    # create the new rows: get the original rows' investment_id and r_id
    # say that the amount was averaged and not weighted
    for investor in investors:
        new_rows_transaction.append([row['investment_id'], investor, amount_per_investor, "Y", "N", row['r_id']])
        new_rows_investor.append([investor, ""])

new_rows_transaction

new_rows_investor

# drop in the tables
tables_df['Transaction'] = tables_df['Transaction'].drop(indexes_to_drop)
print(f"Transaction: Rows with 'Cofinancing': {tables_df['Transaction'].loc[tables_df['Transaction']['investor_name'].str.contains('Cof')].shape[0]}")

indexes_to_drop_invst = tables_df['Investor'].loc[tables_df['Investor']['investor_name'].isin(investors_to_drop)].index.to_list()
tables_df['Investor'] = tables_df['Investor'].drop(indexes_to_drop_invst)
print(f"Investor: Rows with 'Cofinancing': {tables_df['Investor'].loc[tables_df['Investor']['investor_name'].str.contains('Cof')].shape[0]}")

# add the new rows
tables_df['Transaction'] = pd.concat([tables_df['Transaction'], pd.DataFrame(new_rows_transaction, columns=tables_df['Transaction'].columns)])

tables_df['Transaction'].tail()

tables_df['Investor'] = pd.concat([tables_df['Investor'], pd.DataFrame(new_rows_investor, columns=tables_df['Investor'].columns)])

tables_df['Investor'].tail()

print(f"Investor: Rows with 'Cofinancing': {tables_df['Investor'].loc[tables_df['Investor']['investor_name'].str.contains('Cof')].shape[0]}")
print(f"Transaction: Rows with 'Cofinancing': {tables_df['Transaction'].loc[tables_df['Transaction']['investor_name'].str.contains('Cof')].shape[0]}")

tables_df['Transaction'] = my_reset_index(tables_df['Transaction'])
tables_df['Investor'] = my_reset_index(tables_df['Investor'])

"""##### Update the investor names"""

# change the investor names
# TODO: see above why we shouldn't this here....

tables_df['Transaction'] = fix_investor_names(tables_df['Transaction'], "investor_name", False, None, drop_duplicates=False)

tables_df['Investor'] = fix_investor_names(tables_df['Investor'], "investor_name", False, None, drop_duplicates=False)

tables_df['Transaction']

tables_df['Investor']

"""#### 5.5 Make Power Plant and City and Country for new Power Plants found here!

There are new power plants here that were not matched with the ones already in Power Plant.
"""

missing_power_plants = bu_cgef_pp_matched.loc[bu_cgef_pp_matched['PP_key'].str.contains("PLANTKEY_BUCGEF_")]
print("Missing power plant: " + str(missing_power_plants.shape[0]))

# get the data from equity
for entity in ['Power Plant', "Country"]:
    for column in tables_df[entity].columns:
        if column in missing_power_plants.columns:
            tables_df[entity][column] = missing_power_plants[column]

entity = "City"
for column in tables_df[entity].columns:
    if column in missing_power_plants.columns:
        tables_df[entity][column] = missing_power_plants[column]
    elif column == "city_key":
        tables_df[entity][column] = missing_power_plants["PP_key"]

"""Since at the beginning of Section 5 we fixed the PP_keys to ensure the rows with the same values had the same PP_key, we can now drop the duplicated rows."""

before_dropping = tables_df['Power Plant'].shape[0]
tables_df['Power Plant'] = tables_df['Power Plant'].drop_duplicates()
rows_dropped =  before_dropping - tables_df['Power Plant'].shape[0]
print(f"Rows dropped: {rows_dropped}")
print(f"Check: the rows we are dropping now are the same as the rows that we found as duplicated before: {rows_dropped == diff}")

before_dropping = tables_df['City'].shape[0]
tables_df['City'] = tables_df['City'].drop_duplicates()
rows_dropped =  before_dropping - tables_df['City'].shape[0]
print(f"Rows dropped: {rows_dropped}")
print(f"Check: the rows we are dropping now are the same as the rows that we found as duplicated before: {rows_dropped == diff}")

"""#### 5.5 Visual check"""

tables_df['Power Plant']

tables_df['City']

tables_df['Country']

tables_df['Debt']

tables_df['Debt'].loc[tables_df['Debt']['PP_key'].isin(["PLANTKEY_BUCGEF_44", "PLANTKEY_BUCGEF_95"])]

tables_df['Transaction']

tables_df['Investor']

"""#### 5.5 Save"""

print_final_statistics(tables_df, tables_to_create)

# all but Debt get saved in the intermediate folder
for entity in ['Power Plant', 'City', 'Country', 'Transaction', 'Investor']:
    tables_df[entity].to_csv(intermediate_folder + entity + "_BUCGEF.csv", index=False)

for entity in ['Debt']:
    tables_df[entity].to_csv(current_final_tables_folder + entity + ".csv", index=False)

[print(x) for x in tables_df["Power Plant"].columns]