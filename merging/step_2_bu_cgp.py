# -*- coding: utf-8 -*-
"""Step 2 - BU_CGP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KQIYpXmBgZTI7aMJRwzM-Az82bsS0-_Z

Solution for the dropping of duplicates after the merging: https://chatgpt.com/share/2355b9f5-5a4e-4481-8b32-459d3cc19a9b
"""

import pandas as pd
import numpy as np

# import custom-made functions
from functions.cleaning import get_variables, create_placeholder, create_unique_id, get_variables_advanced, convert_primary_fuel, my_reset_index, print_final_statistics
from functions.groupby import do_groupby

from functions.final_tables_creation import get_tables_to_create_empty
from functions.cleaning import convert_equity_investment_type

from functions.cleaning import my_read_excel
from functions.cleaning import check_if_new_country_names, update_country_names
from functions.cleaning import fill_column_in_rows
from functions.cleaning import clean_primary_fuel_column
from functions.cleaning import fix_investor_names, get_joining_info

# for aggregating and joining using a range for commissioning_year
from functions.cleaning import get_comm_year_thr
from functions.groupby import get_pairs_to_fix
from functions.groupby import seperate_rows
from functions.groupby import do_groupby_new
from functions.groupby import do_clustering
from functions.cleaning import prepare_for_joining, count_sources_in_id
from functions.joining import is_close_overlap

# needed folder paths
input_folder = "Cleaned Data/" # or we can put this info in the "files name" sheet
intermediate_folder = "Intermediate Results/" # to save some intermediate results
current_final_tables_folder = "Current Final Tables/" # where we store the final tables (e.g. Power Plant)

# load the excel containing all the mapping information
map_table_name = "Variables.xlsx"
map_table = pd.read_excel(map_table_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(map_table))

# load the excel containing all the mapping information
dictionary_file_name = "Dictionaries.xlsx"
map_dictionaries = pd.read_excel(dictionary_file_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(map_dictionaries))


# the sheet where there is the information that the client can change
files_names_sheet = "file names"
groupby_info_sheet = "formatting"
vertical_slicing_info_sheet = "variables to use"
aggregation_info_sheet = "aggregation"
db_structure_sheet = "db structure"
un_country_info_sheet = "UN Country Info"
country_names_dict_sheet = "country names dicionary"
bri_countries_sheet = "BRI countries"
joining_info_sheet = "joining"
commissioning_year_thresholds_info_sheet = "commissioning_year_thresholds"

# the tables we are creating here
tables_to_create = ["Power Plant", "City", "Country", "Equity", "Debt", "Transaction", "Investor"]
investments_tables = ["Equity", "Debt", "Transaction", "Investor"]

# already exising final data
# power_plant_file = "Power Plant.xlsx"
power_plant_file = "Power Plant.csv"

# names of the databases used here
bu_cgp_name = "BU_CGP"

"""## 1. Read Data

#### 1.1 Load data
"""

# this table contains the name of the data sources
file_names = map_table[files_names_sheet]
file_names

bu_cgp = my_read_excel(input_folder , file_names.loc[file_names['Database'] == bu_cgp_name]["File name"].values[0])

# safe copy
bu_cgp_full = bu_cgp.copy(deep=True)

bu_cgp.head()

bu_cgp.shape

bu_cgp['commissioning_year'].isna().sum()

"""#### 1.2 Load Power Plant for matching later"""

# read the Power Plant data
pp = my_read_excel(current_final_tables_folder, power_plant_file)
pp.head()

pp_copy = pp.copy(deep=True)

count_sources_in_id(pp, "PP_key", "pp")

"""## 2. Clean the data

#### 2.1 Vertical slicing

Only keep the variables that we need because they will be matched to the final database structure.
"""

vertical_slicing = map_table[vertical_slicing_info_sheet]
vertical_slicing.head(5)

bu_cgp_variables = get_variables(vertical_slicing, bu_cgp_name, tables_to_create)
bu_cgp_variables

bu_cgp = bu_cgp[bu_cgp_variables].copy(deep=True)
bu_cgp.head(2)

# also for pp
# we only care in the location_id and primary_fuel and commissioning_year since those are the ones that we use to join, and "PP_key" which we want to get
# this makes it easier to deal with pp
pp = pp[["PP_key",'location_id', "primary_fuel",  "commissioning_year", "compid", "year_range"]]
pp.head()

"""#### 2.2 Fill empty values for location_id"""

bu_cgp.isna().sum()

# TODO: think of a way that I don't hardcode the columns that need to be filled
# TODO: based on these values and those columns that I am intrested in using, then I fill the placeholder

"""So, we just need to fill for some missing location_ids."""

# this is needed because now we are added strings as location ids
bu_cgp['location_id'] = bu_cgp['location_id'].astype("string")

# create a placeholder for location_id with imported custom function
for index, row in bu_cgp.iterrows():
    bu_cgp.at[index, "location_id"] = create_placeholder(row, "location_id", "unknown_BUCGP_location_id_", index)

# check that the filling went smoothly
bu_cgp.isna().sum()

bu_cgp.loc[bu_cgp['location_id'].str.contains("_")]

"""#### 2.3 Clean columns"""

# check for important NaN values
print("location_id NaNs: " + str(bu_cgp['location_id'].isna().sum()))
print("primary_fuel NaNs: " + str(bu_cgp['primary_fuel'].isna().sum()))
print("BU ID NaNs: " + str(bu_cgp['bu_id'].isna().sum()))

"""##### installed_capacity (should be in the cleaning step of the whole analysis)"""

# TODO: I shouldn't do this here
bu_cgp['installed_capacity']

# make the change
bu_cgp['installed_capacity'] = bu_cgp['installed_capacity'].apply(lambda x: x.replace("MW", "").strip())
bu_cgp['installed_capacity'] = bu_cgp['installed_capacity'].astype("float64")
bu_cgp['installed_capacity']

"""##### Change the column names for those that need to"""

# TODO: maybe we should not hardcode this transformation, no? There is already function that finds the renamings needed....
bu_cgp = bu_cgp.rename(columns={"bu_id": "bu_id_bu_cgp"})

"""##### Fix primary_fuel"""

bu_cgp['primary_fuel'].unique()

# make conversion
bu_cgp = clean_primary_fuel_column(bu_cgp)

bu_cgp['primary_fuel'].unique()

"""##### Country"""

# needed to run the functions to do the converting
country_names_dict_df = map_table[country_names_dict_sheet]
un_data = map_table[un_country_info_sheet]

# pre-process
bu_cgp['country'] = bu_cgp['country'].apply(lambda x: x.lower())

# check if there are names that are not standard or that are not in the dictionary to convert the country names
check_if_new_country_names(list(bu_cgp['country'].unique()), un_data, country_names_dict_df)

# convert the names
bu_cgp = update_country_names(country_names_dict_df, bu_cgp)

"""##### Clean commisisoning_year

There is still an issue with some entries having "Pending" as their commissioning_year.
"""

bu_cgp.loc[bu_cgp["commissioning_year"].str.contains("Pendi", na=False)]

bu_cgp['commissioning_year'] = bu_cgp['commissioning_year'].apply(lambda x : np.nan if(x == "Pending") else x)

bu_cgp['commissioning_year'].isna().sum()

"""##### investor_name"""

bu_cgp['Lender'] = bu_cgp['Lender'].fillna("")
bu_cgp['equity_investor_name'] = bu_cgp['equity_investor_name'].fillna("")

bu_cgp = fix_investor_names(bu_cgp, "Lender", False, None, drop_duplicates=False)

bu_cgp = fix_investor_names(bu_cgp, "equity_investor_name", False, None, drop_duplicates=False)

bu_cgp[['Lender', "equity_investor_name"]]

"""##### Fix dtypes"""

bu_cgp.dtypes

bu_cgp["province"] = bu_cgp["province"].astype("string")
bu_cgp["installed_capacity"] = bu_cgp["installed_capacity"].astype("float64")
bu_cgp.dtypes

"""##### Fix equity_investment_type"""

# TODO: Test this
print(f"Before: {bu_cgp['equity_investment_type'].unique()}")
eq_pp = convert_equity_investment_type(map_dictionaries[bu_cgp_name + "_equity_investment_type"], bu_cgp)
print(f"After: {bu_cgp['equity_investment_type'].unique()}")

"""#### 3.3 Prepare commissioning year and primary_fuel for the grouping

As we did for WEPP and GPPD, we will be grouping over primary_fuel and commissioning_year so we need to make sure there are no NaN values and the placeholders that we create are unique.
"""

bu_cgp['commissioning_year'].isna().sum()

bu_cgp['primary_fuel'].isna().sum()

# fix the format to allow for the placeholders
bu_cgp['commissioning_year'] = bu_cgp['commissioning_year'].fillna("")
bu_cgp['commissioning_year'] = bu_cgp['commissioning_year'].astype("string")

bu_cgp['primary_fuel'] = bu_cgp['primary_fuel'].fillna("")
bu_cgp['primary_fuel'] = bu_cgp['primary_fuel'].astype("string")

# check
print(f"Missing year: {bu_cgp.loc[bu_cgp['commissioning_year'] == ''].shape[0]}")
print(f"Missing fuelP: {bu_cgp.loc[bu_cgp['primary_fuel'] == ''].shape[0]}")

# add the placeholders
for col in ['commissioning_year', "primary_fuel"]:
    for i, row in bu_cgp.loc[bu_cgp[col] == ""].iterrows():
        bu_cgp.at[i, col] = create_placeholder(row, col, prefix = f"missing_{col}_bucgp_", unique_number = i)

# check (compare it with the values printed before making the change)
print(f"Missing year but now with placeholder: {bu_cgp.loc[bu_cgp['commissioning_year'].str.contains('missing')].shape[0]}")
print(f"Missing fuelP but now with placeholder: {bu_cgp.loc[bu_cgp['primary_fuel'].str.contains('missing')].shape[0]}")

# check that the placeholders are indeed unique
for col in ['commissioning_year', "primary_fuel"]:
    all_missing = bu_cgp.loc[bu_cgp[col].str.contains("missing")]
    print(f"All unique placeholders for {col}: {all_missing.shape[0] == all_missing[col].nunique()}")

# also change the type in pp
pp['commissioning_year'] = pp['commissioning_year'].fillna("")
pp['commissioning_year'] = pp['commissioning_year'].astype("string")

"""#### 2.4 Prepare location_id for future merging"""

# since here equity_agg and debt_agg have strings as their location_id
pp['location_id'] = pp['location_id'].astype("string")

# note the location_ids had a decimal value

# first fix type
bu_cgp['location_id'] = bu_cgp['location_id'].astype("string")
# make change
bu_cgp['location_id'] = bu_cgp['location_id'].apply(lambda x: x.replace(".0", ""))

"""## 3. Groupby"""

bu_cgp.loc[bu_cgp['power_plant_name'].str.contains("Kpone")]

# ids for which there is more than one fuel and more than one commissioning_year
[x for x in bu_cgp['location_id'].unique() if bu_cgp.loc[bu_cgp['location_id'] == x]['primary_fuel'].nunique() > 1 and bu_cgp.loc[bu_cgp['location_id'] == x]['commissioning_year'].nunique() > 1]

# example: there are two banks investing on the same power plant
bu_cgp.loc[bu_cgp['location_id'] == '1135654']

"""#### 3.0 Strategy discussion

The data is at the unit level. So, as for step 1, we need to aggregate based on location_id and primary_fuel and commissioning_year. But the issue is that there could be different investors on the different units at the same power plant (identified as location_id + primary_fuel).
"""

# example: there are two banks investing on the same power plant
bu_cgp.loc[bu_cgp['location_id'] == "1104058"]

# if we aggregate only on primary_fuel and power_plant, then we loose info e.g. if we only take the max
bu_cgp.loc[bu_cgp['location_id'] == "1104058"].groupby(by=['location_id', "primary_fuel", "commissioning_year"]).agg({"Lender": "max"})

"""For the equity, it is a similar issue with the added thing that the same company could have different investments types per unit (ideally). So the solution has been to aggregate using also the investment information to keep as much of the original information as possible. And to make it easier to later create the final tables (e.g., we could have concatanated the information on the investor names but it was deemed to be less clean).

#### 3.1 Separate in data about Equity and data about Debt

BU_CGP has three kinds of entries:
* only Equity investments: the "Lender" information is NaN
* only Debt investments: the "equity_investor_name" column is NaN
* both Equity and Debt investments: they have all information.

Finding a strategy of aggregating that is valid for all three cases is hard. So, we separate the data according to entries that have Debt information and entries that have Equity information.
* Note that for those entries that have both, we will slice the data vertically so that the Equity information goes with the other Equity data and same for the Debt data.
"""

# entries that have both
bu_cgp.loc[bu_cgp['equity_investment_type'] == "FDI + Policy bank"].shape[0]

# get equity
equity_name = bu_cgp_name + " - Equity" # this name matches the information in Variables.xlsx's "formatting" and "aggregation" sheet
columns_equity = list(bu_cgp.columns)
columns_equity.remove("Lender")
equity = bu_cgp[columns_equity]
equity = equity.loc[equity['equity_investor_name'] != ""]
equity = my_reset_index(equity)
equity.head()

print(f"Equity rows #: {equity.shape[0]}")
print(f"Equity rows % over BU_CGP rows: {equity.shape[0] / bu_cgp.shape[0] * 100}")

# makes sense: only equity investments
equity['equity_investment_type'].unique()

# get debt
debt_name = bu_cgp_name + " - Debt"
columns_debt = list(bu_cgp.columns)
columns_debt.remove("equity_investor_name")
debt = bu_cgp[columns_debt]
debt = debt.loc[debt["Lender"] != ""]
debt = my_reset_index(debt)
debt.head()

print(f"Equity rows #: {debt.shape[0]}")
print(f"Equity rows % over BU_CGP rows: {debt.shape[0] / bu_cgp.shape[0] * 100}")

# makes sense: only debt investments
debt['equity_investment_type'].unique()

for col in ["commissioning_year", "primary_fuel"]:
    debt_missing = debt.loc[debt[col].str.contains("missing")].shape[0]
    equity_missing = equity.loc[equity[col].str.contains("missing")].shape[0]
    print(f"Check: missing {col} in debt: {debt_missing}")
    print(f"Check: missing {col} in equity: {equity_missing}")
    print(f"Check: total {col} missing: {debt_missing + equity_missing}\n")

equity.loc[equity['power_plant_name'].str.contains("Adama")]

debt.loc[debt['power_plant_name'].str.contains("Adama")]

"""#### 3.2 Do groupby

##### 0. Load info

Since there are now two tables that we are dealing with, we added these tables on the "formatting" sheet so to guarantee that the final users can still play around with how we aggregate if needed. The same goes for the "aggregation" sheet.
"""

aggregation_info = map_table[aggregation_info_sheet]
aggregation_info

commissioning_year_thresholds_info = map_table[commissioning_year_thresholds_info_sheet]
commissioning_year_thresholds_info

YEAR_THRESHOLD_AGG_BUCGP = get_comm_year_thr(commissioning_year_thresholds_info, "Aggregating BU_CGP")
YEAR_THRESHOLD_AGG_BUCGP

"""##### A. Study"""

debt_pairs_to_fix = get_pairs_to_fix(debt, "location_id")
print(f"Pairs to fix: {debt_pairs_to_fix.shape[0]}")
debt_pairs_to_fix.head()

# show example
row_example = debt_pairs_to_fix.iloc[3]
debt.loc[(debt['location_id'] == row_example['location_id']) & (debt['primary_fuel'] == row_example['primary_fuel'])]

equity_pairs_to_fix = get_pairs_to_fix(equity, "location_id")
print(f"Pairs to fix: {equity_pairs_to_fix.shape[0]}")
equity_pairs_to_fix.head()

# show example
row_example = equity_pairs_to_fix.iloc[0]
equity.loc[(equity['location_id'] == row_example['location_id']) & (equity['primary_fuel'] == row_example['primary_fuel'])]

"""##### B. Seperate"""

debt_special, debt_normal, debt_pairs_to_fix = seperate_rows(debt, debt_pairs_to_fix, "location_id")

for col in ['commissioning_year', "primary_fuel"]:
    missing = 0
    for db in [debt_special, debt_normal]:
        missing += db.loc[db[col].str.contains("missing")].shape[0]
    print(f"Check: missing {col} in debt: {missing}")

equity_special, equity_normal, equity_pairs_to_fix = seperate_rows(equity, equity_pairs_to_fix, "location_id")

for col in ['commissioning_year', "primary_fuel"]:
    missing = 0
    for db in [equity_special, equity_normal,]:
        missing += db.loc[db[col].str.contains("missing")].shape[0]
    print(f"Check: missing {col} in equity: {missing}")

"""##### C. Do the actual groupby"""

aggregation_info = map_table[aggregation_info_sheet]

"""<b>Normal"""

debt_normal_agg = do_groupby_new(debt_normal, ["location_id", "primary_fuel", "commissioning_year", "Lender"], debt_name, aggregation_info)
print(f"Check: the rows have been aggregated correctly: {debt_normal_agg.shape[0] <= debt_normal.shape[0]}")

# show example
example = ("1104058", "hydropower")
debt_normal.loc[(debt_normal['location_id'] == example[0]) & (debt_normal['primary_fuel'] == example[1])]

# aggregated example
debt_normal_agg.loc[(debt_normal_agg['location_id'] == example[0]) & (debt_normal_agg['primary_fuel'] == example[1])]

equity_normal_agg = do_groupby_new(equity_normal, ["location_id", "primary_fuel","commissioning_year", "equity_investor_name", "equity_investment_type"], equity_name, aggregation_info)
print(f"Check: the rows have been aggregated correctly: {equity_normal_agg.shape[0] <= equity_normal.shape[0]}")

# show example
example = ("1073101", "hydropower")
equity_normal.loc[(equity_normal['location_id'] == example[0]) & (equity_normal['primary_fuel'] == example[1])]

# aggregated example
equity_normal_agg.loc[(equity_normal_agg['location_id'] == example[0]) & (equity_normal_agg['primary_fuel'] == example[1])]

"""<b>Special

First cluster.
"""

# 0.5s
debt_special_clustered = do_clustering(debt_special, debt_pairs_to_fix, YEAR_THRESHOLD_AGG_BUCGP)

# show visual example
debt_special_clustered[['full_index', "commissioning_year", "year_cluster"]].head(10)

# 0.3s
equity_special_clustered = do_clustering(equity_special, equity_pairs_to_fix, YEAR_THRESHOLD_AGG_BUCGP)

# show visual example
equity_special_clustered[['full_index', "commissioning_year", "year_cluster"]].head(15)

"""Then, groupby"""

debt_special_agg = do_groupby_new(debt_special_clustered, ["location_id", "primary_fuel", "year_cluster", "Lender"], debt_name, aggregation_info)

# show example
example = debt_pairs_to_fix.iloc[0]
debt_special_clustered.loc[(debt_special_clustered['location_id'] == example['location_id']) & (debt_special_clustered['primary_fuel'] == example['primary_fuel'])]

# aggregated example
debt_special_agg.loc[(debt_special_agg['location_id'] == example['location_id']) & (debt_special_agg['primary_fuel'] == example['primary_fuel'])]

equity_special_agg = do_groupby_new(equity_special_clustered, ["location_id", "primary_fuel","year_cluster", "equity_investor_name", "equity_investment_type"], equity_name, aggregation_info)

# show example
example = equity_pairs_to_fix.iloc[2]
equity_special_clustered.loc[(equity_special_clustered['location_id'] == example['location_id']) & (equity_special_clustered['primary_fuel'] == example['primary_fuel'])]

# aggregated example
equity_special_agg.loc[(equity_special_agg['location_id'] == example['location_id']) & (equity_special_agg['primary_fuel'] == example['primary_fuel'])]

"""##### D. Put everything back together"""

for col in ['commissioning_year', "primary_fuel"]:
    missing = 0
    for db in [debt_normal_agg, debt_special_agg]:
        missing += db.loc[db[col].str.contains("missing")].shape[0]
    print(f"Check: missing {col} in debt: {missing}")

debt_agg = pd.concat([debt_normal_agg, debt_special_agg])
debt_agg = my_reset_index(debt_agg)

for col in ['commissioning_year', "primary_fuel"]:
    missing = debt_agg.loc[debt_agg[col].str.contains("missing")].shape[0]
    print(f"Check: missing {col} in debt: {missing}")

for col in ['commissioning_year', "primary_fuel"]:
    missing = 0
    for db in [equity_normal_agg, equity_special_agg]:
        missing += db.loc[db[col].str.contains("missing")].shape[0]
    print(f"Check: missing {col} in equity: {missing}")

equity_agg = pd.concat([equity_normal_agg, equity_special_agg])
equity_agg = my_reset_index(equity_agg)

for col in ['commissioning_year', "primary_fuel"]:
    missing = equity_agg.loc[equity_agg[col].str.contains("missing")].shape[0]
    print(f"Check: missing {col} in equity: {missing}")

"""##### E. Check"""

print(f"Check: all the unique debt loc_id+primary_fuels pairs are still in debt_agg: {len(set(debt_agg['full_index'].unique()) & set(debt['full_index'].unique())) == len(set(debt['full_index'].unique()))}")

print(f"Check: all the unique equity loc_id+primary_fuels pairs are still in debt_agg: {len(set(equity_agg['full_index'].unique()) & set(equity_agg['full_index'].unique())) == len(set(equity_agg['full_index'].unique()))}")

"""## 4. Find matches with Power Plant

We have some power plants here in the BU_CGP table. There are two possiblities:
* the power plants here match those presents in Power Plant (which stem from the union of GPPD and WEPP)
    * then we just need to get the power plant's key from there and put it here in the Equity and Debt table.
* the power plants here are not present there.
    * in this case we need to add them and create a unique id ("PLANTKEY_BUCPG_XX") to leave in the Equity and Debt table
    * I first create the new power plant keys for these entries as "PLANTKEY_BUCGP_EQ_index" for those that come from equity data and "PLANTKEY_BUCGP_DT_index" for those coming from debt data. This is need to make sure that there are only unique values *across* the equity and debt dataframes

The join is:
* multi-key: because a power plant is defined by its location_id and primary_fiel and commissioning_year
* how="right": we want to keep all those power plants in bu_cgp that don't have yet a match in Power Plant (the equity and debt data stays on the right of the merge)
"""

# example: a power plant in PP
id = "1138118"
pp.loc[pp['location_id'] == id]

# the same power plant can be found in equity
equity_agg.loc[equity_agg['location_id'] == id]

# but not in debt
debt_agg.loc[debt_agg['location_id'] == id]

"""#### 4.1 Merge equity data

##### A. Prepare

We create the columns that have the upper and lower limit of the year range we have aggregated on. If a row hasn't been aggregated (because it was in equity_normal), we create still these columns by putting them equal to the commissioning_year.
"""

isinstance("aa", str)

from functions.joining import prepare_for_joining

equity_agg = prepare_for_joining(equity_agg)
#@TODO check here!! seems like there happend to be "-" corner case check the func
pp = prepare_for_joining(pp)

# N.B.: There is no need to fill the "commissioning_year" column because the year_range (which we use to do the "lower_limit" and "upper_limit" columns)
# already has the "missing" placeholders so the two limits columns do not get filled up
pp.loc[pp['commissioning_year'].isna()].head()

for col in ['commissioning_year', "primary_fuel"]:
    missing = equity_agg.loc[equity_agg[col].str.contains("missing")].shape[0]
    print(f"Check: missing {col} in equity: {missing}")

pp.head()

pp.loc[pp['location_id'] == "1085906"]

"""##### B. Merge & compute match

First, we match each row in Equity with all the potential matches in PP by matching on location_id+primary_fuel. We also keep the rows in equity that do not have a match in PP (by doing a "right" join).
"""

eq_agg_full = pd.merge(pp, equity_agg, left_on=['location_id', "primary_fuel"], right_on = ['location_id', "primary_fuel"], how="right", suffixes=('_pp', "_equity"))

eq_agg_full

matches_no = eq_agg_full.loc[(~eq_agg_full['lower_limit_pp'].isna()) & (~eq_agg_full['lower_limit_equity'].isna())].shape[0]
print(f"Matches made #: {matches_no}")
print(f"Matches made %: {matches_no / eq_agg_full.shape[0] * 100}")

for col in ['commissioning_year_equity', "primary_fuel"]:
    missing = eq_agg_full.loc[eq_agg_full[col].str.contains("missing")][col].nunique()
    print(f"Check: missing {col} in equity: {missing}")

# visual example
eq_agg_full[['location_id', "primary_fuel", "year_range_equity", "year_range_pp", "PP_key"]].head()

"""Now, for all the potential matches we see if they are good matches. That is, if the range overlaps (we also use a tolerance)."""

# tolerance
YEAR_THRESHOLD_JOIN_BUCGP = get_comm_year_thr(commissioning_year_thresholds_info, "Joining db & BU_CGP")
YEAR_THRESHOLD_JOIN_BUCGP

# compute if the potential are good matches
eq_agg_full['match'] = eq_agg_full.apply(lambda row: is_close_overlap(row['lower_limit_pp'], row['upper_limit_pp'], row['lower_limit_equity'], row['upper_limit_equity'], tolerance=YEAR_THRESHOLD_JOIN_BUCGP) if np.isnan(row['lower_limit_equity']) == False and np.isnan(row['lower_limit_pp']) == False else False, axis=1)

# visual example
eq_agg_full[['location_id', "primary_fuel", "year_range_equity", "year_range_pp", "match"]].head()

# check how many valid pairs we have and how many are True vs False
# valid pairs: they have both information on WEPP and GPPD in their row
valid_matches_made = eq_agg_full.loc[(~eq_agg_full['lower_limit_pp'].isna()) & (~eq_agg_full['lower_limit_equity'].isna())]
print(f"Valid matches #: {valid_matches_made.shape[0]}")
print(f"Valid matches %: {valid_matches_made.shape[0] / eq_agg_full.shape[0] * 100}")
for value in [True, False]:
    print(f"{value} matches #: {valid_matches_made.loc[valid_matches_made['match'] == value].shape[0]}")
    print(f"{value} matches %: {valid_matches_made.loc[valid_matches_made['match'] == value].shape[0] / valid_matches_made.shape[0] * 100}")

# visual check
valid_matches_made.loc[valid_matches_made['match'] == True][['lower_limit_pp', "upper_limit_pp", 'lower_limit_equity', "upper_limit_equity", "match"]]

# check that all those rows that have at least one year missing false as a match
missing_years = eq_agg_full.loc[(eq_agg_full['commissioning_year_equity'].str.contains("missing")) | (eq_agg_full['commissioning_year_pp'].isna()) ]
print(f"Check: all the rows with a missing year are not matched: {missing_years.loc[missing_years['match'] == False].shape[0] == missing_years.shape[0]}")

eq_agg_full.loc[eq_agg_full['power_plant_name'].str.contains("Kpone")][['power_plant_name', "year_range_equity", "year_range_pp", "PP_key", "match"]]

"""##### C. Clean

<b>Clean PP_key

There are two kinds of "False" matches here:
* Those that have been matched to a pp row but the match wasn't good
* Those that have been not matched to a pp row.
"""

# Those that have been matched to a pp row but the match wasn't good
false_1 = eq_agg_full.loc[(eq_agg_full['match'] == False) & (~eq_agg_full['PP_key'].isna())]
false_1[["PP_key", "match", "year_range_equity", "year_range_pp"]]

eq_agg_full.loc[(eq_agg_full['match'] == True)]

# Those that have been not matched to a pp row.
false_2 = eq_agg_full.loc[(eq_agg_full['match'] == False) & (eq_agg_full['PP_key'].isna())]
false_2[["PP_key", "match", "year_range_equity", "year_range_pp"]]

print(f"Check: all the rows with 'False' match belong to one of the two categories above: {false_1.shape[0] + false_2.shape[0] == eq_agg_full.loc[(eq_agg_full['match'] == False)].shape[0]}")

"""In both cases: these don't have a match, so we can just turn the PP_key to nan for all."""

eq_agg_full[['match', "PP_key"]]

# put a nan for all the matches that are not good
eq_agg_full['PP_key'] = eq_agg_full.apply(lambda row: np.nan if row['match'] == False else row['PP_key'], axis=1)

eq_agg_full[['match', "PP_key"]]

for col in ['commissioning_year_equity', "primary_fuel"]:
    missing = eq_agg_full.loc[eq_agg_full[col].str.contains("missing")][col].nunique()
    print(f"Check: missing {col} in equity: {missing}")

"""<b>Clean the columns names

We want to keep the commissioning_year and year_range for those rows that don't have a match in WEPP+GPPD, so we have to rename the columns for them (we need to remove "_equity" otherwise they won't be picked up when creating the final tables in Step 5).
"""

columns_to_keep = ['commissioning_year', 'year_range']
eq_agg_full = eq_agg_full.rename(columns={col+"_equity": col for col in columns_to_keep})

eq_agg_full[['commissioning_year', 'year_range']].head()

"""<b>Drop duplicates

There are duplicates because when we did a merge above we matched a single equity row with all the possible matches in PP (which could be more than 1).
"""

# show example
example = ("1101252", "hydropower", "missing_commissioning_year_bucgp_964")
eq_agg_full.loc[(eq_agg_full['location_id'] == example[0]) & (eq_agg_full['primary_fuel'] == example[1]) & (eq_agg_full['commissioning_year'] == example[2]) ][['location_id', "primary_fuel", "commissioning_year"]]

"""But it is tricky to drop duplicates: What about the PP_keys?

We need to preserve the information that was in equity (so we need to look at the equity columns)
"""

# the same row got matched to a correct PP_key and sometimes it didn't. So we need to keep the row that has the PP_key
eq_agg_full.loc[eq_agg_full['power_plant_name'].str.contains("Kpone")][["PP_key"] + list(equity.columns) ]

# another example
eq_agg_full.loc[eq_agg_full['power_plant_name'].str.contains("Adama")][["PP_key"] + list(equity.columns) ]

"""Strategy:
* for each row with a valid PP_key, we get all the rows that contain the same information in terms of the original equity columns
* we assign to these other rows the same PP_key
* and then we drop the duplicates

We should remain with as many original rows as in equity agg since we set out to find a match for each row in equity.
"""

from functions.cleaning import find_equal_rows

# we use this function to get all the rows that are equal to the row with a valid PP_key

# convinient variable
equity_columns = list(equity.columns)

# always re-index since we are going to change the values within the dataframe
eq_agg_full = my_reset_index(eq_agg_full)

# get all the rows (indexes) that have a valid PP_key
to_fix = eq_agg_full.loc[(~eq_agg_full['PP_key'].isna())].index.to_list()

# make the change
# for each row with a valid PP_key
for index_to_fix in to_fix:
    row = eq_agg_full.iloc[index_to_fix] # get the row
    # get the indexes of all the rows that are equal to this one
    # and for each index update the PP_key
    for i in find_equal_rows(eq_agg_full, row, equity_columns):
        row_t = eq_agg_full.iloc[i] # get the row that we are updating for convinience
        # only update the PP_key column if the value is not there yet
        if isinstance(row_t['PP_key'], str) == False:
            eq_agg_full.at[i, "PP_key"] = row['PP_key']

# show example: now you can see that the rows with the same values have the same PP_key (or the lack thereof)
eq_agg_full.loc[eq_agg_full['power_plant_name'].str.contains("Kpone")][equity_columns+['PP_key']]

# show example: now you can see that the rows with the same values have the same PP_key (or the lack thereof)
eq_agg_full.loc[eq_agg_full['power_plant_name'].str.contains("Adama")][equity_columns+['PP_key']]

# now we can drop based on the equity information & PP_key that we just fixed
eq_agg_full = eq_agg_full.drop_duplicates(subset=equity_columns + ['PP_key'])
eq_agg_full = my_reset_index(eq_agg_full)

# checks
print(f"There are as many rows in eq_agg_full as in equity_agg: {equity_agg.shape[0]== eq_agg_full.shape[0]}")
print(f"All the bu_id_bu_cgp have been preserved: {len(set(equity_agg['bu_id_bu_cgp'].unique()) & set(eq_agg_full['bu_id_bu_cgp'].unique())) == len(set(equity_agg['bu_id_bu_cgp'].unique()))}")
print(f"The two datasets have the same values: {(eq_agg_full[equity_columns] == equity_agg[equity_columns]).all().all()}")

"""#### 4.2 Merge Debt

Exactly what we did for Equity.

##### A. Prepare
"""

debt_agg = prepare_for_joining(debt_agg)

for col in ['commissioning_year', "primary_fuel"]:
    missing = debt_agg.loc[debt_agg[col].str.contains("missing")].shape[0]
    print(f"Check: missing {col} in debt: {missing}")

"""##### B. Merge & compute match"""

dt_agg_full = pd.merge(pp, debt_agg, left_on=['location_id', "primary_fuel"], right_on = ['location_id', "primary_fuel"], how="right", suffixes=('_pp', "_debt"))

matches_no = dt_agg_full.loc[(~dt_agg_full['lower_limit_pp'].isna()) & (~dt_agg_full['lower_limit_debt'].isna())].shape[0]
print(f"Matches made #: {matches_no}")
print(f"Matches made %: {matches_no / dt_agg_full.shape[0] * 100}")

for col in ['commissioning_year_debt', "primary_fuel"]:
    missing = dt_agg_full.loc[dt_agg_full[col].str.contains("missing")][col].nunique()
    print(f"Check: missing {col} in debt: {missing}")

# visual example
dt_agg_full[['location_id', "primary_fuel", "year_range_debt", "year_range_pp", "PP_key"]].head()

dt_agg_full['match'] = dt_agg_full.apply(lambda row: is_close_overlap(row['lower_limit_pp'], row['upper_limit_pp'], row['lower_limit_debt'], row['upper_limit_debt'], tolerance=YEAR_THRESHOLD_JOIN_BUCGP) if np.isnan(row['lower_limit_debt']) == False and np.isnan(row['lower_limit_pp']) == False else False, axis=1)

# visual example
dt_agg_full[['location_id', "primary_fuel", "year_range_debt", "year_range_pp", "match"]].head()

# check how many valid pairs we have and how many are True vs False
# valid pairs: they have both information on WEPP and GPPD in their row
valid_matches_made = dt_agg_full.loc[(~dt_agg_full['lower_limit_pp'].isna()) & (~dt_agg_full['lower_limit_debt'].isna())]
print(f"Valid matches #: {valid_matches_made.shape[0]}")
print(f"Valid matches %: {valid_matches_made.shape[0] / dt_agg_full.shape[0] * 100}")
for value in [True, False]:
    print(f"{value} matches #: {valid_matches_made.loc[valid_matches_made['match'] == value].shape[0]}")
    print(f"{value} matches %: {valid_matches_made.loc[valid_matches_made['match'] == value].shape[0] / valid_matches_made.shape[0] * 100}")

# visual check
valid_matches_made[['lower_limit_pp', "upper_limit_pp", 'lower_limit_debt', "upper_limit_debt", "match"]]

# check that all those rows that have at least one year missing false as a match
missing_years = dt_agg_full.loc[(dt_agg_full['commissioning_year_debt'].str.contains("missing") ) | (dt_agg_full['commissioning_year_pp'].isna()) ]
print(f"Check: all the rows with a missing year are not matched: {missing_years.loc[missing_years['match'] == False].shape[0] == missing_years.shape[0]}")

"""##### C. Clean

<b>Clean PP_key
"""

# Those that have been matched to a pp row but the match wasn't good
false_1 = dt_agg_full.loc[(dt_agg_full['match'] == False) & (~dt_agg_full['PP_key'].isna())]
false_1[["PP_key", "match", "year_range_debt", "year_range_pp"]]

# Those that have been not matched to a pp row.
false_2 = dt_agg_full.loc[(dt_agg_full['match'] == False) & (dt_agg_full['PP_key'].isna())]
false_2[["PP_key", "match", "year_range_debt", "year_range_pp"]]

print(f"Check: all the rows with 'False' match belong to one of the two categories above: {false_1.shape[0] + false_2.shape[0] == dt_agg_full.loc[(dt_agg_full['match'] == False)].shape[0]}")

dt_agg_full[['match', "PP_key"]]

dt_agg_full['PP_key'] = dt_agg_full.apply(lambda row: np.nan if row['match'] == False else row['PP_key'], axis=1)

dt_agg_full[['match', "PP_key"]]

for col in ['commissioning_year_debt', "primary_fuel"]:
    missing = dt_agg_full.loc[dt_agg_full[col].str.contains("missing")][col].nunique()
    print(f"Check: missing {col} in debt: {missing}")

"""<b>Clean the columns names"""

columns_to_keep = ['commissioning_year', 'year_range']
dt_agg_full = dt_agg_full.rename(columns={col+"_debt": col for col in columns_to_keep})

dt_agg_full[['commissioning_year', 'year_range']].head()

"""<b>Drop duplicates"""

# show example of duplicates
example = ("1015553", "coal", "2023.0")
dt_agg_full.loc[(dt_agg_full['location_id'] == example[0]) & (dt_agg_full['primary_fuel'] == example[1]) & (dt_agg_full['commissioning_year'] == example[2]) ][['location_id', "primary_fuel", "commissioning_year"]]

debt_columns = list(debt.columns)

dt_agg_full = my_reset_index(dt_agg_full)

# example
dt_agg_full.loc[dt_agg_full['power_plant_name'].str.contains("Adama")][debt_columns+['PP_key']]

# example
dt_agg_full.loc[dt_agg_full['power_plant_name'].str.contains("Berezovskaya")][debt_columns+['PP_key']]

# example
dt_agg_full.loc[dt_agg_full['power_plant_name'].str.contains("Lukoml")][debt_columns+['PP_key']]

to_fix = dt_agg_full.loc[(~dt_agg_full['PP_key'].isna())].index.to_list()

for index_to_fix in to_fix:
    row = dt_agg_full.iloc[index_to_fix]
    for i in find_equal_rows(dt_agg_full, row, debt_columns):
        row_t = dt_agg_full.iloc[i]
        if isinstance(row_t['PP_key'], str) == False:
            dt_agg_full.at[i, "PP_key"] = row['PP_key']

# example
dt_agg_full.loc[dt_agg_full['power_plant_name'].str.contains("Adama")][debt_columns+['PP_key']]

# example
dt_agg_full.loc[dt_agg_full['power_plant_name'].str.contains("Berezovskaya")][debt_columns+['PP_key']]

# example
dt_agg_full.loc[dt_agg_full['power_plant_name'].str.contains("Lukoml")][debt_columns+['PP_key']]

dt_agg_full = dt_agg_full.drop_duplicates(subset=debt_columns + ['PP_key'])
dt_agg_full = my_reset_index(dt_agg_full)

dt_agg_full.loc[dt_agg_full['power_plant_name'].str.contains("Adama")][debt_columns+['PP_key']]

# example
dt_agg_full.loc[dt_agg_full['power_plant_name'].str.contains("Lukoml")][debt_columns+['PP_key']]

print(f"There are as many rows in eq_agg_full as in equity_agg: {debt_agg.shape[0]== dt_agg_full.shape[0]}")
print(f"All the bu_id_bu_cgp have been preserved: {len(set(debt_agg['bu_id_bu_cgp'].unique()) & set(dt_agg_full['bu_id_bu_cgp'].unique())) == len(set(debt_agg['bu_id_bu_cgp'].unique()))}")
print(f"The two datasets have the same values: {(dt_agg_full[debt_columns] == debt_agg[debt_columns]).all().all()}")

"""#### 4.3 Add a new PP_key for those rows that weren't match"""

# make an unique PP_key for those that don't have (because there is no matching entry in Power Plant for these entries in equity)
eq_agg_full = my_reset_index(eq_agg_full)
eq_agg_full = fill_column_in_rows(eq_agg_full, col="PP_key", prefix =  "PLANTKEY_BUCGP_EQ_")

# number of unmatched power plants (should be equal as the number printed above)
rows_with_bucgp_key_no = eq_agg_full.loc[eq_agg_full["PP_key"].str.contains("PLANTKEY_BUCGP_")].shape[0]
print(f"Rows that now have a BUGCP PP_key: {rows_with_bucgp_key_no}")
print(f"Check: all the rows with BUCGP PP_key had a 'False' match: {rows_with_bucgp_key_no == eq_agg_full.loc[eq_agg_full['match'] == False].shape[0]}")

# make an unique PP_key for those that don't have (because there is no matching entry in Power Plant for these debt in equity)
dt_agg_full = my_reset_index(dt_agg_full)
dt_agg_full = fill_column_in_rows(dt_agg_full, col="PP_key", prefix =  "PLANTKEY_BUCGP_DT_")

# number of unmatched power plants
rows_with_bucgp_key_no = dt_agg_full.loc[dt_agg_full["PP_key"].str.contains("PLANTKEY_BUCGP_")].shape[0]
print(f"Rows that now have a BUGCP PP_key: {rows_with_bucgp_key_no}")
print(f"Check: all the rows with BUCGP PP_key had a 'False' match: {rows_with_bucgp_key_no == dt_agg_full.loc[dt_agg_full['match'] == False].shape[0]}")

"""#### 4.4 Transform back the commissioning year to be nan"""

print(f"Missing commissioning year in equity: {eq_agg_full.loc[eq_agg_full['commissioning_year'].str.contains('missing')].shape[0]}")
print(f"Missing commissioning year in debt: {dt_agg_full.loc[dt_agg_full['commissioning_year'].str.contains('missing')].shape[0]}")

eq_agg_full['commissioning_year'] = eq_agg_full['commissioning_year'].apply(lambda x: np.nan if "missing" in x else x)
dt_agg_full['commissioning_year'] = dt_agg_full['commissioning_year'].apply(lambda x: np.nan if "missing" in x else x)

print(f"Missing commissioning year in equity: {eq_agg_full.loc[eq_agg_full['commissioning_year'].isna()].shape[0]}")
print(f"Missing commissioning year in debt: {dt_agg_full.loc[dt_agg_full['commissioning_year'].isna()].shape[0]}")

"""## 5. Make Final tables

#### 5.1 Create the empty tables
"""

db_structure = map_table[db_structure_sheet]
tables_df = get_tables_to_create_empty(db_structure)

# create support databases
tables_df_tmp = get_tables_to_create_empty(db_structure)

"""#### 5.2 Add equity_id and debt_id

We need to create equity_id and debt_id.
* equity_id: an id to identify the investments. Needed to merge with Transaction.</br>
* debt_id: same as above.

We may need in the future:
* investor_id: we will need to have a bridging table between Transaction and Investor in case there are likely duplicates in the names of the investor.
    * this is in case we have to do something similar to what we did in City: dropping duplicates in City while making sure that we could re-join Power Plant.
    * not needed now because Transaction will also hold a equity_id which for now matches 1-1 with Investor (before dropping duplicates) because each entry in equity_agg (from bu_cgp) only has data on one investor at the time (same for debt).
    * this will be done in the post-processing.
"""

# create unique ids with custom function
eq_agg_full = create_unique_id(eq_agg_full, "equity_id", "EQUITYID_BUCGP_")
dt_agg_full = create_unique_id(dt_agg_full, "debt_id", "DEBTID_BUCGP_")

"""#### 5.3 Make the Equity, Debt tables

Let's make Equity and Debt that can be already created.
"""

# fill the db that can already be filled
# TODO: put "Equity" in a variable (not called equity_name)
entity = "Equity"
for column in tables_df[entity].columns:
    if column in eq_agg_full.columns:
        tables_df[entity][column] = eq_agg_full[column]

# fill the db that can already be filled
# TODO: put "Debt" in a variable (not called equity_name)
entity = "Debt"
for column in tables_df[entity].columns:
    if column in dt_agg_full.columns:
        tables_df[entity][column] = dt_agg_full[column]

# make sure that the number_of_lenders column is NaN always (number_of_lenders is filled only with REF_LOAN data)
tables_df["Debt"]['number_of_lenders'] = np.nan

"""#### 4.4 Make Transaction, and Investor tables

Investor and Transaction need some tweaking because the names of the columns don't match with those of the db structure.
"""

# TODO: what if the clients change what column matches equity_investor_name?

# we need to rename some columns to match the columns in Investor and Transactio
renamings = get_variables_advanced(vertical_slicing, bu_cgp_name, ['Investor', "Transaction"])
# the following two are not in the vertical_slicing (which comes from Variables.xlsx) because we created these two columns not from data coming from the sources (they are based
# on the index of the dataframes....)
renamings["equity_id"] = "investment_id"
renamings["debt_id"] = "investment_id"
renamings

# do renaming
# Note: we need to filter because some renaimings rules only apply to eq_agg_full and some only to dt_agg_full
eq_agg_full_new_names = eq_agg_full.rename(columns=dict(filter(lambda x: x[0] in eq_agg_full.columns, renamings.items())))
eq_agg_full_new_names.head(2)

# do renaming
dt_agg_full_new_names= dt_agg_full.rename(columns=dict(filter(lambda x: x[0] in dt_agg_full.columns, renamings.items())))
dt_agg_full_new_names.head(2)

# create table now that all is set
for entity in ["Transaction", "Investor"]:
    for column in tables_df[entity].columns:
        if column in eq_agg_full_new_names.columns:
            tables_df[entity][column] = eq_agg_full_new_names[column]

# we can manually add here the "investment_average" and "invesmtent_weighted" since for all the rows there is no amount so we didn't do anything
tables_df['Transaction']['investment_averaged'] = "N"
tables_df['Transaction']['investment_weighted'] = "N"

"""Since the Investor and Transaction tables are already full, then we add the data for these two tables that stem from dt_agg_full to these exisitng tables. To do so, we create new ones and then concat them."""

for entity in ["Transaction", "Investor"]:
    for column in tables_df_tmp[entity].columns:
        if column in dt_agg_full_new_names.columns:
            tables_df_tmp[entity][column] = dt_agg_full_new_names[column]

# concat
tables_df['Investor'] = pd.concat([tables_df['Investor'], tables_df_tmp['Investor']])
tables_df['Transaction'] =pd.concat([tables_df['Transaction'], tables_df_tmp['Transaction']])

# put the values for these columns (we didn't deal with the amounts since there was none)
tables_df['Transaction']['investment_averaged'] = "N"
tables_df['Transaction']['investment_weighted'] = "N"

"""#### 5.5 Add the missing power plants to Power Plant

Not all the projects listed in BU_CGP had a matching power plant in Power Plant.
"""

missing_pp_in_eq = eq_agg_full.loc[eq_agg_full["PP_key"].str.contains("PLANTKEY_BUCGP_")]
missing_in_eq_no = missing_pp_in_eq.shape[0]
missing_pp_in_dt = dt_agg_full.loc[dt_agg_full["PP_key"].str.contains("PLANTKEY_BUCGP_")]
missing_in_dt_no = missing_pp_in_dt.shape[0]
print("Missing power plant: " + str(missing_in_eq_no + missing_in_dt_no))
# denominator = all entries: we need to avoid to double count those power plants that are both in eq_agg and dt_agg
# because they contained both equity and debt investments
perc = (missing_in_eq_no + missing_in_dt_no) / (eq_agg_full.shape[0] + dt_agg_full.shape[0] - 49) * 100
print("Missing % over bu_cgp: " + str(perc))

# get the data from equity
for entity in ['Power Plant', "Country"]:
    for column in tables_df[entity].columns:
        if column in missing_pp_in_eq.columns:
            tables_df[entity][column] = missing_pp_in_eq[column]

"""Note that the data here (dt_agg_full and eq_agg_full) don't have "city_key" but have "PP_key" which still needs to go to City so that we can create the bridging table later on."""

entity = "City"
for column in tables_df[entity].columns:
    if column in missing_pp_in_eq.columns:
        tables_df[entity][column] = missing_pp_in_eq[column]
    elif column == "city_key":
        tables_df[entity][column] = missing_pp_in_eq["PP_key"]

"""Get the data from debt. But same as above: since the tables_df tables are already filled then we use a temporary destianation for the data before we concat."""

for entity in ['Power Plant', "Country"]:
    for column in tables_df_tmp[entity].columns:
        if column in missing_pp_in_dt.columns:
            tables_df_tmp[entity][column] = missing_pp_in_dt[column]

# same above
entity = "City"
for column in tables_df[entity].columns:
    if column in missing_pp_in_dt.columns:
        tables_df_tmp[entity][column] = missing_pp_in_dt[column]
    elif column == "city_key":
        tables_df_tmp[entity][column] = missing_pp_in_dt["PP_key"]

# concat
for entity in ['Power Plant', "City", "Country"]:
    tables_df[entity] = pd.concat([tables_df[entity], tables_df_tmp[entity]])

for entity in ['Power Plant', "City", "Country"]:
    print(tables_df[entity].shape[0])

"""#### 5.6 Check that we got all the data from BU_CGP"""

tables_df['Power Plant'].head()

for col in ['commissioning_year', "primary_fuel"]:
    missing = tables_df['Power Plant'].loc[(tables_df['Power Plant'][col].isna()) | (tables_df['Power Plant'][col].str.contains("missing"))][col].nunique()
    print(f"Check: missing {col} in debt: {missing}")

tables_df['City'].head()

tables_df['Country'].head()

tables_df['Debt'].head()

# number of matches with WEPP+GPPD sources
tables_df['Debt'].loc[tables_df['Debt']['PP_key'].str.contains("WEPP")].shape[0]

tables_df['Equity'].head()

# number of matches with WEPP+GPPD sources
tables_df['Equity'].loc[tables_df['Equity']['PP_key'].str.contains("WEPP")].shape[0]

tables_df['Transaction'].head()

tables_df['Investor'].head()

"""#### 5.7 Save"""

print_final_statistics(tables_df, tables_to_create)

"""All the tables but Debt and Equity will go in the "Intermediate Resutls" folders so they can be further processed."""

# for entity in ['Power Plant', 'City', 'Country', 'Transaction', 'Investor']:
#     tables_df[entity].to_excel(intermediate_folder + entity + "_BUCGP.xlsx", index=False)
for entity in ['Power Plant', 'City', 'Country', 'Transaction', 'Investor']:
    tables_df[entity].to_csv(intermediate_folder + entity + "_BUCGP.csv", index=False)
# TODO: this suffix should be a variable

"""Debt and Equity will go directly in the "Current Final Tables" because they don't need further processing (as these have been just created for the first time) and, anyways, they will be updated each time when processing the other sources."""

# for entity in ['Equity', 'Debt']:
#     tables_df[entity].to_excel(current_final_tables_folder + entity + ".xlsx", index=False)
for entity in ['Equity', 'Debt']:
    tables_df[entity].to_csv(current_final_tables_folder + entity + ".csv", index=False)

[print(x) for x in tables_df["Power Plant"].columns]
