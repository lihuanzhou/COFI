# -*- coding: utf-8 -*-
"""Step CLEANING - 1 - Country.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PL8Mkf6wPcQsFDg2bLGDiv2-hf5kBhsm

So, now that we have gathered all the Country information in different tables we can concatanate them and then post-process them together.
"""

import pandas as pd

from functions.cleaning import get_all_files, my_reset_index
from functions.cleaning import check_if_new_country_names, update_country_names
from functions.cleaning import change_weird_apostrophe, my_read_excel

# TODO: instead of changing the weird apostropehe each time in the UN Data (see below) and in the function, update the UN Data once and for all so that the weird apostrphes are gone

# needed folder paths
input_folder = "Cleaned Data/" # or we can put this info in the "files name" sheet
intermediate_folder = "Intermediate Results/" # to save some intermediate results
# output_folder = "Final tables/" # where we store the final tables (e.g. Power Plant)
output_folder = "Current Final Tables/"

# load the excel containing all the mapping information
map_table_name = "Variables.xlsx"
map_table = pd.read_excel(map_table_name, sheet_name=None) # read all the sheets that we will be using togehter
print(len(map_table))


# the sheet where there is the information that the client can change
files_names_sheet = "file names"
groupby_info_sheet = "formatting"
vertical_slicing_info_sheet = "variables to use"
aggregation_info_sheet = "aggregation"
db_structure_sheet = "db structure"
un_country_info_sheet = "UN Country Info"
country_names_dict_sheet = "country names dicionary"
bri_countries_sheet = "BRI countries"

# the name of the Country table
country_name = "Country"

# list of the names of the various Country tables
# TODO: uniform all the names of these files
# country_files_list = ["Country_not_cleaned.xlsx", "Country_need_post_processing_BU_CGP_.xlsx", "BU_CGEF_embedding_power_generation/Country_BUCGEF.xlsx", "BU_CGEF_embedding_multipurpose/Country_BUCGEF.xlsx"]
# country_files_list = ["Country_not_cleaned.xlsx", "Country_need_post_processing_BU_CGP_.xlsx", "BU_CGEF_fuzzy_matching - fuzz.partial_ratio/Country_BUCGEF.xlsx", "Country_SAISCLA_IADGEGI.xlsx", "Country_FDI_RMA.xlsx"]
# country_files_list = ["Country_not_cleaned.xlsx", "Country_need_post_processing_BU_CGP_.xlsx", "BU_CGEF_fuzzy_matching - fuzz.partial_ratio/Country_BUCGEF.xlsx", "Country_SAISCLA_IADGEGI.xlsx", "Country_FDI_RMA.xlsx", "Country_IJ_Global.xlsx"]

"""List of sources to clean and merge together after processing each data source:
* after WEPP & GPPD: ['Country_WEPPGPPD.xlsx']
* after BU_CGP: ['Country_BUCGP.xlsx', 'Country_WEPPGPPD.xlsx']
* after BU_CGEF: ['Country_BUCGEF.xlsx', 'Country_BUCGP.xlsx', 'Country_WEPPGPPD.xlsx']
* after SAIS_CLA+IAD_GEGI: ['Country_SAISCLA_IADGEGI.xlsx',  'Country_BUCGEF.xlsx', 'Country_BUCGP.xlsx', 'Country_WEPPGPPD.xlsx']
* after IJ_GLOBAL: ['Country_IJ_Global.xlsx',
 'Country_SAISCLA_IADGEGI.xlsx',
 'Country_BUCGEF.xlsx',
 'Country_BUCGP.xlsx',
 'Country_WEPPGPPD.xlsx']
* aafter REF_MA: ['Country_REFINITIVMA.xlsx',
 'Country_IJ_Global.xlsx',
 'Country_SAISCLA_IADGEGI.xlsx',
 'Country_BUCGEF.xlsx',
 'Country_BUCGP.xlsx',
 'Country_WEPPGPPD.xlsx']
* after FDI_Markets: ['Country_FDIMARKETS.xlsx',
 'Country_REFINITIVMA.xlsx',
 'Country_IJ_Global.xlsx',
 'Country_SAISCLA_IADGEGI.xlsx',
 'Country_BUCGEF.xlsx',
 'Country_BUCGP.xlsx',
 'Country_WEPPGPPD.xlsx']
* after REF_LOAN: ['Country_REFINITIVLOAN.xlsx',
 'Country_FDIMARKETS.xlsx',
 'Country_REFINITIVMA.xlsx',
 'Country_IJ_Global.xlsx',
 'Country_SAISCLA_IADGEGI.xlsx',
 'Country_BUCGEF.xlsx',
 'Country_BUCGP.xlsx',
 'Country_WEPPGPPD.xlsx']
"""

# get the files
country_files_list = get_all_files(intermediate_folder, "Country")
country_files_list

"""#### 0. Load data

Concatanate the data from the list of files.
"""

# read all files
datasets = []
for file in country_files_list:
    datasets.append(my_read_excel(intermediate_folder, file))

datasets[0].shape

datasets[0].head()

# concatanate
country = pd.concat(datasets)
if "Unnamed: 0" in country.columns:
    country = country.drop(columns=['Unnamed: 0'])
print(country.shape)

country = my_reset_index(country)
country.head()

# check that there were no problems with concat: all entries have been added
country.shape[0] == sum([x.shape[0] for x in datasets])

"""#### 1. Fix the different capitalizations issues and data types, drop duplicates"""

# lower everthing
country['country'] = country['country'].apply(lambda x: x.lower())
# drop the same country names
country = country.drop_duplicates(subset="country")
country.shape[0]
# the number that we have here is around the 230 value

country['country'].head()

country.dtypes

for col in country.columns:
    country[col] = country[col].astype("string")

"""#### 2. Update the countries' names so they match the names in the UN dataset and drop duplicates again"""

# load data
un_data = map_table[un_country_info_sheet]
un_data.head()

# fix capitlaization of country names
un_data['Country or Area'] = un_data['Country or Area'].apply(lambda x: x.lower())

"""To match the countries in Country to the UN data we need to change their names. We can do this thanks to an ad-hoc created dictionary."""

# load dictionary
country_names_dict_df = map_table[country_names_dict_sheet]
country_names_dict_df.head(10)

"""Check if there are new names that are not in the UN Data nor in the synonym dictionary."""

check_if_new_country_names(list(country['country'].unique()), un_data, country_names_dict_df)
# TODO: co

# convert the names (for the most datasets we already clean the country in their noteboooks, so it makes sense that here there is no need to update the names)
country = update_country_names(country_names_dict_df, country)

country.loc[(country['country'].str.contains("’")) | (country['country'].str.contains("’"))]

"""Let's drop duplicates again."""

# befoer dropping
print("before dropping: " + str(country.shape[0]))
# do dropping
country = country.drop_duplicates(subset="country")
# befoer dropping
print("after dropping: " + str(country.shape[0]))

"""#### 3. Do matching with UN Data to get iso3c, region, and subregion values

Now that the data is cleaned, the matching wiil be easy to do.
"""

# update un data
# TODO: we shouldn't do this each time (see above)
un_data['Country or Area'] = un_data['Country or Area'].apply(lambda x: change_weird_apostrophe(x))

# iterate through the Country data
for i, row in country.iterrows():
    data_from_un = un_data.loc[un_data['Country or Area'] == row['country']]
    if len(data_from_un) > 0: # there are some entries that are not in the UN data set
        # update isoc3
        country.at[i, "country_iso3c"] = data_from_un["ISO-alpha3 Code"].values[0]
        # region
        country.at[i, "region"] = data_from_un["Region Name"].values[0]
        # subregion
        country.at[i, "subregion"] = data_from_un["Sub-region Name"].values[0]

country.head()

# check that all countries got their info
print(f"All countries were correct: {country.loc[country['country_iso3c'].isna()].shape[0] == 0 or country.loc[country['country_iso3c'].isna()]['country'].values[0] == 'regional'}")

# only the rows that has "regional" has no iso3 value
country.loc[country['country_iso3c'].isna()]

# fill it with empty strings
country['country_iso3c'] = country['country_iso3c'].fillna("")

"""#### 4. Add bri_countries"""

# load info on which countries belong to the BRI
bri_countries = map_table[bri_countries_sheet]
bri_countries.head()

"""All the country codes of the BRI dataset are in the UN dataset! So we can use the "Country Code" in the BRI dataset to determine whether a country is a BRI one or not, along with the country_iso3c column in the Country table."""

# the bri_countries df and un_data df share the same country code
len(set(bri_countries["Country Code"]) & set(un_data["ISO-alpha3 Code"])) == len(set(bri_countries["Country Code"]))

# only keep the country codes
bri_countries_codes = list(bri_countries["Country Code"])

for i, row in country.iterrows():
    # note: the missing country iso3c are empty strings
    if row['country_iso3c'] == "":
        continue
    # if country is a BRI one:
    if row['country_iso3c'] in bri_countries_codes:
        country.at[i, "bri_country"] = "Y"
    # it is not a BRI country
    else:
        country.at[i, "bri_country"] = "N"

country

# check that all countries got their info
# the only entry that doesn't have a bri_country value is the one that has "regional" as "country" name
country.loc[country['bri_country'].isna()].shape[0] == 1

"""#### 5. Save

Now, the data is all together and ready to be saved in the output folder.
"""

output_folder

# save the full datasets in the Final Tables folder
country = country.sort_values(by="country")
country.to_csv(output_folder + country_name + ".csv", index=False)

